{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "616ab18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Loading: C:\\Users\\Dario\\Desktop\\ThesiS JBP\\Data\\all_signals_processed.mat\n",
      "ðŸ“Œ Feature order: ['Accel1', 'Accel2', 'Accel3', 'Accel4', 'Motor_current', 'Speed', 'Temperature']\n",
      "ðŸ“Œ Total sequences: 50000 | Each: 256Ã—7\n",
      "50000 (256, 7)\n",
      "Features: ['Accel1', 'Accel2', 'Accel3', 'Accel4', 'Motor_current', 'Speed', 'Temperature']\n",
      "N scalers: 24\n"
     ]
    }
   ],
   "source": [
    "from lib.data_preprocess import load_data\n",
    "\n",
    "data_file = r\"C:\\Users\\Dario\\Desktop\\ThesiS JBP\\Data\\all_signals_processed.mat\"\n",
    "seq_len = 256\n",
    "\n",
    "ori_data, scalers, feature_names = load_data(\n",
    "    \"mytests\",\n",
    "    seq_len,\n",
    "    file_list=[data_file],\n",
    "    step=128,\n",
    "    max_sequences=50000\n",
    ")\n",
    "\n",
    "print(len(ori_data), ori_data[0].shape)\n",
    "print(\"Features:\", feature_names)\n",
    "print(\"N scalers:\", len(scalers))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbdcf3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ANALYSIS OF all_signals_processed.mat\n",
      "\n",
      " Total JSON files inside: 204\n",
      "\n",
      " Signals available: ['Accel1', 'Accel2', 'Accel3', 'Accel4', 'Motor_current', 'Speed', 'Temperature']\n",
      " Number of signals: 7\n",
      "\n",
      " Length statistics per file (all signals have same length inside a file):\n",
      "   Min length: 270000\n",
      "   Max length: 270000\n",
      "\n",
      " Total samples (sum of lengths): 55080000\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mat_path = r\"C:\\Users\\Dario\\Desktop\\ThesiS JBP\\Data\\all_signals_processed.mat\"\n",
    "\n",
    "print(\" ANALYSIS OF all_signals_processed.mat\\n\")\n",
    "\n",
    "with h5py.File(mat_path, \"r\") as f:\n",
    "\n",
    "    data_all = f[\"data_all\"]\n",
    "    n_files = data_all.shape[1]\n",
    "\n",
    "    print(f\" Total JSON files inside: {n_files}\")\n",
    "\n",
    "    signal_lengths = []\n",
    "    file_lengths = []\n",
    "    signal_names_global = None\n",
    "\n",
    "    for i in range(n_files):\n",
    "        ref = data_all[0][i]\n",
    "        entry = f[ref]\n",
    "\n",
    "        sig_group = entry[\"signals_processed\"]\n",
    "        sig_names = list(sig_group.keys())\n",
    "\n",
    "        # Save signal names from first file\n",
    "        if signal_names_global is None:\n",
    "            signal_names_global = sig_names\n",
    "\n",
    "        # Read one signal just to know length\n",
    "        first_signal = np.array(sig_group[sig_names[0]][:]).flatten()\n",
    "        file_lengths.append(len(first_signal))\n",
    "\n",
    "        signal_lengths.append([len(np.array(sig_group[s])[:].flatten())\n",
    "                               for s in sig_names])\n",
    "\n",
    "    signal_lengths = np.array(signal_lengths)\n",
    "\n",
    "    print(\"\\n Signals available:\", signal_names_global)\n",
    "    print(\" Number of signals:\", len(signal_names_global))\n",
    "\n",
    "    print(\"\\n Length statistics per file (all signals have same length inside a file):\")\n",
    "    print(f\"   Min length: {signal_lengths.min()}\")\n",
    "    print(f\"   Max length: {signal_lengths.max()}\")\n",
    "  \n",
    "\n",
    "    print(\"\\n Total samples (sum of lengths):\",\n",
    "          int(signal_lengths[:, 0].sum()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5adda4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Adjusted opt.z_dim to match data feature size: 7\n",
      "=== PRETRAINING: Embedding + Recovery ===\n",
      "[DEBUG] Loss this iteration: 0.101021\n",
      "[ER] Iter 0/500 | ER Loss = 0.101021\n",
      "[DEBUG] Loss this iteration: 0.066123\n",
      "[DEBUG] Loss this iteration: 0.043949\n",
      "[DEBUG] Loss this iteration: 0.027737\n",
      "[DEBUG] Loss this iteration: 0.020657\n",
      "[DEBUG] Loss this iteration: 0.015910\n",
      "[DEBUG] Loss this iteration: 0.017436\n",
      "[DEBUG] Loss this iteration: 0.014999\n",
      "[DEBUG] Loss this iteration: 0.012766\n",
      "[DEBUG] Loss this iteration: 0.014178\n",
      "[DEBUG] Loss this iteration: 0.010221\n",
      "[DEBUG] Loss this iteration: 0.008395\n",
      "[DEBUG] Loss this iteration: 0.008508\n",
      "[DEBUG] Loss this iteration: 0.007857\n",
      "[DEBUG] Loss this iteration: 0.008628\n",
      "[DEBUG] Loss this iteration: 0.007084\n",
      "[DEBUG] Loss this iteration: 0.006113\n",
      "[DEBUG] Loss this iteration: 0.004932\n",
      "[DEBUG] Loss this iteration: 0.005260\n",
      "[DEBUG] Loss this iteration: 0.004794\n",
      "[DEBUG] Loss this iteration: 0.004318\n",
      "[DEBUG] Loss this iteration: 0.004113\n",
      "[DEBUG] Loss this iteration: 0.003728\n",
      "[DEBUG] Loss this iteration: 0.003952\n",
      "[DEBUG] Loss this iteration: 0.003709\n",
      "[DEBUG] Loss this iteration: 0.003527\n",
      "[DEBUG] Loss this iteration: 0.003872\n",
      "[DEBUG] Loss this iteration: 0.003105\n",
      "[DEBUG] Loss this iteration: 0.003436\n",
      "[DEBUG] Loss this iteration: 0.002964\n",
      "[DEBUG] Loss this iteration: 0.002888\n",
      "[DEBUG] Loss this iteration: 0.002881\n",
      "[DEBUG] Loss this iteration: 0.002527\n",
      "[DEBUG] Loss this iteration: 0.002526\n",
      "[DEBUG] Loss this iteration: 0.002314\n",
      "[DEBUG] Loss this iteration: 0.002505\n",
      "[DEBUG] Loss this iteration: 0.002215\n",
      "[DEBUG] Loss this iteration: 0.002317\n",
      "[DEBUG] Loss this iteration: 0.002421\n",
      "[DEBUG] Loss this iteration: 0.002401\n",
      "[DEBUG] Loss this iteration: 0.003030\n",
      "[DEBUG] Loss this iteration: 0.002239\n",
      "[DEBUG] Loss this iteration: 0.001801\n",
      "[DEBUG] Loss this iteration: 0.001955\n",
      "[DEBUG] Loss this iteration: 0.001746\n",
      "[DEBUG] Loss this iteration: 0.001578\n",
      "[DEBUG] Loss this iteration: 0.001605\n",
      "[DEBUG] Loss this iteration: 0.001585\n",
      "[DEBUG] Loss this iteration: 0.001487\n",
      "[DEBUG] Loss this iteration: 0.001627\n",
      "[DEBUG] Loss this iteration: 0.001959\n",
      "[DEBUG] Loss this iteration: 0.001727\n",
      "[DEBUG] Loss this iteration: 0.001800\n",
      "[DEBUG] Loss this iteration: 0.001508\n",
      "[DEBUG] Loss this iteration: 0.001522\n",
      "[DEBUG] Loss this iteration: 0.001519\n",
      "[DEBUG] Loss this iteration: 0.001343\n",
      "[DEBUG] Loss this iteration: 0.001453\n",
      "[DEBUG] Loss this iteration: 0.001371\n",
      "[DEBUG] Loss this iteration: 0.001211\n",
      "[DEBUG] Loss this iteration: 0.001249\n",
      "[DEBUG] Loss this iteration: 0.001198\n",
      "[DEBUG] Loss this iteration: 0.001549\n",
      "[DEBUG] Loss this iteration: 0.001485\n",
      "[DEBUG] Loss this iteration: 0.001572\n",
      "[DEBUG] Loss this iteration: 0.001412\n",
      "[DEBUG] Loss this iteration: 0.001048\n",
      "[DEBUG] Loss this iteration: 0.001017\n",
      "[DEBUG] Loss this iteration: 0.000898\n",
      "[DEBUG] Loss this iteration: 0.001050\n",
      "[DEBUG] Loss this iteration: 0.000856\n",
      "[DEBUG] Loss this iteration: 0.000994\n",
      "[DEBUG] Loss this iteration: 0.000888\n",
      "[DEBUG] Loss this iteration: 0.000826\n",
      "[DEBUG] Loss this iteration: 0.001069\n",
      "[DEBUG] Loss this iteration: 0.001086\n",
      "[DEBUG] Loss this iteration: 0.001381\n",
      "[DEBUG] Loss this iteration: 0.001350\n",
      "[DEBUG] Loss this iteration: 0.001036\n",
      "[DEBUG] Loss this iteration: 0.000709\n",
      "[DEBUG] Loss this iteration: 0.000728\n",
      "[DEBUG] Loss this iteration: 0.000721\n",
      "[DEBUG] Loss this iteration: 0.000778\n",
      "[DEBUG] Loss this iteration: 0.000703\n",
      "[DEBUG] Loss this iteration: 0.000840\n",
      "[DEBUG] Loss this iteration: 0.001197\n",
      "[DEBUG] Loss this iteration: 0.000894\n",
      "[DEBUG] Loss this iteration: 0.000651\n",
      "[DEBUG] Loss this iteration: 0.000672\n",
      "[DEBUG] Loss this iteration: 0.000724\n",
      "[DEBUG] Loss this iteration: 0.000654\n",
      "[DEBUG] Loss this iteration: 0.000604\n",
      "[DEBUG] Loss this iteration: 0.000633\n",
      "[DEBUG] Loss this iteration: 0.000869\n",
      "[DEBUG] Loss this iteration: 0.000857\n",
      "[DEBUG] Loss this iteration: 0.000944\n",
      "[DEBUG] Loss this iteration: 0.000758\n",
      "[DEBUG] Loss this iteration: 0.000617\n",
      "[DEBUG] Loss this iteration: 0.000549\n",
      "[DEBUG] Loss this iteration: 0.000802\n",
      "[DEBUG] Loss this iteration: 0.000653\n",
      "[ER] Iter 100/500 | ER Loss = 0.000653\n",
      "[DEBUG] Loss this iteration: 0.000597\n",
      "[DEBUG] Loss this iteration: 0.000571\n",
      "[DEBUG] Loss this iteration: 0.000557\n",
      "[DEBUG] Loss this iteration: 0.000607\n",
      "[DEBUG] Loss this iteration: 0.000581\n",
      "[DEBUG] Loss this iteration: 0.000634\n",
      "[DEBUG] Loss this iteration: 0.000471\n",
      "[DEBUG] Loss this iteration: 0.000425\n",
      "[DEBUG] Loss this iteration: 0.000405\n",
      "[DEBUG] Loss this iteration: 0.000503\n",
      "[DEBUG] Loss this iteration: 0.000408\n",
      "[DEBUG] Loss this iteration: 0.000431\n",
      "[DEBUG] Loss this iteration: 0.000439\n",
      "[DEBUG] Loss this iteration: 0.000674\n",
      "[DEBUG] Loss this iteration: 0.001016\n",
      "[DEBUG] Loss this iteration: 0.000708\n",
      "[DEBUG] Loss this iteration: 0.000677\n",
      "[DEBUG] Loss this iteration: 0.000991\n",
      "[DEBUG] Loss this iteration: 0.001119\n",
      "[DEBUG] Loss this iteration: 0.000460\n",
      "[DEBUG] Loss this iteration: 0.000531\n",
      "[DEBUG] Loss this iteration: 0.000436\n",
      "[DEBUG] Loss this iteration: 0.000374\n",
      "[DEBUG] Loss this iteration: 0.000395\n",
      "[DEBUG] Loss this iteration: 0.000444\n",
      "[DEBUG] Loss this iteration: 0.000371\n",
      "[DEBUG] Loss this iteration: 0.000391\n",
      "[DEBUG] Loss this iteration: 0.000438\n",
      "[DEBUG] Loss this iteration: 0.000508\n",
      "[DEBUG] Loss this iteration: 0.000499\n",
      "[DEBUG] Loss this iteration: 0.000513\n",
      "[DEBUG] Loss this iteration: 0.000443\n",
      "[DEBUG] Loss this iteration: 0.000468\n",
      "[DEBUG] Loss this iteration: 0.000530\n",
      "[DEBUG] Loss this iteration: 0.000651\n",
      "[DEBUG] Loss this iteration: 0.000562\n",
      "[DEBUG] Loss this iteration: 0.000407\n",
      "[DEBUG] Loss this iteration: 0.000372\n",
      "[DEBUG] Loss this iteration: 0.000344\n",
      "[DEBUG] Loss this iteration: 0.000406\n",
      "[DEBUG] Loss this iteration: 0.000332\n",
      "[DEBUG] Loss this iteration: 0.000450\n",
      "[DEBUG] Loss this iteration: 0.000795\n",
      "[DEBUG] Loss this iteration: 0.000935\n",
      "[DEBUG] Loss this iteration: 0.000430\n",
      "[DEBUG] Loss this iteration: 0.000552\n",
      "[DEBUG] Loss this iteration: 0.000506\n",
      "[DEBUG] Loss this iteration: 0.000352\n",
      "[DEBUG] Loss this iteration: 0.000435\n",
      "[DEBUG] Loss this iteration: 0.000523\n",
      "[DEBUG] Loss this iteration: 0.000588\n",
      "[DEBUG] Loss this iteration: 0.000442\n",
      "[DEBUG] Loss this iteration: 0.000359\n",
      "[DEBUG] Loss this iteration: 0.000322\n",
      "[DEBUG] Loss this iteration: 0.000335\n",
      "[DEBUG] Loss this iteration: 0.000334\n",
      "[DEBUG] Loss this iteration: 0.000243\n",
      "[DEBUG] Loss this iteration: 0.000315\n",
      "[DEBUG] Loss this iteration: 0.000314\n",
      "[DEBUG] Loss this iteration: 0.000349\n",
      "[DEBUG] Loss this iteration: 0.000493\n",
      "[DEBUG] Loss this iteration: 0.000779\n",
      "[DEBUG] Loss this iteration: 0.000494\n",
      "[DEBUG] Loss this iteration: 0.000362\n",
      "[DEBUG] Loss this iteration: 0.000461\n",
      "[DEBUG] Loss this iteration: 0.000609\n",
      "[DEBUG] Loss this iteration: 0.000521\n",
      "[DEBUG] Loss this iteration: 0.000419\n",
      "[DEBUG] Loss this iteration: 0.000358\n",
      "[DEBUG] Loss this iteration: 0.000362\n",
      "[DEBUG] Loss this iteration: 0.000313\n",
      "[DEBUG] Loss this iteration: 0.000310\n",
      "[DEBUG] Loss this iteration: 0.000286\n",
      "[DEBUG] Loss this iteration: 0.000298\n",
      "[DEBUG] Loss this iteration: 0.000288\n",
      "[DEBUG] Loss this iteration: 0.000375\n",
      "[DEBUG] Loss this iteration: 0.000797\n",
      "[DEBUG] Loss this iteration: 0.000775\n",
      "[DEBUG] Loss this iteration: 0.000387\n",
      "[DEBUG] Loss this iteration: 0.000359\n",
      "[DEBUG] Loss this iteration: 0.000414\n",
      "[DEBUG] Loss this iteration: 0.000382\n",
      "[DEBUG] Loss this iteration: 0.000371\n",
      "[DEBUG] Loss this iteration: 0.000354\n",
      "[DEBUG] Loss this iteration: 0.000278\n",
      "[DEBUG] Loss this iteration: 0.000329\n",
      "[DEBUG] Loss this iteration: 0.000276\n",
      "[DEBUG] Loss this iteration: 0.000231\n",
      "[DEBUG] Loss this iteration: 0.000259\n",
      "[DEBUG] Loss this iteration: 0.000252\n",
      "[DEBUG] Loss this iteration: 0.000239\n",
      "[DEBUG] Loss this iteration: 0.000243\n",
      "[DEBUG] Loss this iteration: 0.000339\n",
      "[DEBUG] Loss this iteration: 0.000563\n",
      "[DEBUG] Loss this iteration: 0.000898\n",
      "[DEBUG] Loss this iteration: 0.000616\n",
      "[DEBUG] Loss this iteration: 0.000497\n",
      "[DEBUG] Loss this iteration: 0.000590\n",
      "[DEBUG] Loss this iteration: 0.000299\n",
      "[DEBUG] Loss this iteration: 0.000324\n",
      "[ER] Iter 200/500 | ER Loss = 0.000324\n",
      "[DEBUG] Loss this iteration: 0.000251\n",
      "[DEBUG] Loss this iteration: 0.000249\n",
      "[DEBUG] Loss this iteration: 0.000257\n",
      "[DEBUG] Loss this iteration: 0.000255\n",
      "[DEBUG] Loss this iteration: 0.000256\n",
      "[DEBUG] Loss this iteration: 0.000322\n",
      "[DEBUG] Loss this iteration: 0.000439\n",
      "[DEBUG] Loss this iteration: 0.000451\n",
      "[DEBUG] Loss this iteration: 0.000329\n",
      "[DEBUG] Loss this iteration: 0.000242\n",
      "[DEBUG] Loss this iteration: 0.000228\n",
      "[DEBUG] Loss this iteration: 0.000299\n",
      "[DEBUG] Loss this iteration: 0.000282\n",
      "[DEBUG] Loss this iteration: 0.000328\n",
      "[DEBUG] Loss this iteration: 0.000386\n",
      "[DEBUG] Loss this iteration: 0.000391\n",
      "[DEBUG] Loss this iteration: 0.000313\n",
      "[DEBUG] Loss this iteration: 0.000326\n",
      "[DEBUG] Loss this iteration: 0.000405\n",
      "[DEBUG] Loss this iteration: 0.000419\n",
      "[DEBUG] Loss this iteration: 0.000438\n",
      "[DEBUG] Loss this iteration: 0.000303\n",
      "[DEBUG] Loss this iteration: 0.000250\n",
      "[DEBUG] Loss this iteration: 0.000278\n",
      "[DEBUG] Loss this iteration: 0.000238\n",
      "[DEBUG] Loss this iteration: 0.000233\n",
      "[DEBUG] Loss this iteration: 0.000274\n",
      "[DEBUG] Loss this iteration: 0.000246\n",
      "[DEBUG] Loss this iteration: 0.000244\n",
      "[DEBUG] Loss this iteration: 0.000296\n",
      "[DEBUG] Loss this iteration: 0.000445\n",
      "[DEBUG] Loss this iteration: 0.000673\n",
      "[DEBUG] Loss this iteration: 0.000489\n",
      "[DEBUG] Loss this iteration: 0.000282\n",
      "[DEBUG] Loss this iteration: 0.000314\n",
      "[DEBUG] Loss this iteration: 0.000280\n",
      "[DEBUG] Loss this iteration: 0.000291\n",
      "[DEBUG] Loss this iteration: 0.000379\n",
      "[DEBUG] Loss this iteration: 0.000447\n",
      "[DEBUG] Loss this iteration: 0.000397\n",
      "[DEBUG] Loss this iteration: 0.000200\n",
      "[DEBUG] Loss this iteration: 0.000223\n",
      "[DEBUG] Loss this iteration: 0.000206\n",
      "[DEBUG] Loss this iteration: 0.000212\n",
      "[DEBUG] Loss this iteration: 0.000212\n",
      "[DEBUG] Loss this iteration: 0.000199\n",
      "[DEBUG] Loss this iteration: 0.000240\n",
      "[DEBUG] Loss this iteration: 0.000292\n",
      "[DEBUG] Loss this iteration: 0.000444\n",
      "[DEBUG] Loss this iteration: 0.000551\n",
      "[DEBUG] Loss this iteration: 0.000394\n",
      "[DEBUG] Loss this iteration: 0.000219\n",
      "[DEBUG] Loss this iteration: 0.000299\n",
      "[DEBUG] Loss this iteration: 0.000247\n",
      "[DEBUG] Loss this iteration: 0.000307\n",
      "[DEBUG] Loss this iteration: 0.000472\n",
      "[DEBUG] Loss this iteration: 0.000528\n",
      "[DEBUG] Loss this iteration: 0.000342\n",
      "[DEBUG] Loss this iteration: 0.000223\n",
      "[DEBUG] Loss this iteration: 0.000217\n",
      "[DEBUG] Loss this iteration: 0.000231\n",
      "[DEBUG] Loss this iteration: 0.000226\n",
      "[DEBUG] Loss this iteration: 0.000236\n",
      "[DEBUG] Loss this iteration: 0.000231\n",
      "[DEBUG] Loss this iteration: 0.000245\n",
      "[DEBUG] Loss this iteration: 0.000249\n",
      "[DEBUG] Loss this iteration: 0.000257\n",
      "[DEBUG] Loss this iteration: 0.000353\n",
      "[DEBUG] Loss this iteration: 0.000370\n",
      "[DEBUG] Loss this iteration: 0.000297\n",
      "[DEBUG] Loss this iteration: 0.000299\n",
      "[DEBUG] Loss this iteration: 0.000335\n",
      "[DEBUG] Loss this iteration: 0.000300\n",
      "[DEBUG] Loss this iteration: 0.000299\n",
      "[DEBUG] Loss this iteration: 0.000277\n",
      "[DEBUG] Loss this iteration: 0.000288\n",
      "[DEBUG] Loss this iteration: 0.000270\n",
      "[DEBUG] Loss this iteration: 0.000278\n",
      "[DEBUG] Loss this iteration: 0.000284\n",
      "[DEBUG] Loss this iteration: 0.000317\n",
      "[DEBUG] Loss this iteration: 0.000335\n",
      "[DEBUG] Loss this iteration: 0.000292\n",
      "[DEBUG] Loss this iteration: 0.000262\n",
      "[DEBUG] Loss this iteration: 0.000203\n",
      "[DEBUG] Loss this iteration: 0.000188\n",
      "[DEBUG] Loss this iteration: 0.000220\n",
      "[DEBUG] Loss this iteration: 0.000223\n",
      "[DEBUG] Loss this iteration: 0.000199\n",
      "[DEBUG] Loss this iteration: 0.000246\n",
      "[DEBUG] Loss this iteration: 0.000480\n",
      "[DEBUG] Loss this iteration: 0.000741\n",
      "[DEBUG] Loss this iteration: 0.000394\n",
      "[DEBUG] Loss this iteration: 0.000353\n",
      "[DEBUG] Loss this iteration: 0.000402\n",
      "[DEBUG] Loss this iteration: 0.000341\n",
      "[DEBUG] Loss this iteration: 0.000214\n",
      "[DEBUG] Loss this iteration: 0.000193\n",
      "[DEBUG] Loss this iteration: 0.000203\n",
      "[DEBUG] Loss this iteration: 0.000209\n",
      "[DEBUG] Loss this iteration: 0.000226\n",
      "[ER] Iter 300/500 | ER Loss = 0.000226\n",
      "[DEBUG] Loss this iteration: 0.000231\n",
      "[DEBUG] Loss this iteration: 0.000245\n",
      "[DEBUG] Loss this iteration: 0.000241\n",
      "[DEBUG] Loss this iteration: 0.000224\n",
      "[DEBUG] Loss this iteration: 0.000228\n",
      "[DEBUG] Loss this iteration: 0.000252\n",
      "[DEBUG] Loss this iteration: 0.000255\n",
      "[DEBUG] Loss this iteration: 0.000282\n",
      "[DEBUG] Loss this iteration: 0.000223\n",
      "[DEBUG] Loss this iteration: 0.000191\n",
      "[DEBUG] Loss this iteration: 0.000196\n",
      "[DEBUG] Loss this iteration: 0.000264\n",
      "[DEBUG] Loss this iteration: 0.000460\n",
      "[DEBUG] Loss this iteration: 0.000471\n",
      "[DEBUG] Loss this iteration: 0.000216\n",
      "[DEBUG] Loss this iteration: 0.000192\n",
      "[DEBUG] Loss this iteration: 0.000266\n",
      "[DEBUG] Loss this iteration: 0.000280\n",
      "[DEBUG] Loss this iteration: 0.000372\n",
      "[DEBUG] Loss this iteration: 0.000313\n",
      "[DEBUG] Loss this iteration: 0.000253\n",
      "[DEBUG] Loss this iteration: 0.000212\n",
      "[DEBUG] Loss this iteration: 0.000228\n",
      "[DEBUG] Loss this iteration: 0.000271\n",
      "[DEBUG] Loss this iteration: 0.000231\n",
      "[DEBUG] Loss this iteration: 0.000215\n",
      "[DEBUG] Loss this iteration: 0.000184\n",
      "[DEBUG] Loss this iteration: 0.000210\n",
      "[DEBUG] Loss this iteration: 0.000244\n",
      "[DEBUG] Loss this iteration: 0.000387\n",
      "[DEBUG] Loss this iteration: 0.000460\n",
      "[DEBUG] Loss this iteration: 0.000237\n",
      "[DEBUG] Loss this iteration: 0.000266\n",
      "[DEBUG] Loss this iteration: 0.000303\n",
      "[DEBUG] Loss this iteration: 0.000240\n",
      "[DEBUG] Loss this iteration: 0.000230\n",
      "[DEBUG] Loss this iteration: 0.000216\n",
      "[DEBUG] Loss this iteration: 0.000228\n",
      "[DEBUG] Loss this iteration: 0.000242\n",
      "[DEBUG] Loss this iteration: 0.000224\n",
      "[DEBUG] Loss this iteration: 0.000221\n",
      "[DEBUG] Loss this iteration: 0.000225\n",
      "[DEBUG] Loss this iteration: 0.000251\n",
      "[DEBUG] Loss this iteration: 0.000281\n",
      "[DEBUG] Loss this iteration: 0.000234\n",
      "[DEBUG] Loss this iteration: 0.000215\n",
      "[DEBUG] Loss this iteration: 0.000251\n",
      "[DEBUG] Loss this iteration: 0.000396\n",
      "[DEBUG] Loss this iteration: 0.000480\n",
      "[DEBUG] Loss this iteration: 0.000208\n",
      "[DEBUG] Loss this iteration: 0.000248\n",
      "[DEBUG] Loss this iteration: 0.000231\n",
      "[DEBUG] Loss this iteration: 0.000183\n",
      "[DEBUG] Loss this iteration: 0.000192\n",
      "[DEBUG] Loss this iteration: 0.000194\n",
      "[DEBUG] Loss this iteration: 0.000232\n",
      "[DEBUG] Loss this iteration: 0.000312\n",
      "[DEBUG] Loss this iteration: 0.000413\n",
      "[DEBUG] Loss this iteration: 0.000268\n",
      "[DEBUG] Loss this iteration: 0.000184\n",
      "[DEBUG] Loss this iteration: 0.000199\n",
      "[DEBUG] Loss this iteration: 0.000206\n",
      "[DEBUG] Loss this iteration: 0.000201\n",
      "[DEBUG] Loss this iteration: 0.000235\n",
      "[DEBUG] Loss this iteration: 0.000265\n",
      "[DEBUG] Loss this iteration: 0.000326\n",
      "[DEBUG] Loss this iteration: 0.000253\n",
      "[DEBUG] Loss this iteration: 0.000234\n",
      "[DEBUG] Loss this iteration: 0.000224\n",
      "[DEBUG] Loss this iteration: 0.000310\n",
      "[DEBUG] Loss this iteration: 0.000475\n",
      "[DEBUG] Loss this iteration: 0.000225\n",
      "[DEBUG] Loss this iteration: 0.000178\n",
      "[DEBUG] Loss this iteration: 0.000167\n",
      "[DEBUG] Loss this iteration: 0.000203\n",
      "[DEBUG] Loss this iteration: 0.000197\n",
      "[DEBUG] Loss this iteration: 0.000282\n",
      "[DEBUG] Loss this iteration: 0.000407\n",
      "[DEBUG] Loss this iteration: 0.000356\n",
      "[DEBUG] Loss this iteration: 0.000234\n",
      "[DEBUG] Loss this iteration: 0.000220\n",
      "[DEBUG] Loss this iteration: 0.000263\n",
      "[DEBUG] Loss this iteration: 0.000238\n",
      "[DEBUG] Loss this iteration: 0.000249\n",
      "[DEBUG] Loss this iteration: 0.000325\n",
      "[DEBUG] Loss this iteration: 0.000320\n",
      "[DEBUG] Loss this iteration: 0.000218\n",
      "[DEBUG] Loss this iteration: 0.000177\n",
      "[DEBUG] Loss this iteration: 0.000173\n",
      "[DEBUG] Loss this iteration: 0.000190\n",
      "[DEBUG] Loss this iteration: 0.000190\n",
      "[DEBUG] Loss this iteration: 0.000221\n",
      "[DEBUG] Loss this iteration: 0.000255\n",
      "[DEBUG] Loss this iteration: 0.000271\n",
      "[DEBUG] Loss this iteration: 0.000278\n",
      "[DEBUG] Loss this iteration: 0.000226\n",
      "[DEBUG] Loss this iteration: 0.000235\n",
      "[DEBUG] Loss this iteration: 0.000182\n",
      "[DEBUG] Loss this iteration: 0.000183\n",
      "[DEBUG] Loss this iteration: 0.000175\n",
      "[ER] Iter 400/500 | ER Loss = 0.000175\n",
      "[DEBUG] Loss this iteration: 0.000199\n",
      "[DEBUG] Loss this iteration: 0.000244\n",
      "[DEBUG] Loss this iteration: 0.000322\n",
      "[DEBUG] Loss this iteration: 0.000348\n",
      "[DEBUG] Loss this iteration: 0.000215\n",
      "[DEBUG] Loss this iteration: 0.000208\n",
      "[DEBUG] Loss this iteration: 0.000240\n",
      "[DEBUG] Loss this iteration: 0.000251\n",
      "[DEBUG] Loss this iteration: 0.000221\n",
      "[DEBUG] Loss this iteration: 0.000225\n",
      "[DEBUG] Loss this iteration: 0.000244\n",
      "[DEBUG] Loss this iteration: 0.000203\n",
      "[DEBUG] Loss this iteration: 0.000165\n",
      "[DEBUG] Loss this iteration: 0.000157\n",
      "[DEBUG] Loss this iteration: 0.000168\n",
      "[DEBUG] Loss this iteration: 0.000178\n",
      "[DEBUG] Loss this iteration: 0.000231\n",
      "[DEBUG] Loss this iteration: 0.000307\n",
      "[DEBUG] Loss this iteration: 0.000325\n",
      "[DEBUG] Loss this iteration: 0.000246\n",
      "[DEBUG] Loss this iteration: 0.000210\n",
      "[DEBUG] Loss this iteration: 0.000207\n",
      "[DEBUG] Loss this iteration: 0.000259\n",
      "[DEBUG] Loss this iteration: 0.000296\n",
      "[DEBUG] Loss this iteration: 0.000368\n",
      "[DEBUG] Loss this iteration: 0.000288\n",
      "[DEBUG] Loss this iteration: 0.000194\n",
      "[DEBUG] Loss this iteration: 0.000176\n",
      "[DEBUG] Loss this iteration: 0.000205\n",
      "[DEBUG] Loss this iteration: 0.000198\n",
      "[DEBUG] Loss this iteration: 0.000176\n",
      "[DEBUG] Loss this iteration: 0.000203\n",
      "[DEBUG] Loss this iteration: 0.000245\n",
      "[DEBUG] Loss this iteration: 0.000250\n",
      "[DEBUG] Loss this iteration: 0.000224\n",
      "[DEBUG] Loss this iteration: 0.000177\n",
      "[DEBUG] Loss this iteration: 0.000184\n",
      "[DEBUG] Loss this iteration: 0.000230\n",
      "[DEBUG] Loss this iteration: 0.000246\n",
      "[DEBUG] Loss this iteration: 0.000372\n",
      "[DEBUG] Loss this iteration: 0.000358\n",
      "[DEBUG] Loss this iteration: 0.000215\n",
      "[DEBUG] Loss this iteration: 0.000237\n",
      "[DEBUG] Loss this iteration: 0.000221\n",
      "[DEBUG] Loss this iteration: 0.000188\n",
      "[DEBUG] Loss this iteration: 0.000175\n",
      "[DEBUG] Loss this iteration: 0.000155\n",
      "[DEBUG] Loss this iteration: 0.000153\n",
      "[DEBUG] Loss this iteration: 0.000173\n",
      "[DEBUG] Loss this iteration: 0.000175\n",
      "[DEBUG] Loss this iteration: 0.000194\n",
      "[DEBUG] Loss this iteration: 0.000414\n",
      "[DEBUG] Loss this iteration: 0.000581\n",
      "[DEBUG] Loss this iteration: 0.000246\n",
      "[DEBUG] Loss this iteration: 0.000215\n",
      "[DEBUG] Loss this iteration: 0.000226\n",
      "[DEBUG] Loss this iteration: 0.000189\n",
      "[DEBUG] Loss this iteration: 0.000179\n",
      "[DEBUG] Loss this iteration: 0.000170\n",
      "[DEBUG] Loss this iteration: 0.000159\n",
      "[DEBUG] Loss this iteration: 0.000163\n",
      "[DEBUG] Loss this iteration: 0.000177\n",
      "[DEBUG] Loss this iteration: 0.000184\n",
      "[DEBUG] Loss this iteration: 0.000189\n",
      "[DEBUG] Loss this iteration: 0.000241\n",
      "[DEBUG] Loss this iteration: 0.000305\n",
      "[DEBUG] Loss this iteration: 0.000289\n",
      "[DEBUG] Loss this iteration: 0.000216\n",
      "[DEBUG] Loss this iteration: 0.000195\n",
      "[DEBUG] Loss this iteration: 0.000215\n",
      "[DEBUG] Loss this iteration: 0.000220\n",
      "[DEBUG] Loss this iteration: 0.000204\n",
      "[DEBUG] Loss this iteration: 0.000235\n",
      "[DEBUG] Loss this iteration: 0.000218\n",
      "[DEBUG] Loss this iteration: 0.000220\n",
      "[DEBUG] Loss this iteration: 0.000168\n",
      "[DEBUG] Loss this iteration: 0.000160\n",
      "[DEBUG] Loss this iteration: 0.000165\n",
      "[DEBUG] Loss this iteration: 0.000221\n",
      "[DEBUG] Loss this iteration: 0.000329\n",
      "[DEBUG] Loss this iteration: 0.000291\n",
      "[DEBUG] Loss this iteration: 0.000187\n",
      "[DEBUG] Loss this iteration: 0.000171\n",
      "[DEBUG] Loss this iteration: 0.000184\n",
      "[DEBUG] Loss this iteration: 0.000209\n",
      "[DEBUG] Loss this iteration: 0.000266\n",
      "[DEBUG] Loss this iteration: 0.000306\n",
      "[DEBUG] Loss this iteration: 0.000214\n",
      "[DEBUG] Loss this iteration: 0.000181\n",
      "[DEBUG] Loss this iteration: 0.000179\n",
      "[DEBUG] Loss this iteration: 0.000158\n",
      "[DEBUG] Loss this iteration: 0.000159\n",
      "[DEBUG] Loss this iteration: 0.000187\n",
      "[DEBUG] Loss this iteration: 0.000278\n",
      "[DEBUG] Loss this iteration: 0.000310\n",
      "[DEBUG] Loss this iteration: 0.000267\n",
      "[DEBUG] Loss this iteration: 0.000195\n",
      "[DEBUG] Loss this iteration: 0.000236\n",
      "[DEBUG] Loss this iteration: 0.000222\n",
      "=== PRETRAINING: Supervisor ===\n",
      "Loss S:  tensor(1.3104, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "[S] Iter 0/500 | Supervisor Loss = 1.310388\n",
      "Loss S:  tensor(1.3032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(1.2795, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(1.2032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(1.1610, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(1.1324, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(1.0757, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(1.0420, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(1.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.9727, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.9685, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.9080, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.8884, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.8528, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.8213, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.8184, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.7770, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.7723, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.7497, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.7249, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.6785, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.6539, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.6604, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.6201, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.5896, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.5546, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.5872, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.5268, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.5613, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.5253, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.5354, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4831, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4672, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4340, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4364, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4303, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4133, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4252, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3984, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3679, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3634, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3556, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3782, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3334, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3268, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3290, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3361, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3283, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3304, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2983, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2931, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2724, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2672, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2862, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2691, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2697, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2706, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2701, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2641, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2675, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2407, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2541, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2390, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2443, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2379, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2480, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2397, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2255, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2221, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2212, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2207, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2181, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2224, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2063, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2062, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2080, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2120, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2083, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2092, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1980, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1990, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1937, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1920, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1885, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1797, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1907, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1864, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1806, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1776, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1804, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1758, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1746, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1739, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1733, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1745, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1697, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1707, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "[S] Iter 100/500 | Supervisor Loss = 0.170708\n",
      "Loss S:  tensor(0.1653, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1620, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1628, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1649, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1623, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1639, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1599, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1545, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1545, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1519, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1535, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1508, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1526, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1476, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1497, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1480, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1439, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1449, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1445, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1442, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1445, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1421, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1394, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1392, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1395, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1380, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1379, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1343, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1342, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1335, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1338, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1312, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1338, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1297, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1306, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1293, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1304, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1254, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1279, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1259, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1249, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1247, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1241, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1222, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1232, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1252, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1209, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1211, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1211, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1203, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1194, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1210, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1177, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1197, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1194, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1171, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1174, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1156, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1177, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1144, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1164, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1130, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1130, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1121, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1145, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1116, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1131, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1108, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1099, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1103, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1103, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1093, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1097, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1082, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1090, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1060, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1068, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1077, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1056, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1069, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1053, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1062, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1053, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1065, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1043, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1043, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0997, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0994, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "[S] Iter 200/500 | Supervisor Loss = 0.099381\n",
      "Loss S:  tensor(0.1007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0976, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0992, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0994, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0979, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0979, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0981, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0982, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0974, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0982, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0967, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0956, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0970, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0973, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0959, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0974, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0963, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0972, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0950, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0957, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0946, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0956, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0932, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0945, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0938, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0946, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0940, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0946, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0925, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0951, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0932, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0933, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0915, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0907, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0922, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0918, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0920, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0915, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0923, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0915, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0906, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0916, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0907, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0907, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0919, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0913, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0901, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0904, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0889, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0906, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0886, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0877, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0900, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0886, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0873, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0900, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0887, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0881, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0882, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0884, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0868, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0883, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0869, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0880, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0882, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0878, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0878, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0873, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0859, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0876, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0871, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0863, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0859, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0877, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0870, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0869, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0864, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0851, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0840, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0871, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0860, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0853, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0836, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0855, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0863, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0847, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0838, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0851, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0838, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0848, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0836, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0840, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0841, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0825, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0849, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0839, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0849, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0824, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0849, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "[S] Iter 300/500 | Supervisor Loss = 0.084914\n",
      "Loss S:  tensor(0.0843, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0833, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0855, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0829, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0855, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0829, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0839, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0829, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0839, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0825, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0826, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0820, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0839, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0836, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0826, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0817, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0822, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0814, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0833, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0820, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0828, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0813, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0817, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0816, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0832, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0821, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0820, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0829, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0832, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0833, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0822, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0809, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0804, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0823, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0824, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0809, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0823, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0801, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0832, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0809, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0822, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0806, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0810, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0807, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0815, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0801, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0811, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0792, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0822, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0803, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0789, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0805, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0803, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0805, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0809, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0790, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0807, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0802, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0811, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0803, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0801, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0802, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0796, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0803, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0801, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0799, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0801, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0802, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0794, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0794, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0805, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0781, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0797, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0794, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0785, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0790, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0810, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0797, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0800, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0780, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0770, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0775, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0782, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0795, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0784, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0778, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0787, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0791, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0797, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0788, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0791, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0780, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0779, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0782, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0789, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0781, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0796, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0783, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0789, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0785, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "[S] Iter 400/500 | Supervisor Loss = 0.078491\n",
      "Loss S:  tensor(0.0776, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0783, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0781, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0785, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0793, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0782, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0802, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0784, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0788, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0762, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0775, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0781, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0775, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0766, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0782, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0775, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0784, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0780, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0752, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0765, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0762, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0777, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0780, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0765, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0795, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0790, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0775, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0768, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0769, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0777, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0773, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0779, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0777, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0760, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0763, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0782, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0772, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0771, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0770, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0773, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0775, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0763, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0783, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0788, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0760, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0771, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0763, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0767, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0777, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0772, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0764, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0779, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0771, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0777, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0765, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0778, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0769, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0780, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0764, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0766, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0762, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0773, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0766, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0787, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0762, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0771, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0761, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0759, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0765, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0754, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0753, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0756, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0748, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0752, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0767, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0759, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0763, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0751, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0767, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0745, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0757, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0758, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0777, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0774, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0763, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0741, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0749, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0751, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0759, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0749, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0757, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0775, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0765, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0770, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0757, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0755, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0764, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0766, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.0748, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "=== ADVERSARIAL TRAINING (WGAN-GP) ===\n",
      "Loss G (total):  tensor(2.6357, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "----------------------------------------------\n",
      "[WGAN] Iter 0/500\n",
      "  D Loss = 5723.227051\n",
      "  G Loss = 2.635663\n",
      "----------------------------------------------\n",
      "Loss G (total):  tensor(3.1224, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(3.4761, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(3.7670, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(3.8736, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(3.9891, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.1118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.1416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.2062, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.2559, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.2529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.3062, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.3079, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.3129, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.3304, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.3514, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.3682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.3910, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.3804, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.4469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.5012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.4727, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.5040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.5523, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.5779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.6369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.7058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.7768, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.8714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.9888, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(5.1606, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(5.3211, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(5.5198, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(5.7694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(6.0588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(6.3848, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(6.7404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(7.1019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(7.5258, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(7.9535, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(8.3878, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(8.7617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(9.1825, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(9.5942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(10.0386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(10.4481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(10.8589, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(11.2999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(11.6835, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(12.0550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(12.4707, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(12.8549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(13.2481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(13.6612, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(13.9922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(14.3650, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(14.7373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(15.0991, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(15.4566, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(15.8013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(16.1365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(16.4387, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(16.7673, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(17.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(17.3880, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(17.6865, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(17.9829, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(18.2707, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(18.5454, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(18.8286, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(19.1020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(19.3776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(19.6353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(19.9189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(20.1904, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(20.4461, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(20.7124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(20.9656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(21.2269, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(21.4767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(21.7307, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(21.9729, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(22.2199, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(22.4579, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(22.7079, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(22.9786, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(23.2101, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(23.4635, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(23.7228, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(23.9575, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(24.2153, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(24.4725, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(24.6948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(24.9448, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(25.2052, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(25.4528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(25.7007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(25.9591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(26.2112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(26.4813, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(26.7255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "----------------------------------------------\n",
      "[WGAN] Iter 100/500\n",
      "  D Loss = -13.418276\n",
      "  G Loss = 26.725519\n",
      "----------------------------------------------\n",
      "Loss G (total):  tensor(26.9598, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(27.2050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(27.4702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(27.7076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(27.9776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(28.2233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(28.4434, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(28.7210, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(28.9615, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(29.2388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(29.4681, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(29.7214, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(29.9984, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(30.2437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(30.5058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(30.7894, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(31.0385, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(31.3164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(31.5489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(31.8076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(32.0597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(32.3401, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(32.5794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(32.8479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(33.1138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(33.3929, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(33.6485, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(33.9055, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(34.1504, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(34.4224, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(34.7008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(34.9553, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(35.2398, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(35.5134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(35.7809, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(36.0488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(36.3170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(36.6040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(36.8783, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(37.1312, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(37.4035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(37.6888, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(37.9537, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(38.2251, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(38.4908, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(38.7833, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(39.0201, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(39.2902, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(39.5614, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(39.8303, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(40.1431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(40.3826, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(40.6818, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(40.9437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(41.2311, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(41.5150, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(41.7788, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(42.0639, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(42.3743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(42.6040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(42.8835, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(43.1803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(43.4665, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(43.7265, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(44.0100, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(44.2718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(44.5484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(44.8135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(45.0979, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(45.3754, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(45.6536, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(45.9785, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(46.2511, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(46.5187, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(46.7931, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(47.0842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(47.3644, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(47.6502, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(47.9343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(48.2256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(48.4925, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(48.7776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(49.0942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(49.3290, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(49.6388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(49.8841, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(50.1965, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(50.4590, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(50.7569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(51.0325, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(51.3084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(51.6018, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(51.9157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(52.1732, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(52.4714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(52.7527, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(53.0330, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(53.3448, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(53.6229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(53.9205, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "----------------------------------------------\n",
      "[WGAN] Iter 200/500\n",
      "  D Loss = -26.981888\n",
      "  G Loss = 53.920490\n",
      "----------------------------------------------\n",
      "Loss G (total):  tensor(54.2006, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(54.4628, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(54.7550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(55.0415, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(55.3274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(55.5968, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(55.9338, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(56.1863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(56.4734, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(56.7831, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(57.0421, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(57.3547, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(57.6242, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(57.9125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(58.2134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(58.5108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(58.8407, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(59.0853, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(59.4035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(59.6743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(59.9803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(60.2414, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(60.5472, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(60.8396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(61.1299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(61.3903, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(61.6787, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(61.9757, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(62.2957, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(62.5736, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(62.8462, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(63.1404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(63.4378, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(63.7338, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(64.0407, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(64.3347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(64.6164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(64.9052, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(65.2115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(65.4942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(65.7820, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(66.0845, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(66.3566, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(66.6482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(66.9570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(67.2158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(67.5238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(67.8220, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(68.1097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(68.3848, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(68.6652, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(68.9677, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(69.2921, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(69.5705, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(69.8539, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(70.1364, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(70.4287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(70.7352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(71.0399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(71.3109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(71.5998, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(71.9041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(72.1979, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(72.4818, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(72.8028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(73.0571, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(73.3517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(73.6284, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(73.9249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(74.2158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(74.5057, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(74.7804, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(75.1073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(75.3656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(75.6954, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(75.9703, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(76.2591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(76.5636, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(76.8535, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(77.1308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(77.4301, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(77.7154, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(78.0505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(78.3232, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(78.6062, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(78.8651, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(79.1551, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(79.4495, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(79.7399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(80.0265, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(80.3112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(80.6420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(80.9123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(81.1994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(81.4928, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(81.7724, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(82.0859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(82.3866, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(82.6931, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(82.9456, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "----------------------------------------------\n",
      "[WGAN] Iter 300/500\n",
      "  D Loss = -41.566463\n",
      "  G Loss = 82.945610\n",
      "----------------------------------------------\n",
      "Loss G (total):  tensor(83.2428, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(83.5221, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(83.8073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(84.1178, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(84.4161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(84.6671, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(84.9613, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(85.2871, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(85.5192, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(85.8120, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(86.1360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(86.3936, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(86.6952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(86.9783, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(87.2708, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(87.5860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(87.8646, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(88.1350, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(88.4375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(88.7352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(88.9966, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(89.2913, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(89.5853, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(89.8728, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(90.1624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(90.4432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(90.7228, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(91.0286, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(91.2879, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(91.5707, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(91.8696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(92.1528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(92.4259, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(92.7195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(93.0516, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(93.2996, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(93.5848, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(93.8783, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(94.1648, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(94.4584, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(94.7929, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(95.0368, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(95.2930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(95.5661, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(95.8648, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(96.1618, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(96.4123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(96.7607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(97.0093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(97.2729, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(97.5427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(97.8228, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(98.1115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(98.3792, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(98.6829, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(98.9742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(99.2528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(99.5396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(99.8506, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(100.1165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(100.4049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(100.6499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(100.9261, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(101.2025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(101.5095, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(101.7892, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(102.0818, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(102.3098, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(102.6130, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(102.8730, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(103.1938, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(103.4248, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(103.7636, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(103.9990, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(104.3096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(104.6171, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(104.8602, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(105.1756, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(105.4086, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(105.7318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(105.9795, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(106.2218, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(106.4951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(106.7595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(107.0676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(107.3349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(107.6121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(107.8955, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(108.1591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(108.4299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(108.6755, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(108.9616, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(109.2307, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(109.5344, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(109.7860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(110.1018, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(110.3483, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(110.6355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(110.8922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(111.1672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "----------------------------------------------\n",
      "[WGAN] Iter 400/500\n",
      "  D Loss = -55.691463\n",
      "  G Loss = 111.167198\n",
      "----------------------------------------------\n",
      "Loss G (total):  tensor(111.4488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(111.7060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(111.9796, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(112.2301, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(112.5107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(112.7864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(113.0497, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(113.3220, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(113.5976, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(113.8712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(114.1373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(114.4114, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(114.7021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(114.9529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(115.2338, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(115.4919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(115.7811, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(116.0521, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(116.3282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(116.6208, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(116.8779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(117.0998, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(117.3704, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(117.6967, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(117.8963, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(118.1858, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(118.4328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(118.7021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(118.9680, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(119.2407, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(119.5348, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(119.7786, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(120.0229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(120.2878, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(120.5728, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(120.8583, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(121.1010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(121.4012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(121.6549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(121.8914, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(122.1712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(122.4527, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(122.6771, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(122.9358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(123.1903, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(123.4484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(123.6974, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(123.9926, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(124.2312, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(124.5015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(124.7426, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(125.0175, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(125.2735, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(125.5430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(125.8004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(126.0352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(126.2982, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(126.5840, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(126.8558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(127.0909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(127.3481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(127.6132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(127.8548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(128.1066, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(128.3543, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(128.5931, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(128.8528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(129.1093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(129.3554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(129.6394, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(129.8857, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(130.1289, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(130.3802, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(130.6448, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(130.9313, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(131.1740, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(131.4138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(131.6701, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(131.9255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(132.1896, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(132.4208, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(132.6663, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(132.9232, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(133.1737, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(133.4127, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(133.6616, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(133.9264, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(134.1836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(134.4119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(134.6603, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(134.9198, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(135.1586, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(135.4118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(135.6693, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(135.9221, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(136.1824, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(136.4187, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(136.6659, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(136.9061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Finish Synthetic Data Generation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from options_TGAN import Options\n",
    "from lib.TimeGAN import TimeGAN\n",
    "\n",
    "# 1. Options\n",
    "opt = Options().parse()\n",
    "\n",
    "\n",
    "opt.print_freq = 100    # imprime cada iteraciÃ³n\n",
    "\n",
    "# 2. Set paper-style hyperparameters\n",
    "opt.lr_g = 1e-4     # LR para el generador\n",
    "opt.lr_d = 5e-4    # LR para el discriminador\n",
    "opt.lr_e = 1e-3     # encoder\n",
    "opt.lr_r = 1e-3     # recovery\n",
    "opt.lr_s = 1e-3     # supervisor\n",
    "\n",
    "\n",
    "opt.beta1 = 0.5\n",
    "\n",
    "opt.batch_size = 64\n",
    "opt.iteration = 500\n",
    "\n",
    "opt.hidden_dim = 32   # muy importante\n",
    "opt.num_layer = 4\n",
    "\n",
    "opt.n_critic = 5      # OK\n",
    "opt.gp_lambda = 10   # OK\n",
    "opt.name = \"TimeGAN_real_paper_settings\"\n",
    "\n",
    "opt.w_g    = 1.0   # antes 80\n",
    "opt.w_e0   = 1.0   # antes 10\n",
    "opt.w_es   = 1.0   # antes 0.1, lo dejamos en la misma escala\n",
    "opt.w_gamma = 1.0  # este se puede mantener\n",
    "\n",
    "\n",
    "# 3. Create model\n",
    "model = TimeGAN(opt, ori_data)\n",
    "\n",
    "# 4. Train\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ade89b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§© Generating 50 samples in 1 batches of 64...\n",
      "  âœ… Batch 1/1 generated (50 samples)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ€ Ejecutando t-SNE (puede tardar unos segundos)...\n",
      "[t-SNE] Computing 99 nearest neighbors...\n",
      "[t-SNE] Indexed 100 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 100 samples in 0.014s...\n",
      "[t-SNE] Computed conditional probabilities for sample 100 / 100\n",
      "[t-SNE] Mean sigma: 0.534282\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 51.030140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dario\\Desktop\\ThesiS JBP\\jordan_venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] KL divergence after 500 iterations: 0.053610\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from generation_TGAN import safe_generation\n",
    "import numpy as np\n",
    "\n",
    "generated_data = safe_generation(model, num_samples=50, batch_size=64)\n",
    "\n",
    "n_vis = 5000  # target\n",
    "\n",
    "n_real = min(n_vis, len(ori_data))\n",
    "n_fake = min(n_vis, len(generated_data))\n",
    "\n",
    "idx_real = np.random.choice(len(ori_data), n_real, replace=False)\n",
    "idx_fake = np.random.choice(len(generated_data), n_fake, replace=False)\n",
    "\n",
    "ori_vis = [ori_data[i] for i in idx_real]\n",
    "gen_vis = [generated_data[i] for i in idx_fake]\n",
    "\n",
    "from visualization_TGAN import visualization\n",
    "\n",
    "visualization(ori_vis, gen_vis, 'pca')\n",
    "visualization(ori_vis, gen_vis, 'tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e403be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Auto-set z_dim = 7 (feature size from data)\n",
      "=== PRETRAINING: Embedding + Recovery ===\n",
      "[DEBUG] Loss this iteration: 0.075309\n",
      "[ER] Iter 0/500 | ER Loss = 0.075309\n",
      "[DEBUG] Loss this iteration: 0.058950\n",
      "[DEBUG] Loss this iteration: 0.052088\n",
      "[DEBUG] Loss this iteration: 0.050860\n",
      "[DEBUG] Loss this iteration: 0.041472\n",
      "[DEBUG] Loss this iteration: 0.039616\n",
      "[DEBUG] Loss this iteration: 0.036590\n",
      "[DEBUG] Loss this iteration: 0.032616\n",
      "[DEBUG] Loss this iteration: 0.029282\n",
      "[DEBUG] Loss this iteration: 0.028659\n",
      "[DEBUG] Loss this iteration: 0.028478\n",
      "[DEBUG] Loss this iteration: 0.026280\n",
      "[DEBUG] Loss this iteration: 0.025607\n",
      "[DEBUG] Loss this iteration: 0.022331\n",
      "[DEBUG] Loss this iteration: 0.023095\n",
      "[DEBUG] Loss this iteration: 0.022438\n",
      "[DEBUG] Loss this iteration: 0.020097\n",
      "[DEBUG] Loss this iteration: 0.018281\n",
      "[DEBUG] Loss this iteration: 0.018367\n",
      "[DEBUG] Loss this iteration: 0.017852\n",
      "[DEBUG] Loss this iteration: 0.017331\n",
      "[DEBUG] Loss this iteration: 0.016546\n",
      "[DEBUG] Loss this iteration: 0.015876\n",
      "[DEBUG] Loss this iteration: 0.017097\n",
      "[DEBUG] Loss this iteration: 0.017944\n",
      "[DEBUG] Loss this iteration: 0.015511\n",
      "[DEBUG] Loss this iteration: 0.014860\n",
      "[DEBUG] Loss this iteration: 0.014078\n",
      "[DEBUG] Loss this iteration: 0.014495\n",
      "[DEBUG] Loss this iteration: 0.013444\n",
      "[DEBUG] Loss this iteration: 0.013618\n",
      "[DEBUG] Loss this iteration: 0.014444\n",
      "[DEBUG] Loss this iteration: 0.013632\n",
      "[DEBUG] Loss this iteration: 0.013145\n",
      "[DEBUG] Loss this iteration: 0.012800\n",
      "[DEBUG] Loss this iteration: 0.013405\n",
      "[DEBUG] Loss this iteration: 0.012561\n",
      "[DEBUG] Loss this iteration: 0.011720\n",
      "[DEBUG] Loss this iteration: 0.013011\n",
      "[DEBUG] Loss this iteration: 0.012195\n",
      "[DEBUG] Loss this iteration: 0.011331\n",
      "[DEBUG] Loss this iteration: 0.011387\n",
      "[DEBUG] Loss this iteration: 0.011264\n",
      "[DEBUG] Loss this iteration: 0.011949\n",
      "[DEBUG] Loss this iteration: 0.011088\n",
      "[DEBUG] Loss this iteration: 0.012078\n",
      "[DEBUG] Loss this iteration: 0.012096\n",
      "[DEBUG] Loss this iteration: 0.011218\n",
      "[DEBUG] Loss this iteration: 0.010776\n",
      "[DEBUG] Loss this iteration: 0.009923\n",
      "[DEBUG] Loss this iteration: 0.010409\n",
      "[DEBUG] Loss this iteration: 0.010318\n",
      "[DEBUG] Loss this iteration: 0.010212\n",
      "[DEBUG] Loss this iteration: 0.011044\n",
      "[DEBUG] Loss this iteration: 0.010307\n",
      "[DEBUG] Loss this iteration: 0.009651\n",
      "[DEBUG] Loss this iteration: 0.009694\n",
      "[DEBUG] Loss this iteration: 0.009494\n",
      "[DEBUG] Loss this iteration: 0.009892\n",
      "[DEBUG] Loss this iteration: 0.010171\n",
      "[DEBUG] Loss this iteration: 0.010363\n",
      "[DEBUG] Loss this iteration: 0.009492\n",
      "[DEBUG] Loss this iteration: 0.009007\n",
      "[DEBUG] Loss this iteration: 0.008784\n",
      "[DEBUG] Loss this iteration: 0.008596\n",
      "[DEBUG] Loss this iteration: 0.009316\n",
      "[DEBUG] Loss this iteration: 0.009046\n",
      "[DEBUG] Loss this iteration: 0.009020\n",
      "[DEBUG] Loss this iteration: 0.008654\n",
      "[DEBUG] Loss this iteration: 0.009031\n",
      "[DEBUG] Loss this iteration: 0.009208\n",
      "[DEBUG] Loss this iteration: 0.009417\n",
      "[DEBUG] Loss this iteration: 0.008806\n",
      "[DEBUG] Loss this iteration: 0.008317\n",
      "[DEBUG] Loss this iteration: 0.008379\n",
      "[DEBUG] Loss this iteration: 0.009172\n",
      "[DEBUG] Loss this iteration: 0.008617\n",
      "[DEBUG] Loss this iteration: 0.007847\n",
      "[DEBUG] Loss this iteration: 0.007620\n",
      "[DEBUG] Loss this iteration: 0.007963\n",
      "[DEBUG] Loss this iteration: 0.007609\n",
      "[DEBUG] Loss this iteration: 0.007819\n",
      "[DEBUG] Loss this iteration: 0.007894\n",
      "[DEBUG] Loss this iteration: 0.007726\n",
      "[DEBUG] Loss this iteration: 0.007920\n",
      "[DEBUG] Loss this iteration: 0.007782\n",
      "[DEBUG] Loss this iteration: 0.008514\n",
      "[DEBUG] Loss this iteration: 0.007846\n",
      "[DEBUG] Loss this iteration: 0.007644\n",
      "[DEBUG] Loss this iteration: 0.007275\n",
      "[DEBUG] Loss this iteration: 0.007526\n",
      "[DEBUG] Loss this iteration: 0.007103\n",
      "[DEBUG] Loss this iteration: 0.007441\n",
      "[DEBUG] Loss this iteration: 0.006939\n",
      "[DEBUG] Loss this iteration: 0.007307\n",
      "[DEBUG] Loss this iteration: 0.008016\n",
      "[DEBUG] Loss this iteration: 0.007855\n",
      "[DEBUG] Loss this iteration: 0.007345\n",
      "[DEBUG] Loss this iteration: 0.007526\n",
      "[DEBUG] Loss this iteration: 0.007574\n",
      "[DEBUG] Loss this iteration: 0.006757\n",
      "[ER] Iter 100/500 | ER Loss = 0.006757\n",
      "[DEBUG] Loss this iteration: 0.007255\n",
      "[DEBUG] Loss this iteration: 0.007401\n",
      "[DEBUG] Loss this iteration: 0.007210\n",
      "[DEBUG] Loss this iteration: 0.006576\n",
      "[DEBUG] Loss this iteration: 0.006624\n",
      "[DEBUG] Loss this iteration: 0.006641\n",
      "[DEBUG] Loss this iteration: 0.006669\n",
      "[DEBUG] Loss this iteration: 0.006689\n",
      "[DEBUG] Loss this iteration: 0.006712\n",
      "[DEBUG] Loss this iteration: 0.006777\n",
      "[DEBUG] Loss this iteration: 0.006598\n",
      "[DEBUG] Loss this iteration: 0.007212\n",
      "[DEBUG] Loss this iteration: 0.006378\n",
      "[DEBUG] Loss this iteration: 0.006064\n",
      "[DEBUG] Loss this iteration: 0.006195\n",
      "[DEBUG] Loss this iteration: 0.006020\n",
      "[DEBUG] Loss this iteration: 0.006310\n",
      "[DEBUG] Loss this iteration: 0.006252\n",
      "[DEBUG] Loss this iteration: 0.006887\n",
      "[DEBUG] Loss this iteration: 0.006857\n",
      "[DEBUG] Loss this iteration: 0.006235\n",
      "[DEBUG] Loss this iteration: 0.006394\n",
      "[DEBUG] Loss this iteration: 0.006479\n",
      "[DEBUG] Loss this iteration: 0.006369\n",
      "[DEBUG] Loss this iteration: 0.006189\n",
      "[DEBUG] Loss this iteration: 0.006016\n",
      "[DEBUG] Loss this iteration: 0.006194\n",
      "[DEBUG] Loss this iteration: 0.006326\n",
      "[DEBUG] Loss this iteration: 0.005697\n",
      "[DEBUG] Loss this iteration: 0.005732\n",
      "[DEBUG] Loss this iteration: 0.005776\n",
      "[DEBUG] Loss this iteration: 0.006103\n",
      "[DEBUG] Loss this iteration: 0.006153\n",
      "[DEBUG] Loss this iteration: 0.005980\n",
      "[DEBUG] Loss this iteration: 0.005760\n",
      "[DEBUG] Loss this iteration: 0.005892\n",
      "[DEBUG] Loss this iteration: 0.005941\n",
      "[DEBUG] Loss this iteration: 0.005670\n",
      "[DEBUG] Loss this iteration: 0.005590\n",
      "[DEBUG] Loss this iteration: 0.005815\n",
      "[DEBUG] Loss this iteration: 0.006137\n",
      "[DEBUG] Loss this iteration: 0.005829\n",
      "[DEBUG] Loss this iteration: 0.005715\n",
      "[DEBUG] Loss this iteration: 0.005274\n",
      "[DEBUG] Loss this iteration: 0.005252\n",
      "[DEBUG] Loss this iteration: 0.005376\n",
      "[DEBUG] Loss this iteration: 0.005611\n",
      "[DEBUG] Loss this iteration: 0.005670\n",
      "[DEBUG] Loss this iteration: 0.005480\n",
      "[DEBUG] Loss this iteration: 0.005623\n",
      "[DEBUG] Loss this iteration: 0.005715\n",
      "[DEBUG] Loss this iteration: 0.005235\n",
      "[DEBUG] Loss this iteration: 0.005246\n",
      "[DEBUG] Loss this iteration: 0.005393\n",
      "[DEBUG] Loss this iteration: 0.004788\n",
      "[DEBUG] Loss this iteration: 0.005459\n",
      "[DEBUG] Loss this iteration: 0.006104\n",
      "[DEBUG] Loss this iteration: 0.006713\n",
      "[DEBUG] Loss this iteration: 0.005973\n",
      "[DEBUG] Loss this iteration: 0.005344\n",
      "[DEBUG] Loss this iteration: 0.005186\n",
      "[DEBUG] Loss this iteration: 0.005079\n",
      "[DEBUG] Loss this iteration: 0.005129\n",
      "[DEBUG] Loss this iteration: 0.005375\n",
      "[DEBUG] Loss this iteration: 0.004883\n",
      "[DEBUG] Loss this iteration: 0.004986\n",
      "[DEBUG] Loss this iteration: 0.004803\n",
      "[DEBUG] Loss this iteration: 0.005119\n",
      "[DEBUG] Loss this iteration: 0.005045\n",
      "[DEBUG] Loss this iteration: 0.004879\n",
      "[DEBUG] Loss this iteration: 0.005159\n",
      "[DEBUG] Loss this iteration: 0.004876\n",
      "[DEBUG] Loss this iteration: 0.004882\n",
      "[DEBUG] Loss this iteration: 0.005199\n",
      "[DEBUG] Loss this iteration: 0.005739\n",
      "[DEBUG] Loss this iteration: 0.005086\n",
      "[DEBUG] Loss this iteration: 0.004630\n",
      "[DEBUG] Loss this iteration: 0.005106\n",
      "[DEBUG] Loss this iteration: 0.004807\n",
      "[DEBUG] Loss this iteration: 0.004545\n",
      "[DEBUG] Loss this iteration: 0.004651\n",
      "[DEBUG] Loss this iteration: 0.004624\n",
      "[DEBUG] Loss this iteration: 0.004563\n",
      "[DEBUG] Loss this iteration: 0.004467\n",
      "[DEBUG] Loss this iteration: 0.004748\n",
      "[DEBUG] Loss this iteration: 0.005053\n",
      "[DEBUG] Loss this iteration: 0.005553\n",
      "[DEBUG] Loss this iteration: 0.004733\n",
      "[DEBUG] Loss this iteration: 0.004620\n",
      "[DEBUG] Loss this iteration: 0.004534\n",
      "[DEBUG] Loss this iteration: 0.004423\n",
      "[DEBUG] Loss this iteration: 0.004575\n",
      "[DEBUG] Loss this iteration: 0.004421\n",
      "[DEBUG] Loss this iteration: 0.004288\n",
      "[DEBUG] Loss this iteration: 0.004708\n",
      "[DEBUG] Loss this iteration: 0.005147\n",
      "[DEBUG] Loss this iteration: 0.004666\n",
      "[DEBUG] Loss this iteration: 0.004326\n",
      "[DEBUG] Loss this iteration: 0.004671\n",
      "[DEBUG] Loss this iteration: 0.004573\n",
      "[ER] Iter 200/500 | ER Loss = 0.004573\n",
      "[DEBUG] Loss this iteration: 0.004352\n",
      "[DEBUG] Loss this iteration: 0.004287\n",
      "[DEBUG] Loss this iteration: 0.004400\n",
      "[DEBUG] Loss this iteration: 0.004770\n",
      "[DEBUG] Loss this iteration: 0.005555\n",
      "[DEBUG] Loss this iteration: 0.004865\n",
      "[DEBUG] Loss this iteration: 0.004501\n",
      "[DEBUG] Loss this iteration: 0.004512\n",
      "[DEBUG] Loss this iteration: 0.004113\n",
      "[DEBUG] Loss this iteration: 0.003972\n",
      "[DEBUG] Loss this iteration: 0.004101\n",
      "[DEBUG] Loss this iteration: 0.003999\n",
      "[DEBUG] Loss this iteration: 0.004244\n",
      "[DEBUG] Loss this iteration: 0.004803\n",
      "[DEBUG] Loss this iteration: 0.005250\n",
      "[DEBUG] Loss this iteration: 0.004834\n",
      "[DEBUG] Loss this iteration: 0.004144\n",
      "[DEBUG] Loss this iteration: 0.004045\n",
      "[DEBUG] Loss this iteration: 0.004080\n",
      "[DEBUG] Loss this iteration: 0.004140\n",
      "[DEBUG] Loss this iteration: 0.004196\n",
      "[DEBUG] Loss this iteration: 0.003939\n",
      "[DEBUG] Loss this iteration: 0.003989\n",
      "[DEBUG] Loss this iteration: 0.004097\n",
      "[DEBUG] Loss this iteration: 0.004110\n",
      "[DEBUG] Loss this iteration: 0.004070\n",
      "[DEBUG] Loss this iteration: 0.004029\n",
      "[DEBUG] Loss this iteration: 0.004253\n",
      "[DEBUG] Loss this iteration: 0.004355\n",
      "[DEBUG] Loss this iteration: 0.004414\n",
      "[DEBUG] Loss this iteration: 0.004415\n",
      "[DEBUG] Loss this iteration: 0.004235\n",
      "[DEBUG] Loss this iteration: 0.004139\n",
      "[DEBUG] Loss this iteration: 0.003785\n",
      "[DEBUG] Loss this iteration: 0.003858\n",
      "[DEBUG] Loss this iteration: 0.003643\n",
      "[DEBUG] Loss this iteration: 0.003672\n",
      "[DEBUG] Loss this iteration: 0.003645\n",
      "[DEBUG] Loss this iteration: 0.003774\n",
      "[DEBUG] Loss this iteration: 0.004547\n",
      "[DEBUG] Loss this iteration: 0.005664\n",
      "[DEBUG] Loss this iteration: 0.004663\n",
      "[DEBUG] Loss this iteration: 0.003937\n",
      "[DEBUG] Loss this iteration: 0.003682\n",
      "[DEBUG] Loss this iteration: 0.003728\n",
      "[DEBUG] Loss this iteration: 0.003662\n",
      "[DEBUG] Loss this iteration: 0.003705\n",
      "[DEBUG] Loss this iteration: 0.003645\n",
      "[DEBUG] Loss this iteration: 0.003728\n",
      "[DEBUG] Loss this iteration: 0.003522\n",
      "[DEBUG] Loss this iteration: 0.003728\n",
      "[DEBUG] Loss this iteration: 0.003576\n",
      "[DEBUG] Loss this iteration: 0.003756\n",
      "[DEBUG] Loss this iteration: 0.004029\n",
      "[DEBUG] Loss this iteration: 0.004251\n",
      "[DEBUG] Loss this iteration: 0.004004\n",
      "[DEBUG] Loss this iteration: 0.003753\n",
      "[DEBUG] Loss this iteration: 0.003757\n",
      "[DEBUG] Loss this iteration: 0.003713\n",
      "[DEBUG] Loss this iteration: 0.003633\n",
      "[DEBUG] Loss this iteration: 0.003576\n",
      "[DEBUG] Loss this iteration: 0.003651\n",
      "[DEBUG] Loss this iteration: 0.003562\n",
      "[DEBUG] Loss this iteration: 0.003518\n",
      "[DEBUG] Loss this iteration: 0.003898\n",
      "[DEBUG] Loss this iteration: 0.004485\n",
      "[DEBUG] Loss this iteration: 0.003808\n",
      "[DEBUG] Loss this iteration: 0.003620\n",
      "[DEBUG] Loss this iteration: 0.003631\n",
      "[DEBUG] Loss this iteration: 0.003501\n",
      "[DEBUG] Loss this iteration: 0.003483\n",
      "[DEBUG] Loss this iteration: 0.003524\n",
      "[DEBUG] Loss this iteration: 0.003483\n",
      "[DEBUG] Loss this iteration: 0.003594\n",
      "[DEBUG] Loss this iteration: 0.003463\n",
      "[DEBUG] Loss this iteration: 0.003632\n",
      "[DEBUG] Loss this iteration: 0.003711\n",
      "[DEBUG] Loss this iteration: 0.003653\n",
      "[DEBUG] Loss this iteration: 0.003448\n",
      "[DEBUG] Loss this iteration: 0.003263\n",
      "[DEBUG] Loss this iteration: 0.003552\n",
      "[DEBUG] Loss this iteration: 0.003596\n",
      "[DEBUG] Loss this iteration: 0.003828\n",
      "[DEBUG] Loss this iteration: 0.003827\n",
      "[DEBUG] Loss this iteration: 0.003860\n",
      "[DEBUG] Loss this iteration: 0.003546\n",
      "[DEBUG] Loss this iteration: 0.003766\n",
      "[DEBUG] Loss this iteration: 0.003417\n",
      "[DEBUG] Loss this iteration: 0.003241\n",
      "[DEBUG] Loss this iteration: 0.003196\n",
      "[DEBUG] Loss this iteration: 0.003169\n",
      "[DEBUG] Loss this iteration: 0.003251\n",
      "[DEBUG] Loss this iteration: 0.003253\n",
      "[DEBUG] Loss this iteration: 0.003250\n",
      "[DEBUG] Loss this iteration: 0.003250\n",
      "[DEBUG] Loss this iteration: 0.003162\n",
      "[DEBUG] Loss this iteration: 0.003059\n",
      "[DEBUG] Loss this iteration: 0.003176\n",
      "[DEBUG] Loss this iteration: 0.003384\n",
      "[DEBUG] Loss this iteration: 0.003582\n",
      "[ER] Iter 300/500 | ER Loss = 0.003582\n",
      "[DEBUG] Loss this iteration: 0.003358\n",
      "[DEBUG] Loss this iteration: 0.003388\n",
      "[DEBUG] Loss this iteration: 0.003780\n",
      "[DEBUG] Loss this iteration: 0.003944\n",
      "[DEBUG] Loss this iteration: 0.003643\n",
      "[DEBUG] Loss this iteration: 0.003169\n",
      "[DEBUG] Loss this iteration: 0.003095\n",
      "[DEBUG] Loss this iteration: 0.003001\n",
      "[DEBUG] Loss this iteration: 0.003055\n",
      "[DEBUG] Loss this iteration: 0.003010\n",
      "[DEBUG] Loss this iteration: 0.003018\n",
      "[DEBUG] Loss this iteration: 0.003109\n",
      "[DEBUG] Loss this iteration: 0.003213\n",
      "[DEBUG] Loss this iteration: 0.003464\n",
      "[DEBUG] Loss this iteration: 0.003838\n",
      "[DEBUG] Loss this iteration: 0.003442\n",
      "[DEBUG] Loss this iteration: 0.003064\n",
      "[DEBUG] Loss this iteration: 0.002976\n",
      "[DEBUG] Loss this iteration: 0.003055\n",
      "[DEBUG] Loss this iteration: 0.003003\n",
      "[DEBUG] Loss this iteration: 0.003077\n",
      "[DEBUG] Loss this iteration: 0.003155\n",
      "[DEBUG] Loss this iteration: 0.003390\n",
      "[DEBUG] Loss this iteration: 0.003562\n",
      "[DEBUG] Loss this iteration: 0.003482\n",
      "[DEBUG] Loss this iteration: 0.003117\n",
      "[DEBUG] Loss this iteration: 0.002976\n",
      "[DEBUG] Loss this iteration: 0.003002\n",
      "[DEBUG] Loss this iteration: 0.002883\n",
      "[DEBUG] Loss this iteration: 0.002842\n",
      "[DEBUG] Loss this iteration: 0.002877\n",
      "[DEBUG] Loss this iteration: 0.002955\n",
      "[DEBUG] Loss this iteration: 0.002949\n",
      "[DEBUG] Loss this iteration: 0.003302\n",
      "[DEBUG] Loss this iteration: 0.003666\n",
      "[DEBUG] Loss this iteration: 0.003296\n",
      "[DEBUG] Loss this iteration: 0.002961\n",
      "[DEBUG] Loss this iteration: 0.003141\n",
      "[DEBUG] Loss this iteration: 0.002970\n",
      "[DEBUG] Loss this iteration: 0.002803\n",
      "[DEBUG] Loss this iteration: 0.002895\n",
      "[DEBUG] Loss this iteration: 0.002968\n",
      "[DEBUG] Loss this iteration: 0.002908\n",
      "[DEBUG] Loss this iteration: 0.002903\n",
      "[DEBUG] Loss this iteration: 0.002839\n",
      "[DEBUG] Loss this iteration: 0.003157\n",
      "[DEBUG] Loss this iteration: 0.003260\n",
      "[DEBUG] Loss this iteration: 0.002840\n",
      "[DEBUG] Loss this iteration: 0.002887\n",
      "[DEBUG] Loss this iteration: 0.003097\n",
      "[DEBUG] Loss this iteration: 0.002774\n",
      "[DEBUG] Loss this iteration: 0.002965\n",
      "[DEBUG] Loss this iteration: 0.002823\n",
      "[DEBUG] Loss this iteration: 0.003064\n",
      "[DEBUG] Loss this iteration: 0.003259\n",
      "[DEBUG] Loss this iteration: 0.003223\n",
      "[DEBUG] Loss this iteration: 0.002820\n",
      "[DEBUG] Loss this iteration: 0.002767\n",
      "[DEBUG] Loss this iteration: 0.002628\n",
      "[DEBUG] Loss this iteration: 0.002603\n",
      "[DEBUG] Loss this iteration: 0.002710\n",
      "[DEBUG] Loss this iteration: 0.002859\n",
      "[DEBUG] Loss this iteration: 0.002779\n",
      "[DEBUG] Loss this iteration: 0.002989\n",
      "[DEBUG] Loss this iteration: 0.002946\n",
      "[DEBUG] Loss this iteration: 0.002859\n",
      "[DEBUG] Loss this iteration: 0.002830\n",
      "[DEBUG] Loss this iteration: 0.002715\n",
      "[DEBUG] Loss this iteration: 0.002817\n",
      "[DEBUG] Loss this iteration: 0.002743\n",
      "[DEBUG] Loss this iteration: 0.003078\n",
      "[DEBUG] Loss this iteration: 0.003300\n",
      "[DEBUG] Loss this iteration: 0.002814\n",
      "[DEBUG] Loss this iteration: 0.002662\n",
      "[DEBUG] Loss this iteration: 0.002664\n",
      "[DEBUG] Loss this iteration: 0.002586\n",
      "[DEBUG] Loss this iteration: 0.002543\n",
      "[DEBUG] Loss this iteration: 0.002594\n",
      "[DEBUG] Loss this iteration: 0.002609\n",
      "[DEBUG] Loss this iteration: 0.002561\n",
      "[DEBUG] Loss this iteration: 0.002946\n",
      "[DEBUG] Loss this iteration: 0.003539\n",
      "[DEBUG] Loss this iteration: 0.003237\n",
      "[DEBUG] Loss this iteration: 0.002885\n",
      "[DEBUG] Loss this iteration: 0.002787\n",
      "[DEBUG] Loss this iteration: 0.002590\n",
      "[DEBUG] Loss this iteration: 0.002554\n",
      "[DEBUG] Loss this iteration: 0.002571\n",
      "[DEBUG] Loss this iteration: 0.002532\n",
      "[DEBUG] Loss this iteration: 0.002522\n",
      "[DEBUG] Loss this iteration: 0.002497\n",
      "[DEBUG] Loss this iteration: 0.002439\n",
      "[DEBUG] Loss this iteration: 0.002581\n",
      "[DEBUG] Loss this iteration: 0.002572\n",
      "[DEBUG] Loss this iteration: 0.002733\n",
      "[DEBUG] Loss this iteration: 0.003054\n",
      "[DEBUG] Loss this iteration: 0.002899\n",
      "[DEBUG] Loss this iteration: 0.002695\n",
      "[DEBUG] Loss this iteration: 0.002834\n",
      "[DEBUG] Loss this iteration: 0.002572\n",
      "[ER] Iter 400/500 | ER Loss = 0.002572\n",
      "[DEBUG] Loss this iteration: 0.002466\n",
      "[DEBUG] Loss this iteration: 0.002531\n",
      "[DEBUG] Loss this iteration: 0.002563\n",
      "[DEBUG] Loss this iteration: 0.002522\n",
      "[DEBUG] Loss this iteration: 0.002719\n",
      "[DEBUG] Loss this iteration: 0.002688\n",
      "[DEBUG] Loss this iteration: 0.002500\n",
      "[DEBUG] Loss this iteration: 0.002435\n",
      "[DEBUG] Loss this iteration: 0.002531\n",
      "[DEBUG] Loss this iteration: 0.002411\n",
      "[DEBUG] Loss this iteration: 0.002306\n",
      "[DEBUG] Loss this iteration: 0.002483\n",
      "[DEBUG] Loss this iteration: 0.002628\n",
      "[DEBUG] Loss this iteration: 0.002610\n",
      "[DEBUG] Loss this iteration: 0.002612\n",
      "[DEBUG] Loss this iteration: 0.002559\n",
      "[DEBUG] Loss this iteration: 0.002614\n",
      "[DEBUG] Loss this iteration: 0.002618\n",
      "[DEBUG] Loss this iteration: 0.002830\n",
      "[DEBUG] Loss this iteration: 0.003012\n",
      "[DEBUG] Loss this iteration: 0.002616\n",
      "[DEBUG] Loss this iteration: 0.002365\n",
      "[DEBUG] Loss this iteration: 0.002360\n",
      "[DEBUG] Loss this iteration: 0.002434\n",
      "[DEBUG] Loss this iteration: 0.002387\n",
      "[DEBUG] Loss this iteration: 0.002280\n",
      "[DEBUG] Loss this iteration: 0.002440\n",
      "[DEBUG] Loss this iteration: 0.002398\n",
      "[DEBUG] Loss this iteration: 0.002423\n",
      "[DEBUG] Loss this iteration: 0.002714\n",
      "[DEBUG] Loss this iteration: 0.002536\n",
      "[DEBUG] Loss this iteration: 0.002487\n",
      "[DEBUG] Loss this iteration: 0.002465\n",
      "[DEBUG] Loss this iteration: 0.002467\n",
      "[DEBUG] Loss this iteration: 0.002462\n",
      "[DEBUG] Loss this iteration: 0.002485\n",
      "[DEBUG] Loss this iteration: 0.002476\n",
      "[DEBUG] Loss this iteration: 0.002421\n",
      "[DEBUG] Loss this iteration: 0.002441\n",
      "[DEBUG] Loss this iteration: 0.002451\n",
      "[DEBUG] Loss this iteration: 0.002357\n",
      "[DEBUG] Loss this iteration: 0.002302\n",
      "[DEBUG] Loss this iteration: 0.002385\n",
      "[DEBUG] Loss this iteration: 0.002286\n",
      "[DEBUG] Loss this iteration: 0.002477\n",
      "[DEBUG] Loss this iteration: 0.002528\n",
      "[DEBUG] Loss this iteration: 0.002462\n",
      "[DEBUG] Loss this iteration: 0.002372\n",
      "[DEBUG] Loss this iteration: 0.002365\n",
      "[DEBUG] Loss this iteration: 0.002528\n",
      "[DEBUG] Loss this iteration: 0.002685\n",
      "[DEBUG] Loss this iteration: 0.002608\n",
      "[DEBUG] Loss this iteration: 0.002445\n",
      "[DEBUG] Loss this iteration: 0.002302\n",
      "[DEBUG] Loss this iteration: 0.002279\n",
      "[DEBUG] Loss this iteration: 0.002277\n",
      "[DEBUG] Loss this iteration: 0.002308\n",
      "[DEBUG] Loss this iteration: 0.002346\n",
      "[DEBUG] Loss this iteration: 0.002387\n",
      "[DEBUG] Loss this iteration: 0.002481\n",
      "[DEBUG] Loss this iteration: 0.002447\n",
      "[DEBUG] Loss this iteration: 0.002427\n",
      "[DEBUG] Loss this iteration: 0.002271\n",
      "[DEBUG] Loss this iteration: 0.002180\n",
      "[DEBUG] Loss this iteration: 0.002222\n",
      "[DEBUG] Loss this iteration: 0.002141\n",
      "[DEBUG] Loss this iteration: 0.002071\n",
      "[DEBUG] Loss this iteration: 0.002147\n",
      "[DEBUG] Loss this iteration: 0.002123\n",
      "[DEBUG] Loss this iteration: 0.002346\n",
      "[DEBUG] Loss this iteration: 0.002441\n",
      "[DEBUG] Loss this iteration: 0.002435\n",
      "[DEBUG] Loss this iteration: 0.002357\n",
      "[DEBUG] Loss this iteration: 0.002362\n",
      "[DEBUG] Loss this iteration: 0.002562\n",
      "[DEBUG] Loss this iteration: 0.002986\n",
      "[DEBUG] Loss this iteration: 0.002882\n",
      "[DEBUG] Loss this iteration: 0.002631\n",
      "[DEBUG] Loss this iteration: 0.002257\n",
      "[DEBUG] Loss this iteration: 0.002173\n",
      "[DEBUG] Loss this iteration: 0.002095\n",
      "[DEBUG] Loss this iteration: 0.002186\n",
      "[DEBUG] Loss this iteration: 0.002107\n",
      "[DEBUG] Loss this iteration: 0.002166\n",
      "[DEBUG] Loss this iteration: 0.002058\n",
      "[DEBUG] Loss this iteration: 0.002009\n",
      "[DEBUG] Loss this iteration: 0.002088\n",
      "[DEBUG] Loss this iteration: 0.002319\n",
      "[DEBUG] Loss this iteration: 0.002449\n",
      "[DEBUG] Loss this iteration: 0.002144\n",
      "[DEBUG] Loss this iteration: 0.002160\n",
      "[DEBUG] Loss this iteration: 0.002263\n",
      "[DEBUG] Loss this iteration: 0.002320\n",
      "[DEBUG] Loss this iteration: 0.002426\n",
      "[DEBUG] Loss this iteration: 0.002567\n",
      "[DEBUG] Loss this iteration: 0.002668\n",
      "[DEBUG] Loss this iteration: 0.002748\n",
      "[DEBUG] Loss this iteration: 0.002227\n",
      "[DEBUG] Loss this iteration: 0.002264\n",
      "=== PRETRAINING: Supervisor ===\n",
      "Loss S:  tensor(1.2474, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "[S] Iter 0/500 | Supervisor Loss = 1.247407\n",
      "Loss S:  tensor(1.2184, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(1.1457, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(1.1459, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(1.1234, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(1.0583, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(1.0371, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(1.0067, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.9892, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.9840, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.9276, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.8929, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.8676, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.8471, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.8222, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.8001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.7874, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.7564, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.7392, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.7182, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.7216, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.7028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.6782, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.6552, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.6202, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.6324, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.6152, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.6010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.6020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.5522, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.5577, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.5408, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.5376, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.5235, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.5345, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.5099, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4787, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4891, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4875, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4662, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4820, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4446, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4590, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4348, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4184, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4383, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4161, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4162, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.4071, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3969, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3992, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3765, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3925, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3731, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3736, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3542, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3677, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3490, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3489, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3437, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3418, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3524, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3451, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3422, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3245, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3301, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3251, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3092, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3219, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3107, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3078, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3043, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.3035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2911, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2929, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2838, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2894, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2867, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2866, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2819, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2864, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2735, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2764, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2832, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2706, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2620, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2679, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2704, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2619, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2521, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2489, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2519, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2424, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2512, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2488, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2338, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2466, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2375, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2432, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "[S] Iter 100/500 | Supervisor Loss = 0.243220\n",
      "Loss S:  tensor(0.2397, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2322, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2376, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2275, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2276, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2253, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2268, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2296, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2252, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2259, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2241, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2206, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2226, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2150, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2153, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2183, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2176, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2150, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2149, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2124, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2118, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2061, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1987, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1995, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1963, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.2004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1962, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1955, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1983, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1912, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1970, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1895, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1922, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1918, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1955, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1874, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1921, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1863, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1861, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1833, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1875, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1806, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1805, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1823, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1805, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1800, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1825, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1856, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1739, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1842, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1785, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1785, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1771, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1774, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1753, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1758, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1735, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1727, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1779, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1749, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1738, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1725, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1713, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1714, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1708, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1695, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1698, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1620, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1728, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1645, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1690, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1670, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1649, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1640, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1642, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1709, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1608, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1633, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1642, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1587, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1597, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1646, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1608, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1620, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1645, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1539, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1601, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1601, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1577, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1569, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1571, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1606, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1597, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1571, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "[S] Iter 200/500 | Supervisor Loss = 0.157075\n",
      "Loss S:  tensor(0.1551, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1528, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1536, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1555, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1534, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1589, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1592, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1537, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1539, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1527, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1513, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1505, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1516, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1553, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1541, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1513, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1497, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1555, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1521, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1496, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1507, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1512, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1552, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1493, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1529, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1516, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1450, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1514, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1459, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1484, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1473, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1505, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1472, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1489, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1456, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1486, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1478, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1495, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1459, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1520, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1471, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1453, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1445, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1441, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1438, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1477, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1469, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1484, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1458, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1454, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1474, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1408, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1480, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1413, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1407, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1446, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1436, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1433, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1428, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1406, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1378, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1382, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1456, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1403, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1394, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1408, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1417, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1375, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1435, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1439, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1440, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1456, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1431, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1378, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1426, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1451, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1406, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1398, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1422, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1423, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1376, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1417, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1394, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1364, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1372, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1390, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1371, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1424, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1408, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1385, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1361, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1388, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1369, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1368, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1379, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1385, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1398, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1356, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1375, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1403, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "[S] Iter 300/500 | Supervisor Loss = 0.140327\n",
      "Loss S:  tensor(0.1392, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1370, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1366, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1376, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1354, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1427, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1369, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1341, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1371, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1381, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1407, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1420, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1347, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1366, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1332, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1335, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1344, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1339, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1354, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1366, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1364, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1371, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1298, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1316, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1349, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1349, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1358, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1377, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1311, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1374, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1312, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1335, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1376, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1348, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1332, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1301, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1343, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1360, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1319, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1348, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1328, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1319, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1340, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1338, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1320, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1335, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1336, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1327, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1310, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1332, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1365, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1343, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1329, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1315, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1347, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1334, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1272, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1337, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1340, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1359, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1367, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1343, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1271, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1322, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1312, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1338, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1288, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1319, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1310, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1321, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1310, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1322, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1332, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1293, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1332, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1327, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1324, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1328, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1316, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1312, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1307, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1314, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1314, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1291, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1288, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1304, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1288, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1299, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1294, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1310, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1292, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1294, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1295, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1327, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1274, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1284, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1329, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1275, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1322, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1291, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "[S] Iter 400/500 | Supervisor Loss = 0.129127\n",
      "Loss S:  tensor(0.1337, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1315, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1288, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1314, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1318, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1290, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1280, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1321, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1285, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1297, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1266, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1283, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1276, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1313, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1300, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1312, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1328, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1289, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1282, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1289, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1322, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1296, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1290, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1303, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1271, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1323, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1268, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1297, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1261, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1271, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1281, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1264, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1287, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1317, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1312, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1335, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1269, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1272, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1243, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1310, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1275, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1283, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1266, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1306, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1272, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1269, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1314, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1288, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1256, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1295, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1286, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1276, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1249, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1267, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1265, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1313, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1287, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1316, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1262, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1286, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1248, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1221, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1287, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1265, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1288, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1256, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1262, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1257, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1298, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1274, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1311, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1287, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1216, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1259, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1312, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1273, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1264, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1245, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1264, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1256, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1275, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1286, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1273, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1278, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1240, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1297, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1273, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1282, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1253, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1239, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1269, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1246, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1284, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1306, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1262, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1241, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1240, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1293, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss S:  tensor(0.1263, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "=== ADVERSARIAL TRAINING (WGAN-GP) ===\n",
      "Loss G (total):  tensor(1.8022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "----------------------------------------------\n",
      "[WGAN] Iter 0/500\n",
      "  D Loss = 992.590515\n",
      "  G Loss = 1.802180\n",
      "----------------------------------------------\n",
      "Loss G (total):  tensor(1.9963, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(2.0062, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(2.0783, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(2.1867, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(2.3061, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(2.3480, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(2.4168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(2.4664, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(2.4920, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(2.5723, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(2.6112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(2.6368, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(2.6450, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(2.7197, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(2.7458, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(2.8274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(2.8886, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(2.9970, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(3.1377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(3.2744, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(3.4302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(3.6007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(3.8211, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.0738, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.3537, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(4.6765, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(5.0329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(5.4769, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(5.9184, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(6.4441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(6.9758, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(7.5462, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(8.1607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(8.8258, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(9.4330, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(10.0586, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(10.6861, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(11.3139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(11.9299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(12.5320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(13.0493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(13.6093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(14.1846, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(14.6780, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(15.1923, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(15.6981, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(16.1633, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(16.6226, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(17.0693, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(17.4931, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(17.9021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(18.3184, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(18.6922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(19.0673, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(19.4678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(19.8158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(20.1839, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(20.5173, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(20.8767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(21.2010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(21.4985, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(21.8138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(22.1414, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(22.4269, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(22.7205, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(23.0283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(23.3203, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(23.6227, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(23.9170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(24.1859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(24.4726, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(24.7650, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(25.0432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(25.3032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(25.5962, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(25.8629, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(26.1493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(26.4335, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(26.7017, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(26.9781, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(27.2442, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(27.5085, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(27.7943, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(28.0555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(28.3470, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(28.5850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(28.8780, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(29.1386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(29.4056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(29.6867, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(29.9464, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(30.2320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(30.5130, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(30.8028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(31.0702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(31.3497, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(31.6258, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(31.9152, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(32.2066, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(32.4718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "----------------------------------------------\n",
      "[WGAN] Iter 100/500\n",
      "  D Loss = -16.320292\n",
      "  G Loss = 32.471779\n",
      "----------------------------------------------\n",
      "Loss G (total):  tensor(32.7450, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(33.0144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(33.2811, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(33.5530, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(33.8417, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(34.1346, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(34.3957, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(34.6675, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(34.9581, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(35.2320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(35.5124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(35.7893, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(36.0954, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(36.3802, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(36.6651, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(36.9380, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(37.2373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(37.5131, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(37.8175, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(38.1024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(38.3695, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(38.6594, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(38.9292, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(39.2249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(39.5060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(39.7948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(40.0908, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(40.3778, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(40.6558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(40.9363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(41.2277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(41.5305, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(41.8192, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(42.1041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(42.3989, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(42.6844, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(42.9789, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(43.3169, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(43.5877, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(43.8933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(44.1672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(44.4541, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(44.7599, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(45.0372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(45.3232, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(45.6210, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(45.9115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(46.2120, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(46.5170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(46.8072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(47.0976, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(47.3988, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(47.6999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(47.9925, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(48.2873, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(48.5963, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(48.9003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(49.2036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(49.5150, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(49.7904, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(50.0936, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(50.3864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(50.7043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(50.9698, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(51.2800, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(51.5705, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(51.8738, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(52.1787, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(52.4686, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(52.7630, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(53.0729, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(53.3633, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(53.6832, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(53.9834, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(54.2840, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(54.6004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(54.8966, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(55.2047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(55.5021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(55.8261, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(56.1123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(56.4136, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(56.7321, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(57.0117, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(57.3130, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(57.6187, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(57.9236, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(58.2135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(58.5024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(58.8337, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(59.1394, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(59.4416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(59.7377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(60.0668, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(60.3495, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(60.6731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(60.9789, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(61.2941, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(61.6014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(61.9029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "----------------------------------------------\n",
      "[WGAN] Iter 200/500\n",
      "  D Loss = -31.036358\n",
      "  G Loss = 61.902870\n",
      "----------------------------------------------\n",
      "Loss G (total):  tensor(62.1984, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(62.5100, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(62.8210, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(63.0975, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(63.4077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(63.7079, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(64.0110, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(64.3199, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(64.6341, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(64.9449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(65.2597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(65.5447, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(65.8623, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(66.1696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(66.4738, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(66.7791, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(67.1130, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(67.4256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(67.7307, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(68.0389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(68.3215, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(68.6494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(68.9439, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(69.2426, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(69.5325, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(69.8518, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(70.1658, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(70.4527, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(70.7726, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(71.0821, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(71.3681, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(71.6825, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(72.0007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(72.3194, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(72.6234, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(72.9335, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(73.2554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(73.5695, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(73.8733, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(74.1858, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(74.4670, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(74.7771, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(75.0883, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(75.3902, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(75.6898, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(75.9977, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(76.3111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(76.6171, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(76.9240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(77.2322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(77.5283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(77.8284, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(78.1531, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(78.4620, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(78.7743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(79.0986, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(79.3847, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(79.7067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(80.0190, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(80.3359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(80.5988, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(80.9305, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(81.2308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(81.5364, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(81.8285, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(82.1360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(82.4267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(82.7406, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(83.0549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(83.3780, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(83.6956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(83.9749, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(84.2896, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(84.5974, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(84.8995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(85.2116, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(85.5324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(85.8385, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(86.1417, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(86.4441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(86.7462, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(87.0632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(87.3512, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(87.6534, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(87.9472, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(88.2476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(88.5449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(88.8760, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(89.1418, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(89.4721, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(89.7621, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(90.0898, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(90.3828, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(90.7034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(90.9948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(91.3201, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(91.6235, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(91.9108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(92.2169, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(92.5214, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "----------------------------------------------\n",
      "[WGAN] Iter 300/500\n",
      "  D Loss = -46.352837\n",
      "  G Loss = 92.521378\n",
      "----------------------------------------------\n",
      "Loss G (total):  tensor(92.8160, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(93.1049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(93.4018, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(93.6956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(94.0164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(94.3077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(94.6253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(94.9265, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(95.2155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(95.5227, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(95.8244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(96.1153, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(96.4270, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(96.7416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(97.0362, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(97.3674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(97.6551, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(97.9714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(98.2750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(98.5692, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(98.8425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(99.1454, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(99.4452, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(99.7337, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(100.0341, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(100.3199, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(100.6335, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(100.9204, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(101.2041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(101.5067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(101.8066, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(102.1439, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(102.4391, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(102.7105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(103.0341, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(103.3475, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(103.6216, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(103.9180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(104.2291, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(104.5128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(104.8091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(105.1131, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(105.3937, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(105.6845, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(105.9941, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(106.2525, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(106.5918, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(106.8702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(107.1675, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(107.4379, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(107.7351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(108.0387, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(108.3196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(108.6020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(108.9206, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(109.2157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(109.5248, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(109.8118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(110.1208, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(110.4000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(110.6964, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(110.9918, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(111.2795, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(111.5665, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(111.8354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(112.1177, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(112.4171, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(112.7171, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(112.9936, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(113.2887, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(113.5758, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(113.8584, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(114.1399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(114.4525, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(114.7471, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(115.0161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(115.3225, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(115.6141, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(115.9084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(116.1837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(116.4700, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(116.7750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(117.0251, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(117.3385, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(117.6173, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(117.9106, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(118.1631, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(118.4643, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(118.7463, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(119.0448, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(119.3140, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(119.5947, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(119.9097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(120.1842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(120.4622, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(120.7682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(121.0320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(121.3388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(121.6187, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(121.9072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "----------------------------------------------\n",
      "[WGAN] Iter 400/500\n",
      "  D Loss = -61.048409\n",
      "  G Loss = 121.907166\n",
      "----------------------------------------------\n",
      "Loss G (total):  tensor(122.1884, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(122.4460, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(122.7157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(122.9980, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(123.2830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(123.5532, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(123.8598, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(124.1331, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(124.4181, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(124.6920, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(124.9509, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(125.2319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(125.5129, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(125.7875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(126.0744, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(126.3572, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(126.6597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(126.9237, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(127.2074, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(127.4714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(127.7630, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(128.0206, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(128.2911, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(128.5548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(128.8337, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(129.1229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(129.3960, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(129.6662, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(129.9640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(130.2118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(130.5713, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(130.7669, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(131.0354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(131.3533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(131.6034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(131.8601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(132.1488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(132.4370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(132.6999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(132.9635, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(133.2367, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(133.5079, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(133.7638, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(134.0289, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(134.3262, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(134.5813, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(134.8480, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(135.1588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(135.4201, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(135.6724, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(135.9601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(136.1822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(136.4874, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(136.7525, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(137.0037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(137.2902, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(137.5475, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(137.8505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(138.1125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(138.3913, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(138.6174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(138.8846, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(139.1734, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(139.4052, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(139.6778, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(139.9181, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(140.2383, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(140.4283, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(140.7062, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(140.9743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(141.2358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(141.5447, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(141.8114, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(142.0383, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(142.3220, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(142.5738, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(142.8591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(143.0969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(143.3800, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(143.6388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(143.9557, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(144.1301, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(144.3895, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(144.6508, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(144.9194, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(145.1647, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(145.4466, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(145.6669, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(145.9495, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(146.1967, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(146.4935, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(146.7182, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(146.9836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(147.2765, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(147.5020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(147.7777, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(148.0186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(148.2797, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss G (total):  tensor(148.5681, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Finish Synthetic Data Generation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from options_TGAN import Options\n",
    "from lib.TimeGAN import TimeGAN\n",
    "\n",
    "# 1. Options\n",
    "opt = Options().parse()\n",
    "\n",
    "\n",
    "opt.print_freq = 100    # imprime cada iteraciÃ³n\n",
    "\n",
    "# 2. Set paper-style hyperparameters\n",
    "opt.lr_g = 1e-4     # LR para el generador\n",
    "opt.lr_d = 5e-4    # LR para el discriminador\n",
    "opt.lr_e = 1e-3     # encoder\n",
    "opt.lr_r = 1e-3     # recovery\n",
    "opt.lr_s = 1e-3     # supervisor\n",
    "\n",
    "\n",
    "opt.beta1 = 0.5\n",
    "\n",
    "opt.batch_size = 64\n",
    "opt.iteration = 500\n",
    "\n",
    "opt.hidden_dim = 32   # muy importante\n",
    "opt.num_layer = 4\n",
    "\n",
    "opt.n_critic = 5     # OK\n",
    "opt.gp_lambda = 1   # OK\n",
    "opt.name = \"TimeGAN_real_paper_settings\"\n",
    "\n",
    "opt.w_g    = 1.0   # antes 80\n",
    "opt.w_e0   = 1.0   # antes 10\n",
    "opt.w_es   = 1.0   # antes 0.1, lo dejamos en la misma escala\n",
    "opt.w_gamma = 1.0  # este se puede mantener\n",
    "\n",
    "\n",
    "# 3. Create model\n",
    "model = TimeGAN(opt, ori_data)\n",
    "\n",
    "# 4. Train\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e09323c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§© Generating 50 samples in 1 batches of 64...\n",
      "  âœ… Batch 1/1 generated (50 samples)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ€ Ejecutando t-SNE (puede tardar unos segundos)...\n",
      "[t-SNE] Computing 99 nearest neighbors...\n",
      "[t-SNE] Indexed 100 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 100 samples in 0.016s...\n",
      "[t-SNE] Computed conditional probabilities for sample 100 / 100\n",
      "[t-SNE] Mean sigma: 4.087503\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 74.199066\n",
      "[t-SNE] KL divergence after 500 iterations: 0.422140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dario\\Desktop\\ThesiS JBP\\jordan_venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from generation_TGAN import safe_generation\n",
    "import numpy as np\n",
    "\n",
    "generated_data = safe_generation(model, num_samples=50, batch_size=64)\n",
    "\n",
    "n_vis = 50000  # target\n",
    "\n",
    "n_real = min(n_vis, len(ori_data))\n",
    "n_fake = min(n_vis, len(generated_data))\n",
    "\n",
    "idx_real = np.random.choice(len(ori_data), n_real, replace=False)\n",
    "idx_fake = np.random.choice(len(generated_data), n_fake, replace=False)\n",
    "\n",
    "ori_vis = [ori_data[i] for i in idx_real]\n",
    "gen_vis = [generated_data[i] for i in idx_fake]\n",
    "\n",
    "from visualization_TGAN import visualization\n",
    "\n",
    "visualization(ori_vis, gen_vis, 'pca')\n",
    "visualization(ori_vis, gen_vis, 'tsne')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jordan_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
