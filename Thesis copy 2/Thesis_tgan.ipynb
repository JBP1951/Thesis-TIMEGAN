{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f024853",
   "metadata": {},
   "source": [
    "IMPLEMENTATION\n",
    "- LSTM/RNN AND THEN TRANSFORMERS INSTEAD RNN\n",
    "- MODIFY THE GENERATOR AND INTRODUCE A WASSERSTEIN + GRADIENT PENALTY\n",
    "- IMPLEMENTATION IN THE ORIGINAL TIME GAN CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b98c7df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos encontrados:\n",
      "   C:\\Users\\Dario\\Desktop\\ThesiS JBP\\Data\\signals_for_GAN_01.mat\n",
      "\n",
      "‚úî Se√±al total concatenada: (6144001, 6)\n",
      "‚úî Normalizaci√≥n global con MinMaxScaler aplicada\n",
      "‚úÖ Created 6143745 sequences of length 256\n",
      "\n",
      "‚úî Total de ventanas creadas: 6143745\n",
      "‚úî Forma de una ventana: (256, 6)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# ‚úÖ CELDA 1 ‚Äî Carga real de se√±ales + normalizaci√≥n + ventanas\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import glob\n",
    "from lib.data_preprocess import MinMax_Scaler, real_data_loading\n",
    "\n",
    "# Ruta donde est√°n los .mat\n",
    "data_dir = r\"C:\\Users\\Dario\\Desktop\\ThesiS JBP\\Data\"\n",
    "file_list = sorted(glob.glob(f\"{data_dir}/signals_for_GAN_*.mat\"))\n",
    "\n",
    "print(\"Archivos encontrados:\")\n",
    "for f in file_list:\n",
    "    print(\"  \", f)\n",
    "\n",
    "if len(file_list) == 0:\n",
    "    raise FileNotFoundError(\"‚ùå No se encontraron archivos signals_for_GAN_*.mat\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üîπ 1) Concatenar toda tu se√±al cruda en un solo array\n",
    "# ------------------------------------------------------------\n",
    "all_data = []\n",
    "for file in file_list:\n",
    "    mat = sio.loadmat(file)\n",
    "    data = mat['data_all']   # shape [N, 6]\n",
    "    all_data.append(data)\n",
    "\n",
    "data_global = np.vstack(all_data)   # [total_muestras, 6]\n",
    "print(f\"\\n‚úî Se√±al total concatenada: {data_global.shape}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üîπ 2) Normalizar TODO globalmente (muy importante)\n",
    "# ------------------------------------------------------------\n",
    "norm_data, scaler = MinMax_Scaler(data_global)\n",
    "print(\"‚úî Normalizaci√≥n global con MinMaxScaler aplicada\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üîπ 3) Crear ventanas con sliding-window (seguro y generalizado)\n",
    "# ------------------------------------------------------------\n",
    "seq_len = 256        # tama√±o fijo de ventana\n",
    "step = 128           # overlap 50% ‚Üí recomendado\n",
    "\n",
    "ori_data = real_data_loading(norm_data, seq_len, step)\n",
    "\n",
    "print(f\"\\n‚úî Total de ventanas creadas: {len(ori_data)}\")\n",
    "print(f\"‚úî Forma de una ventana: {ori_data[0].shape}\")  # (256, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5245412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Adjusted opt.z_dim to match data feature size: 6\n",
      "[DEBUG] Loss this iteration: 0.077676\n",
      "Encoder training step: 0/1000\n",
      "[DEBUG] Loss this iteration: 0.048913\n",
      "Encoder training step: 1/1000\n",
      "[DEBUG] Loss this iteration: 0.023869\n",
      "Encoder training step: 2/1000\n",
      "[DEBUG] Loss this iteration: 0.017437\n",
      "Encoder training step: 3/1000\n",
      "[DEBUG] Loss this iteration: 0.019619\n",
      "Encoder training step: 4/1000\n",
      "[DEBUG] Loss this iteration: 0.017665\n",
      "Encoder training step: 5/1000\n",
      "[DEBUG] Loss this iteration: 0.017516\n",
      "Encoder training step: 6/1000\n",
      "[DEBUG] Loss this iteration: 0.021335\n",
      "Encoder training step: 7/1000\n",
      "[DEBUG] Loss this iteration: 0.018692\n",
      "Encoder training step: 8/1000\n",
      "[DEBUG] Loss this iteration: 0.017317\n",
      "Encoder training step: 9/1000\n",
      "[DEBUG] Loss this iteration: 0.016043\n",
      "Encoder training step: 10/1000\n",
      "[DEBUG] Loss this iteration: 0.020323\n",
      "Encoder training step: 11/1000\n",
      "[DEBUG] Loss this iteration: 0.017608\n",
      "Encoder training step: 12/1000\n",
      "[DEBUG] Loss this iteration: 0.017456\n",
      "Encoder training step: 13/1000\n",
      "[DEBUG] Loss this iteration: 0.016025\n",
      "Encoder training step: 14/1000\n",
      "[DEBUG] Loss this iteration: 0.018847\n",
      "Encoder training step: 15/1000\n",
      "[DEBUG] Loss this iteration: 0.015174\n",
      "Encoder training step: 16/1000\n",
      "[DEBUG] Loss this iteration: 0.016838\n",
      "Encoder training step: 17/1000\n",
      "[DEBUG] Loss this iteration: 0.019010\n",
      "Encoder training step: 18/1000\n",
      "[DEBUG] Loss this iteration: 0.020684\n",
      "Encoder training step: 19/1000\n",
      "[DEBUG] Loss this iteration: 0.013892\n",
      "Encoder training step: 20/1000\n",
      "[DEBUG] Loss this iteration: 0.021231\n",
      "Encoder training step: 21/1000\n",
      "[DEBUG] Loss this iteration: 0.017575\n",
      "Encoder training step: 22/1000\n",
      "[DEBUG] Loss this iteration: 0.014296\n",
      "Encoder training step: 23/1000\n",
      "[DEBUG] Loss this iteration: 0.022679\n",
      "Encoder training step: 24/1000\n",
      "[DEBUG] Loss this iteration: 0.014775\n",
      "Encoder training step: 25/1000\n",
      "[DEBUG] Loss this iteration: 0.015689\n",
      "Encoder training step: 26/1000\n",
      "[DEBUG] Loss this iteration: 0.023458\n",
      "Encoder training step: 27/1000\n",
      "[DEBUG] Loss this iteration: 0.015014\n",
      "Encoder training step: 28/1000\n",
      "[DEBUG] Loss this iteration: 0.018761\n",
      "Encoder training step: 29/1000\n",
      "[DEBUG] Loss this iteration: 0.018017\n",
      "Encoder training step: 30/1000\n",
      "[DEBUG] Loss this iteration: 0.013424\n",
      "Encoder training step: 31/1000\n",
      "[DEBUG] Loss this iteration: 0.023403\n",
      "Encoder training step: 32/1000\n",
      "[DEBUG] Loss this iteration: 0.025310\n",
      "Encoder training step: 33/1000\n",
      "[DEBUG] Loss this iteration: 0.018610\n",
      "Encoder training step: 34/1000\n",
      "[DEBUG] Loss this iteration: 0.017655\n",
      "Encoder training step: 35/1000\n",
      "[DEBUG] Loss this iteration: 0.015916\n",
      "Encoder training step: 36/1000\n",
      "[DEBUG] Loss this iteration: 0.014520\n",
      "Encoder training step: 37/1000\n",
      "[DEBUG] Loss this iteration: 0.021069\n",
      "Encoder training step: 38/1000\n",
      "[DEBUG] Loss this iteration: 0.014581\n",
      "Encoder training step: 39/1000\n",
      "[DEBUG] Loss this iteration: 0.015367\n",
      "Encoder training step: 40/1000\n",
      "[DEBUG] Loss this iteration: 0.018136\n",
      "Encoder training step: 41/1000\n",
      "[DEBUG] Loss this iteration: 0.016017\n",
      "Encoder training step: 42/1000\n",
      "[DEBUG] Loss this iteration: 0.016633\n",
      "Encoder training step: 43/1000\n",
      "[DEBUG] Loss this iteration: 0.015995\n",
      "Encoder training step: 44/1000\n",
      "[DEBUG] Loss this iteration: 0.020708\n",
      "Encoder training step: 45/1000\n",
      "[DEBUG] Loss this iteration: 0.022845\n",
      "Encoder training step: 46/1000\n",
      "[DEBUG] Loss this iteration: 0.020032\n",
      "Encoder training step: 47/1000\n",
      "[DEBUG] Loss this iteration: 0.017155\n",
      "Encoder training step: 48/1000\n",
      "[DEBUG] Loss this iteration: 0.016358\n",
      "Encoder training step: 49/1000\n",
      "[DEBUG] Loss this iteration: 0.015952\n",
      "Encoder training step: 50/1000\n",
      "[DEBUG] Loss this iteration: 0.018470\n",
      "Encoder training step: 51/1000\n",
      "[DEBUG] Loss this iteration: 0.017605\n",
      "Encoder training step: 52/1000\n",
      "[DEBUG] Loss this iteration: 0.015061\n",
      "Encoder training step: 53/1000\n",
      "[DEBUG] Loss this iteration: 0.015442\n",
      "Encoder training step: 54/1000\n",
      "[DEBUG] Loss this iteration: 0.017501\n",
      "Encoder training step: 55/1000\n",
      "[DEBUG] Loss this iteration: 0.015984\n",
      "Encoder training step: 56/1000\n",
      "[DEBUG] Loss this iteration: 0.018660\n",
      "Encoder training step: 57/1000\n",
      "[DEBUG] Loss this iteration: 0.016457\n",
      "Encoder training step: 58/1000\n",
      "[DEBUG] Loss this iteration: 0.015061\n",
      "Encoder training step: 59/1000\n",
      "[DEBUG] Loss this iteration: 0.018989\n",
      "Encoder training step: 60/1000\n",
      "[DEBUG] Loss this iteration: 0.023355\n",
      "Encoder training step: 61/1000\n",
      "[DEBUG] Loss this iteration: 0.019558\n",
      "Encoder training step: 62/1000\n",
      "[DEBUG] Loss this iteration: 0.015507\n",
      "Encoder training step: 63/1000\n",
      "[DEBUG] Loss this iteration: 0.016901\n",
      "Encoder training step: 64/1000\n",
      "[DEBUG] Loss this iteration: 0.018447\n",
      "Encoder training step: 65/1000\n",
      "[DEBUG] Loss this iteration: 0.014850\n",
      "Encoder training step: 66/1000\n",
      "[DEBUG] Loss this iteration: 0.014679\n",
      "Encoder training step: 67/1000\n",
      "[DEBUG] Loss this iteration: 0.019528\n",
      "Encoder training step: 68/1000\n",
      "[DEBUG] Loss this iteration: 0.017936\n",
      "Encoder training step: 69/1000\n",
      "[DEBUG] Loss this iteration: 0.017247\n",
      "Encoder training step: 70/1000\n",
      "[DEBUG] Loss this iteration: 0.019088\n",
      "Encoder training step: 71/1000\n",
      "[DEBUG] Loss this iteration: 0.017860\n",
      "Encoder training step: 72/1000\n",
      "[DEBUG] Loss this iteration: 0.016066\n",
      "Encoder training step: 73/1000\n",
      "[DEBUG] Loss this iteration: 0.016474\n",
      "Encoder training step: 74/1000\n",
      "[DEBUG] Loss this iteration: 0.014574\n",
      "Encoder training step: 75/1000\n",
      "[DEBUG] Loss this iteration: 0.018398\n",
      "Encoder training step: 76/1000\n",
      "[DEBUG] Loss this iteration: 0.019086\n",
      "Encoder training step: 77/1000\n",
      "[DEBUG] Loss this iteration: 0.017633\n",
      "Encoder training step: 78/1000\n",
      "[DEBUG] Loss this iteration: 0.021125\n",
      "Encoder training step: 79/1000\n",
      "[DEBUG] Loss this iteration: 0.023005\n",
      "Encoder training step: 80/1000\n",
      "[DEBUG] Loss this iteration: 0.017437\n",
      "Encoder training step: 81/1000\n",
      "[DEBUG] Loss this iteration: 0.019248\n",
      "Encoder training step: 82/1000\n",
      "[DEBUG] Loss this iteration: 0.022322\n",
      "Encoder training step: 83/1000\n",
      "[DEBUG] Loss this iteration: 0.015348\n",
      "Encoder training step: 84/1000\n",
      "[DEBUG] Loss this iteration: 0.014019\n",
      "Encoder training step: 85/1000\n",
      "[DEBUG] Loss this iteration: 0.015090\n",
      "Encoder training step: 86/1000\n",
      "[DEBUG] Loss this iteration: 0.016711\n",
      "Encoder training step: 87/1000\n",
      "[DEBUG] Loss this iteration: 0.020103\n",
      "Encoder training step: 88/1000\n",
      "[DEBUG] Loss this iteration: 0.015604\n",
      "Encoder training step: 89/1000\n",
      "[DEBUG] Loss this iteration: 0.019821\n",
      "Encoder training step: 90/1000\n",
      "[DEBUG] Loss this iteration: 0.016275\n",
      "Encoder training step: 91/1000\n",
      "[DEBUG] Loss this iteration: 0.016301\n",
      "Encoder training step: 92/1000\n",
      "[DEBUG] Loss this iteration: 0.019655\n",
      "Encoder training step: 93/1000\n",
      "[DEBUG] Loss this iteration: 0.019380\n",
      "Encoder training step: 94/1000\n",
      "[DEBUG] Loss this iteration: 0.016678\n",
      "Encoder training step: 95/1000\n",
      "[DEBUG] Loss this iteration: 0.018268\n",
      "Encoder training step: 96/1000\n",
      "[DEBUG] Loss this iteration: 0.015662\n",
      "Encoder training step: 97/1000\n",
      "[DEBUG] Loss this iteration: 0.014579\n",
      "Encoder training step: 98/1000\n",
      "[DEBUG] Loss this iteration: 0.013281\n",
      "Encoder training step: 99/1000\n",
      "[DEBUG] Loss this iteration: 0.021881\n",
      "Encoder training step: 100/1000\n",
      "[DEBUG] Loss this iteration: 0.015376\n",
      "Encoder training step: 101/1000\n",
      "[DEBUG] Loss this iteration: 0.011339\n",
      "Encoder training step: 102/1000\n",
      "[DEBUG] Loss this iteration: 0.018095\n",
      "Encoder training step: 103/1000\n",
      "[DEBUG] Loss this iteration: 0.015590\n",
      "Encoder training step: 104/1000\n",
      "[DEBUG] Loss this iteration: 0.025914\n",
      "Encoder training step: 105/1000\n",
      "[DEBUG] Loss this iteration: 0.018358\n",
      "Encoder training step: 106/1000\n",
      "[DEBUG] Loss this iteration: 0.018491\n",
      "Encoder training step: 107/1000\n",
      "[DEBUG] Loss this iteration: 0.018555\n",
      "Encoder training step: 108/1000\n",
      "[DEBUG] Loss this iteration: 0.017781\n",
      "Encoder training step: 109/1000\n",
      "[DEBUG] Loss this iteration: 0.018495\n",
      "Encoder training step: 110/1000\n",
      "[DEBUG] Loss this iteration: 0.020244\n",
      "Encoder training step: 111/1000\n",
      "[DEBUG] Loss this iteration: 0.019321\n",
      "Encoder training step: 112/1000\n",
      "[DEBUG] Loss this iteration: 0.020339\n",
      "Encoder training step: 113/1000\n",
      "[DEBUG] Loss this iteration: 0.014796\n",
      "Encoder training step: 114/1000\n",
      "[DEBUG] Loss this iteration: 0.015964\n",
      "Encoder training step: 115/1000\n",
      "[DEBUG] Loss this iteration: 0.014627\n",
      "Encoder training step: 116/1000\n",
      "[DEBUG] Loss this iteration: 0.019317\n",
      "Encoder training step: 117/1000\n",
      "[DEBUG] Loss this iteration: 0.017264\n",
      "Encoder training step: 118/1000\n",
      "[DEBUG] Loss this iteration: 0.016115\n",
      "Encoder training step: 119/1000\n",
      "[DEBUG] Loss this iteration: 0.016923\n",
      "Encoder training step: 120/1000\n",
      "[DEBUG] Loss this iteration: 0.017894\n",
      "Encoder training step: 121/1000\n",
      "[DEBUG] Loss this iteration: 0.020759\n",
      "Encoder training step: 122/1000\n",
      "[DEBUG] Loss this iteration: 0.017595\n",
      "Encoder training step: 123/1000\n",
      "[DEBUG] Loss this iteration: 0.018202\n",
      "Encoder training step: 124/1000\n",
      "[DEBUG] Loss this iteration: 0.020304\n",
      "Encoder training step: 125/1000\n",
      "[DEBUG] Loss this iteration: 0.016715\n",
      "Encoder training step: 126/1000\n",
      "[DEBUG] Loss this iteration: 0.018832\n",
      "Encoder training step: 127/1000\n",
      "[DEBUG] Loss this iteration: 0.012963\n",
      "Encoder training step: 128/1000\n",
      "[DEBUG] Loss this iteration: 0.013534\n",
      "Encoder training step: 129/1000\n",
      "[DEBUG] Loss this iteration: 0.017399\n",
      "Encoder training step: 130/1000\n",
      "[DEBUG] Loss this iteration: 0.021607\n",
      "Encoder training step: 131/1000\n",
      "[DEBUG] Loss this iteration: 0.020148\n",
      "Encoder training step: 132/1000\n",
      "[DEBUG] Loss this iteration: 0.012463\n",
      "Encoder training step: 133/1000\n",
      "[DEBUG] Loss this iteration: 0.017064\n",
      "Encoder training step: 134/1000\n",
      "[DEBUG] Loss this iteration: 0.015844\n",
      "Encoder training step: 135/1000\n",
      "[DEBUG] Loss this iteration: 0.017144\n",
      "Encoder training step: 136/1000\n",
      "[DEBUG] Loss this iteration: 0.015495\n",
      "Encoder training step: 137/1000\n",
      "[DEBUG] Loss this iteration: 0.020390\n",
      "Encoder training step: 138/1000\n",
      "[DEBUG] Loss this iteration: 0.014877\n",
      "Encoder training step: 139/1000\n",
      "[DEBUG] Loss this iteration: 0.016252\n",
      "Encoder training step: 140/1000\n",
      "[DEBUG] Loss this iteration: 0.012724\n",
      "Encoder training step: 141/1000\n",
      "[DEBUG] Loss this iteration: 0.014625\n",
      "Encoder training step: 142/1000\n",
      "[DEBUG] Loss this iteration: 0.016597\n",
      "Encoder training step: 143/1000\n",
      "[DEBUG] Loss this iteration: 0.018264\n",
      "Encoder training step: 144/1000\n",
      "[DEBUG] Loss this iteration: 0.016702\n",
      "Encoder training step: 145/1000\n",
      "[DEBUG] Loss this iteration: 0.018040\n",
      "Encoder training step: 146/1000\n",
      "[DEBUG] Loss this iteration: 0.016077\n",
      "Encoder training step: 147/1000\n",
      "[DEBUG] Loss this iteration: 0.019845\n",
      "Encoder training step: 148/1000\n",
      "[DEBUG] Loss this iteration: 0.012266\n",
      "Encoder training step: 149/1000\n",
      "[DEBUG] Loss this iteration: 0.015081\n",
      "Encoder training step: 150/1000\n",
      "[DEBUG] Loss this iteration: 0.017552\n",
      "Encoder training step: 151/1000\n",
      "[DEBUG] Loss this iteration: 0.019748\n",
      "Encoder training step: 152/1000\n",
      "[DEBUG] Loss this iteration: 0.015883\n",
      "Encoder training step: 153/1000\n",
      "[DEBUG] Loss this iteration: 0.015324\n",
      "Encoder training step: 154/1000\n",
      "[DEBUG] Loss this iteration: 0.013362\n",
      "Encoder training step: 155/1000\n",
      "[DEBUG] Loss this iteration: 0.021509\n",
      "Encoder training step: 156/1000\n",
      "[DEBUG] Loss this iteration: 0.014853\n",
      "Encoder training step: 157/1000\n",
      "[DEBUG] Loss this iteration: 0.020474\n",
      "Encoder training step: 158/1000\n",
      "[DEBUG] Loss this iteration: 0.018052\n",
      "Encoder training step: 159/1000\n",
      "[DEBUG] Loss this iteration: 0.017743\n",
      "Encoder training step: 160/1000\n",
      "[DEBUG] Loss this iteration: 0.016978\n",
      "Encoder training step: 161/1000\n",
      "[DEBUG] Loss this iteration: 0.016392\n",
      "Encoder training step: 162/1000\n",
      "[DEBUG] Loss this iteration: 0.016521\n",
      "Encoder training step: 163/1000\n",
      "[DEBUG] Loss this iteration: 0.015352\n",
      "Encoder training step: 164/1000\n",
      "[DEBUG] Loss this iteration: 0.015709\n",
      "Encoder training step: 165/1000\n",
      "[DEBUG] Loss this iteration: 0.015290\n",
      "Encoder training step: 166/1000\n",
      "[DEBUG] Loss this iteration: 0.015487\n",
      "Encoder training step: 167/1000\n",
      "[DEBUG] Loss this iteration: 0.018638\n",
      "Encoder training step: 168/1000\n",
      "[DEBUG] Loss this iteration: 0.021183\n",
      "Encoder training step: 169/1000\n",
      "[DEBUG] Loss this iteration: 0.015453\n",
      "Encoder training step: 170/1000\n",
      "[DEBUG] Loss this iteration: 0.019929\n",
      "Encoder training step: 171/1000\n",
      "[DEBUG] Loss this iteration: 0.017560\n",
      "Encoder training step: 172/1000\n",
      "[DEBUG] Loss this iteration: 0.017372\n",
      "Encoder training step: 173/1000\n",
      "[DEBUG] Loss this iteration: 0.018799\n",
      "Encoder training step: 174/1000\n",
      "[DEBUG] Loss this iteration: 0.017221\n",
      "Encoder training step: 175/1000\n",
      "[DEBUG] Loss this iteration: 0.016979\n",
      "Encoder training step: 176/1000\n",
      "[DEBUG] Loss this iteration: 0.013358\n",
      "Encoder training step: 177/1000\n",
      "[DEBUG] Loss this iteration: 0.015542\n",
      "Encoder training step: 178/1000\n",
      "[DEBUG] Loss this iteration: 0.016809\n",
      "Encoder training step: 179/1000\n",
      "[DEBUG] Loss this iteration: 0.014665\n",
      "Encoder training step: 180/1000\n",
      "[DEBUG] Loss this iteration: 0.013755\n",
      "Encoder training step: 181/1000\n",
      "[DEBUG] Loss this iteration: 0.015838\n",
      "Encoder training step: 182/1000\n",
      "[DEBUG] Loss this iteration: 0.019023\n",
      "Encoder training step: 183/1000\n",
      "[DEBUG] Loss this iteration: 0.014912\n",
      "Encoder training step: 184/1000\n",
      "[DEBUG] Loss this iteration: 0.017930\n",
      "Encoder training step: 185/1000\n",
      "[DEBUG] Loss this iteration: 0.014770\n",
      "Encoder training step: 186/1000\n",
      "[DEBUG] Loss this iteration: 0.022382\n",
      "Encoder training step: 187/1000\n",
      "[DEBUG] Loss this iteration: 0.014505\n",
      "Encoder training step: 188/1000\n",
      "[DEBUG] Loss this iteration: 0.015776\n",
      "Encoder training step: 189/1000\n",
      "[DEBUG] Loss this iteration: 0.014678\n",
      "Encoder training step: 190/1000\n",
      "[DEBUG] Loss this iteration: 0.014751\n",
      "Encoder training step: 191/1000\n",
      "[DEBUG] Loss this iteration: 0.020746\n",
      "Encoder training step: 192/1000\n",
      "[DEBUG] Loss this iteration: 0.020140\n",
      "Encoder training step: 193/1000\n",
      "[DEBUG] Loss this iteration: 0.013529\n",
      "Encoder training step: 194/1000\n",
      "[DEBUG] Loss this iteration: 0.014654\n",
      "Encoder training step: 195/1000\n",
      "[DEBUG] Loss this iteration: 0.017603\n",
      "Encoder training step: 196/1000\n",
      "[DEBUG] Loss this iteration: 0.015293\n",
      "Encoder training step: 197/1000\n",
      "[DEBUG] Loss this iteration: 0.015230\n",
      "Encoder training step: 198/1000\n",
      "[DEBUG] Loss this iteration: 0.012878\n",
      "Encoder training step: 199/1000\n",
      "[DEBUG] Loss this iteration: 0.014867\n",
      "Encoder training step: 200/1000\n",
      "[DEBUG] Loss this iteration: 0.016329\n",
      "Encoder training step: 201/1000\n",
      "[DEBUG] Loss this iteration: 0.013582\n",
      "Encoder training step: 202/1000\n",
      "[DEBUG] Loss this iteration: 0.016230\n",
      "Encoder training step: 203/1000\n",
      "[DEBUG] Loss this iteration: 0.017436\n",
      "Encoder training step: 204/1000\n",
      "[DEBUG] Loss this iteration: 0.015205\n",
      "Encoder training step: 205/1000\n",
      "[DEBUG] Loss this iteration: 0.010783\n",
      "Encoder training step: 206/1000\n",
      "[DEBUG] Loss this iteration: 0.011936\n",
      "Encoder training step: 207/1000\n",
      "[DEBUG] Loss this iteration: 0.020441\n",
      "Encoder training step: 208/1000\n",
      "[DEBUG] Loss this iteration: 0.015727\n",
      "Encoder training step: 209/1000\n",
      "[DEBUG] Loss this iteration: 0.019843\n",
      "Encoder training step: 210/1000\n",
      "[DEBUG] Loss this iteration: 0.017601\n",
      "Encoder training step: 211/1000\n",
      "[DEBUG] Loss this iteration: 0.020654\n",
      "Encoder training step: 212/1000\n",
      "[DEBUG] Loss this iteration: 0.013892\n",
      "Encoder training step: 213/1000\n",
      "[DEBUG] Loss this iteration: 0.014410\n",
      "Encoder training step: 214/1000\n",
      "[DEBUG] Loss this iteration: 0.014255\n",
      "Encoder training step: 215/1000\n",
      "[DEBUG] Loss this iteration: 0.012647\n",
      "Encoder training step: 216/1000\n",
      "[DEBUG] Loss this iteration: 0.014848\n",
      "Encoder training step: 217/1000\n",
      "[DEBUG] Loss this iteration: 0.016729\n",
      "Encoder training step: 218/1000\n",
      "[DEBUG] Loss this iteration: 0.016688\n",
      "Encoder training step: 219/1000\n",
      "[DEBUG] Loss this iteration: 0.019745\n",
      "Encoder training step: 220/1000\n",
      "[DEBUG] Loss this iteration: 0.013625\n",
      "Encoder training step: 221/1000\n",
      "[DEBUG] Loss this iteration: 0.015384\n",
      "Encoder training step: 222/1000\n",
      "[DEBUG] Loss this iteration: 0.012528\n",
      "Encoder training step: 223/1000\n",
      "[DEBUG] Loss this iteration: 0.015672\n",
      "Encoder training step: 224/1000\n",
      "[DEBUG] Loss this iteration: 0.017113\n",
      "Encoder training step: 225/1000\n",
      "[DEBUG] Loss this iteration: 0.018888\n",
      "Encoder training step: 226/1000\n",
      "[DEBUG] Loss this iteration: 0.013571\n",
      "Encoder training step: 227/1000\n",
      "[DEBUG] Loss this iteration: 0.017053\n",
      "Encoder training step: 228/1000\n",
      "[DEBUG] Loss this iteration: 0.016326\n",
      "Encoder training step: 229/1000\n",
      "[DEBUG] Loss this iteration: 0.010808\n",
      "Encoder training step: 230/1000\n",
      "[DEBUG] Loss this iteration: 0.012915\n",
      "Encoder training step: 231/1000\n",
      "[DEBUG] Loss this iteration: 0.017850\n",
      "Encoder training step: 232/1000\n",
      "[DEBUG] Loss this iteration: 0.015414\n",
      "Encoder training step: 233/1000\n",
      "[DEBUG] Loss this iteration: 0.012381\n",
      "Encoder training step: 234/1000\n",
      "[DEBUG] Loss this iteration: 0.016320\n",
      "Encoder training step: 235/1000\n",
      "[DEBUG] Loss this iteration: 0.013593\n",
      "Encoder training step: 236/1000\n",
      "[DEBUG] Loss this iteration: 0.014786\n",
      "Encoder training step: 237/1000\n",
      "[DEBUG] Loss this iteration: 0.014434\n",
      "Encoder training step: 238/1000\n",
      "[DEBUG] Loss this iteration: 0.016884\n",
      "Encoder training step: 239/1000\n",
      "[DEBUG] Loss this iteration: 0.015964\n",
      "Encoder training step: 240/1000\n",
      "[DEBUG] Loss this iteration: 0.012377\n",
      "Encoder training step: 241/1000\n",
      "[DEBUG] Loss this iteration: 0.012646\n",
      "Encoder training step: 242/1000\n",
      "[DEBUG] Loss this iteration: 0.010850\n",
      "Encoder training step: 243/1000\n",
      "[DEBUG] Loss this iteration: 0.016735\n",
      "Encoder training step: 244/1000\n",
      "[DEBUG] Loss this iteration: 0.014846\n",
      "Encoder training step: 245/1000\n",
      "[DEBUG] Loss this iteration: 0.013899\n",
      "Encoder training step: 246/1000\n",
      "[DEBUG] Loss this iteration: 0.012540\n",
      "Encoder training step: 247/1000\n",
      "[DEBUG] Loss this iteration: 0.011800\n",
      "Encoder training step: 248/1000\n",
      "[DEBUG] Loss this iteration: 0.012464\n",
      "Encoder training step: 249/1000\n",
      "[DEBUG] Loss this iteration: 0.011633\n",
      "Encoder training step: 250/1000\n",
      "[DEBUG] Loss this iteration: 0.012158\n",
      "Encoder training step: 251/1000\n",
      "[DEBUG] Loss this iteration: 0.012984\n",
      "Encoder training step: 252/1000\n",
      "[DEBUG] Loss this iteration: 0.011345\n",
      "Encoder training step: 253/1000\n",
      "[DEBUG] Loss this iteration: 0.011316\n",
      "Encoder training step: 254/1000\n",
      "[DEBUG] Loss this iteration: 0.009726\n",
      "Encoder training step: 255/1000\n",
      "[DEBUG] Loss this iteration: 0.011470\n",
      "Encoder training step: 256/1000\n",
      "[DEBUG] Loss this iteration: 0.010532\n",
      "Encoder training step: 257/1000\n",
      "[DEBUG] Loss this iteration: 0.009288\n",
      "Encoder training step: 258/1000\n",
      "[DEBUG] Loss this iteration: 0.014515\n",
      "Encoder training step: 259/1000\n",
      "[DEBUG] Loss this iteration: 0.009266\n",
      "Encoder training step: 260/1000\n",
      "[DEBUG] Loss this iteration: 0.008909\n",
      "Encoder training step: 261/1000\n",
      "[DEBUG] Loss this iteration: 0.008929\n",
      "Encoder training step: 262/1000\n",
      "[DEBUG] Loss this iteration: 0.008730\n",
      "Encoder training step: 263/1000\n",
      "[DEBUG] Loss this iteration: 0.008981\n",
      "Encoder training step: 264/1000\n",
      "[DEBUG] Loss this iteration: 0.009027\n",
      "Encoder training step: 265/1000\n",
      "[DEBUG] Loss this iteration: 0.006877\n",
      "Encoder training step: 266/1000\n",
      "[DEBUG] Loss this iteration: 0.006484\n",
      "Encoder training step: 267/1000\n",
      "[DEBUG] Loss this iteration: 0.007002\n",
      "Encoder training step: 268/1000\n",
      "[DEBUG] Loss this iteration: 0.006103\n",
      "Encoder training step: 269/1000\n",
      "[DEBUG] Loss this iteration: 0.005562\n",
      "Encoder training step: 270/1000\n",
      "[DEBUG] Loss this iteration: 0.006675\n",
      "Encoder training step: 271/1000\n",
      "[DEBUG] Loss this iteration: 0.008356\n",
      "Encoder training step: 272/1000\n",
      "[DEBUG] Loss this iteration: 0.005891\n",
      "Encoder training step: 273/1000\n",
      "[DEBUG] Loss this iteration: 0.006376\n",
      "Encoder training step: 274/1000\n",
      "[DEBUG] Loss this iteration: 0.005714\n",
      "Encoder training step: 275/1000\n",
      "[DEBUG] Loss this iteration: 0.004892\n",
      "Encoder training step: 276/1000\n",
      "[DEBUG] Loss this iteration: 0.004275\n",
      "Encoder training step: 277/1000\n",
      "[DEBUG] Loss this iteration: 0.005157\n",
      "Encoder training step: 278/1000\n",
      "[DEBUG] Loss this iteration: 0.005271\n",
      "Encoder training step: 279/1000\n",
      "[DEBUG] Loss this iteration: 0.004761\n",
      "Encoder training step: 280/1000\n",
      "[DEBUG] Loss this iteration: 0.005147\n",
      "Encoder training step: 281/1000\n",
      "[DEBUG] Loss this iteration: 0.004953\n",
      "Encoder training step: 282/1000\n",
      "[DEBUG] Loss this iteration: 0.004874\n",
      "Encoder training step: 283/1000\n",
      "[DEBUG] Loss this iteration: 0.004050\n",
      "Encoder training step: 284/1000\n",
      "[DEBUG] Loss this iteration: 0.004850\n",
      "Encoder training step: 285/1000\n",
      "[DEBUG] Loss this iteration: 0.003942\n",
      "Encoder training step: 286/1000\n",
      "[DEBUG] Loss this iteration: 0.004047\n",
      "Encoder training step: 287/1000\n",
      "[DEBUG] Loss this iteration: 0.004862\n",
      "Encoder training step: 288/1000\n",
      "[DEBUG] Loss this iteration: 0.004378\n",
      "Encoder training step: 289/1000\n",
      "[DEBUG] Loss this iteration: 0.004479\n",
      "Encoder training step: 290/1000\n",
      "[DEBUG] Loss this iteration: 0.004485\n",
      "Encoder training step: 291/1000\n",
      "[DEBUG] Loss this iteration: 0.004421\n",
      "Encoder training step: 292/1000\n",
      "[DEBUG] Loss this iteration: 0.004201\n",
      "Encoder training step: 293/1000\n",
      "[DEBUG] Loss this iteration: 0.004488\n",
      "Encoder training step: 294/1000\n",
      "[DEBUG] Loss this iteration: 0.003856\n",
      "Encoder training step: 295/1000\n",
      "[DEBUG] Loss this iteration: 0.003439\n",
      "Encoder training step: 296/1000\n",
      "[DEBUG] Loss this iteration: 0.003588\n",
      "Encoder training step: 297/1000\n",
      "[DEBUG] Loss this iteration: 0.003827\n",
      "Encoder training step: 298/1000\n",
      "[DEBUG] Loss this iteration: 0.003600\n",
      "Encoder training step: 299/1000\n",
      "[DEBUG] Loss this iteration: 0.003995\n",
      "Encoder training step: 300/1000\n",
      "[DEBUG] Loss this iteration: 0.003813\n",
      "Encoder training step: 301/1000\n",
      "[DEBUG] Loss this iteration: 0.003587\n",
      "Encoder training step: 302/1000\n",
      "[DEBUG] Loss this iteration: 0.004688\n",
      "Encoder training step: 303/1000\n",
      "[DEBUG] Loss this iteration: 0.003868\n",
      "Encoder training step: 304/1000\n",
      "[DEBUG] Loss this iteration: 0.004423\n",
      "Encoder training step: 305/1000\n",
      "[DEBUG] Loss this iteration: 0.003205\n",
      "Encoder training step: 306/1000\n",
      "[DEBUG] Loss this iteration: 0.003699\n",
      "Encoder training step: 307/1000\n",
      "[DEBUG] Loss this iteration: 0.003595\n",
      "Encoder training step: 308/1000\n",
      "[DEBUG] Loss this iteration: 0.003987\n",
      "Encoder training step: 309/1000\n",
      "[DEBUG] Loss this iteration: 0.003469\n",
      "Encoder training step: 310/1000\n",
      "[DEBUG] Loss this iteration: 0.003108\n",
      "Encoder training step: 311/1000\n",
      "[DEBUG] Loss this iteration: 0.003311\n",
      "Encoder training step: 312/1000\n",
      "[DEBUG] Loss this iteration: 0.003625\n",
      "Encoder training step: 313/1000\n",
      "[DEBUG] Loss this iteration: 0.003677\n",
      "Encoder training step: 314/1000\n",
      "[DEBUG] Loss this iteration: 0.003714\n",
      "Encoder training step: 315/1000\n",
      "[DEBUG] Loss this iteration: 0.003603\n",
      "Encoder training step: 316/1000\n",
      "[DEBUG] Loss this iteration: 0.003309\n",
      "Encoder training step: 317/1000\n",
      "[DEBUG] Loss this iteration: 0.004535\n",
      "Encoder training step: 318/1000\n",
      "[DEBUG] Loss this iteration: 0.003565\n",
      "Encoder training step: 319/1000\n",
      "[DEBUG] Loss this iteration: 0.003306\n",
      "Encoder training step: 320/1000\n",
      "[DEBUG] Loss this iteration: 0.003575\n",
      "Encoder training step: 321/1000\n",
      "[DEBUG] Loss this iteration: 0.003453\n",
      "Encoder training step: 322/1000\n",
      "[DEBUG] Loss this iteration: 0.003445\n",
      "Encoder training step: 323/1000\n",
      "[DEBUG] Loss this iteration: 0.003910\n",
      "Encoder training step: 324/1000\n",
      "[DEBUG] Loss this iteration: 0.003447\n",
      "Encoder training step: 325/1000\n",
      "[DEBUG] Loss this iteration: 0.003645\n",
      "Encoder training step: 326/1000\n",
      "[DEBUG] Loss this iteration: 0.004010\n",
      "Encoder training step: 327/1000\n",
      "[DEBUG] Loss this iteration: 0.003678\n",
      "Encoder training step: 328/1000\n",
      "[DEBUG] Loss this iteration: 0.003821\n",
      "Encoder training step: 329/1000\n",
      "[DEBUG] Loss this iteration: 0.003586\n",
      "Encoder training step: 330/1000\n",
      "[DEBUG] Loss this iteration: 0.003917\n",
      "Encoder training step: 331/1000\n",
      "[DEBUG] Loss this iteration: 0.004112\n",
      "Encoder training step: 332/1000\n",
      "[DEBUG] Loss this iteration: 0.003985\n",
      "Encoder training step: 333/1000\n",
      "[DEBUG] Loss this iteration: 0.003870\n",
      "Encoder training step: 334/1000\n",
      "[DEBUG] Loss this iteration: 0.003097\n",
      "Encoder training step: 335/1000\n",
      "[DEBUG] Loss this iteration: 0.003741\n",
      "Encoder training step: 336/1000\n",
      "[DEBUG] Loss this iteration: 0.003615\n",
      "Encoder training step: 337/1000\n",
      "[DEBUG] Loss this iteration: 0.003375\n",
      "Encoder training step: 338/1000\n",
      "[DEBUG] Loss this iteration: 0.004006\n",
      "Encoder training step: 339/1000\n",
      "[DEBUG] Loss this iteration: 0.003456\n",
      "Encoder training step: 340/1000\n",
      "[DEBUG] Loss this iteration: 0.003775\n",
      "Encoder training step: 341/1000\n",
      "[DEBUG] Loss this iteration: 0.003338\n",
      "Encoder training step: 342/1000\n",
      "[DEBUG] Loss this iteration: 0.003327\n",
      "Encoder training step: 343/1000\n",
      "[DEBUG] Loss this iteration: 0.003689\n",
      "Encoder training step: 344/1000\n",
      "[DEBUG] Loss this iteration: 0.003863\n",
      "Encoder training step: 345/1000\n",
      "[DEBUG] Loss this iteration: 0.003622\n",
      "Encoder training step: 346/1000\n",
      "[DEBUG] Loss this iteration: 0.003850\n",
      "Encoder training step: 347/1000\n",
      "[DEBUG] Loss this iteration: 0.003715\n",
      "Encoder training step: 348/1000\n",
      "[DEBUG] Loss this iteration: 0.004124\n",
      "Encoder training step: 349/1000\n",
      "[DEBUG] Loss this iteration: 0.003072\n",
      "Encoder training step: 350/1000\n",
      "[DEBUG] Loss this iteration: 0.003117\n",
      "Encoder training step: 351/1000\n",
      "[DEBUG] Loss this iteration: 0.003018\n",
      "Encoder training step: 352/1000\n",
      "[DEBUG] Loss this iteration: 0.003546\n",
      "Encoder training step: 353/1000\n",
      "[DEBUG] Loss this iteration: 0.003204\n",
      "Encoder training step: 354/1000\n",
      "[DEBUG] Loss this iteration: 0.003239\n",
      "Encoder training step: 355/1000\n",
      "[DEBUG] Loss this iteration: 0.003184\n",
      "Encoder training step: 356/1000\n",
      "[DEBUG] Loss this iteration: 0.003700\n",
      "Encoder training step: 357/1000\n",
      "[DEBUG] Loss this iteration: 0.003197\n",
      "Encoder training step: 358/1000\n",
      "[DEBUG] Loss this iteration: 0.003314\n",
      "Encoder training step: 359/1000\n",
      "[DEBUG] Loss this iteration: 0.003365\n",
      "Encoder training step: 360/1000\n",
      "[DEBUG] Loss this iteration: 0.003969\n",
      "Encoder training step: 361/1000\n",
      "[DEBUG] Loss this iteration: 0.003694\n",
      "Encoder training step: 362/1000\n",
      "[DEBUG] Loss this iteration: 0.003111\n",
      "Encoder training step: 363/1000\n",
      "[DEBUG] Loss this iteration: 0.003201\n",
      "Encoder training step: 364/1000\n",
      "[DEBUG] Loss this iteration: 0.003661\n",
      "Encoder training step: 365/1000\n",
      "[DEBUG] Loss this iteration: 0.003512\n",
      "Encoder training step: 366/1000\n",
      "[DEBUG] Loss this iteration: 0.003113\n",
      "Encoder training step: 367/1000\n",
      "[DEBUG] Loss this iteration: 0.003115\n",
      "Encoder training step: 368/1000\n",
      "[DEBUG] Loss this iteration: 0.003737\n",
      "Encoder training step: 369/1000\n",
      "[DEBUG] Loss this iteration: 0.002987\n",
      "Encoder training step: 370/1000\n",
      "[DEBUG] Loss this iteration: 0.003205\n",
      "Encoder training step: 371/1000\n",
      "[DEBUG] Loss this iteration: 0.003513\n",
      "Encoder training step: 372/1000\n",
      "[DEBUG] Loss this iteration: 0.003540\n",
      "Encoder training step: 373/1000\n",
      "[DEBUG] Loss this iteration: 0.003747\n",
      "Encoder training step: 374/1000\n",
      "[DEBUG] Loss this iteration: 0.003433\n",
      "Encoder training step: 375/1000\n",
      "[DEBUG] Loss this iteration: 0.002997\n",
      "Encoder training step: 376/1000\n",
      "[DEBUG] Loss this iteration: 0.003122\n",
      "Encoder training step: 377/1000\n",
      "[DEBUG] Loss this iteration: 0.003428\n",
      "Encoder training step: 378/1000\n",
      "[DEBUG] Loss this iteration: 0.003444\n",
      "Encoder training step: 379/1000\n",
      "[DEBUG] Loss this iteration: 0.002934\n",
      "Encoder training step: 380/1000\n",
      "[DEBUG] Loss this iteration: 0.003279\n",
      "Encoder training step: 381/1000\n",
      "[DEBUG] Loss this iteration: 0.003038\n",
      "Encoder training step: 382/1000\n",
      "[DEBUG] Loss this iteration: 0.002923\n",
      "Encoder training step: 383/1000\n",
      "[DEBUG] Loss this iteration: 0.003496\n",
      "Encoder training step: 384/1000\n",
      "[DEBUG] Loss this iteration: 0.003550\n",
      "Encoder training step: 385/1000\n",
      "[DEBUG] Loss this iteration: 0.003342\n",
      "Encoder training step: 386/1000\n",
      "[DEBUG] Loss this iteration: 0.003697\n",
      "Encoder training step: 387/1000\n",
      "[DEBUG] Loss this iteration: 0.003806\n",
      "Encoder training step: 388/1000\n",
      "[DEBUG] Loss this iteration: 0.003380\n",
      "Encoder training step: 389/1000\n",
      "[DEBUG] Loss this iteration: 0.003101\n",
      "Encoder training step: 390/1000\n",
      "[DEBUG] Loss this iteration: 0.003307\n",
      "Encoder training step: 391/1000\n",
      "[DEBUG] Loss this iteration: 0.003424\n",
      "Encoder training step: 392/1000\n",
      "[DEBUG] Loss this iteration: 0.003716\n",
      "Encoder training step: 393/1000\n",
      "[DEBUG] Loss this iteration: 0.003410\n",
      "Encoder training step: 394/1000\n",
      "[DEBUG] Loss this iteration: 0.003128\n",
      "Encoder training step: 395/1000\n",
      "[DEBUG] Loss this iteration: 0.003522\n",
      "Encoder training step: 396/1000\n",
      "[DEBUG] Loss this iteration: 0.003159\n",
      "Encoder training step: 397/1000\n",
      "[DEBUG] Loss this iteration: 0.003130\n",
      "Encoder training step: 398/1000\n",
      "[DEBUG] Loss this iteration: 0.003950\n",
      "Encoder training step: 399/1000\n",
      "[DEBUG] Loss this iteration: 0.003126\n",
      "Encoder training step: 400/1000\n",
      "[DEBUG] Loss this iteration: 0.003501\n",
      "Encoder training step: 401/1000\n",
      "[DEBUG] Loss this iteration: 0.003172\n",
      "Encoder training step: 402/1000\n",
      "[DEBUG] Loss this iteration: 0.002941\n",
      "Encoder training step: 403/1000\n",
      "[DEBUG] Loss this iteration: 0.003222\n",
      "Encoder training step: 404/1000\n",
      "[DEBUG] Loss this iteration: 0.003054\n",
      "Encoder training step: 405/1000\n",
      "[DEBUG] Loss this iteration: 0.003013\n",
      "Encoder training step: 406/1000\n",
      "[DEBUG] Loss this iteration: 0.003341\n",
      "Encoder training step: 407/1000\n",
      "[DEBUG] Loss this iteration: 0.003667\n",
      "Encoder training step: 408/1000\n",
      "[DEBUG] Loss this iteration: 0.003383\n",
      "Encoder training step: 409/1000\n",
      "[DEBUG] Loss this iteration: 0.003155\n",
      "Encoder training step: 410/1000\n",
      "[DEBUG] Loss this iteration: 0.003325\n",
      "Encoder training step: 411/1000\n",
      "[DEBUG] Loss this iteration: 0.003217\n",
      "Encoder training step: 412/1000\n",
      "[DEBUG] Loss this iteration: 0.003452\n",
      "Encoder training step: 413/1000\n",
      "[DEBUG] Loss this iteration: 0.003709\n",
      "Encoder training step: 414/1000\n",
      "[DEBUG] Loss this iteration: 0.003369\n",
      "Encoder training step: 415/1000\n",
      "[DEBUG] Loss this iteration: 0.003751\n",
      "Encoder training step: 416/1000\n",
      "[DEBUG] Loss this iteration: 0.003189\n",
      "Encoder training step: 417/1000\n",
      "[DEBUG] Loss this iteration: 0.003686\n",
      "Encoder training step: 418/1000\n",
      "[DEBUG] Loss this iteration: 0.003196\n",
      "Encoder training step: 419/1000\n",
      "[DEBUG] Loss this iteration: 0.003123\n",
      "Encoder training step: 420/1000\n",
      "[DEBUG] Loss this iteration: 0.003559\n",
      "Encoder training step: 421/1000\n",
      "[DEBUG] Loss this iteration: 0.003326\n",
      "Encoder training step: 422/1000\n",
      "[DEBUG] Loss this iteration: 0.003038\n",
      "Encoder training step: 423/1000\n",
      "[DEBUG] Loss this iteration: 0.003436\n",
      "Encoder training step: 424/1000\n",
      "[DEBUG] Loss this iteration: 0.003103\n",
      "Encoder training step: 425/1000\n",
      "[DEBUG] Loss this iteration: 0.003128\n",
      "Encoder training step: 426/1000\n",
      "[DEBUG] Loss this iteration: 0.003023\n",
      "Encoder training step: 427/1000\n",
      "[DEBUG] Loss this iteration: 0.003224\n",
      "Encoder training step: 428/1000\n",
      "[DEBUG] Loss this iteration: 0.003312\n",
      "Encoder training step: 429/1000\n",
      "[DEBUG] Loss this iteration: 0.003243\n",
      "Encoder training step: 430/1000\n",
      "[DEBUG] Loss this iteration: 0.003234\n",
      "Encoder training step: 431/1000\n",
      "[DEBUG] Loss this iteration: 0.002820\n",
      "Encoder training step: 432/1000\n",
      "[DEBUG] Loss this iteration: 0.003401\n",
      "Encoder training step: 433/1000\n",
      "[DEBUG] Loss this iteration: 0.003150\n",
      "Encoder training step: 434/1000\n",
      "[DEBUG] Loss this iteration: 0.002987\n",
      "Encoder training step: 435/1000\n",
      "[DEBUG] Loss this iteration: 0.003161\n",
      "Encoder training step: 436/1000\n",
      "[DEBUG] Loss this iteration: 0.003167\n",
      "Encoder training step: 437/1000\n",
      "[DEBUG] Loss this iteration: 0.003579\n",
      "Encoder training step: 438/1000\n",
      "[DEBUG] Loss this iteration: 0.003046\n",
      "Encoder training step: 439/1000\n",
      "[DEBUG] Loss this iteration: 0.003701\n",
      "Encoder training step: 440/1000\n",
      "[DEBUG] Loss this iteration: 0.003787\n",
      "Encoder training step: 441/1000\n",
      "[DEBUG] Loss this iteration: 0.003464\n",
      "Encoder training step: 442/1000\n",
      "[DEBUG] Loss this iteration: 0.003289\n",
      "Encoder training step: 443/1000\n",
      "[DEBUG] Loss this iteration: 0.003225\n",
      "Encoder training step: 444/1000\n",
      "[DEBUG] Loss this iteration: 0.003911\n",
      "Encoder training step: 445/1000\n",
      "[DEBUG] Loss this iteration: 0.003782\n",
      "Encoder training step: 446/1000\n",
      "[DEBUG] Loss this iteration: 0.004011\n",
      "Encoder training step: 447/1000\n",
      "[DEBUG] Loss this iteration: 0.003012\n",
      "Encoder training step: 448/1000\n",
      "[DEBUG] Loss this iteration: 0.003715\n",
      "Encoder training step: 449/1000\n",
      "[DEBUG] Loss this iteration: 0.003810\n",
      "Encoder training step: 450/1000\n",
      "[DEBUG] Loss this iteration: 0.003797\n",
      "Encoder training step: 451/1000\n",
      "[DEBUG] Loss this iteration: 0.003010\n",
      "Encoder training step: 452/1000\n",
      "[DEBUG] Loss this iteration: 0.003394\n",
      "Encoder training step: 453/1000\n",
      "[DEBUG] Loss this iteration: 0.003140\n",
      "Encoder training step: 454/1000\n",
      "[DEBUG] Loss this iteration: 0.002980\n",
      "Encoder training step: 455/1000\n",
      "[DEBUG] Loss this iteration: 0.003147\n",
      "Encoder training step: 456/1000\n",
      "[DEBUG] Loss this iteration: 0.002893\n",
      "Encoder training step: 457/1000\n",
      "[DEBUG] Loss this iteration: 0.003453\n",
      "Encoder training step: 458/1000\n",
      "[DEBUG] Loss this iteration: 0.003698\n",
      "Encoder training step: 459/1000\n",
      "[DEBUG] Loss this iteration: 0.003307\n",
      "Encoder training step: 460/1000\n",
      "[DEBUG] Loss this iteration: 0.003272\n",
      "Encoder training step: 461/1000\n",
      "[DEBUG] Loss this iteration: 0.003382\n",
      "Encoder training step: 462/1000\n",
      "[DEBUG] Loss this iteration: 0.003228\n",
      "Encoder training step: 463/1000\n",
      "[DEBUG] Loss this iteration: 0.003156\n",
      "Encoder training step: 464/1000\n",
      "[DEBUG] Loss this iteration: 0.002864\n",
      "Encoder training step: 465/1000\n",
      "[DEBUG] Loss this iteration: 0.002862\n",
      "Encoder training step: 466/1000\n",
      "[DEBUG] Loss this iteration: 0.003305\n",
      "Encoder training step: 467/1000\n",
      "[DEBUG] Loss this iteration: 0.002770\n",
      "Encoder training step: 468/1000\n",
      "[DEBUG] Loss this iteration: 0.003393\n",
      "Encoder training step: 469/1000\n",
      "[DEBUG] Loss this iteration: 0.003245\n",
      "Encoder training step: 470/1000\n",
      "[DEBUG] Loss this iteration: 0.003291\n",
      "Encoder training step: 471/1000\n",
      "[DEBUG] Loss this iteration: 0.003042\n",
      "Encoder training step: 472/1000\n",
      "[DEBUG] Loss this iteration: 0.003148\n",
      "Encoder training step: 473/1000\n",
      "[DEBUG] Loss this iteration: 0.003450\n",
      "Encoder training step: 474/1000\n",
      "[DEBUG] Loss this iteration: 0.003559\n",
      "Encoder training step: 475/1000\n",
      "[DEBUG] Loss this iteration: 0.003525\n",
      "Encoder training step: 476/1000\n",
      "[DEBUG] Loss this iteration: 0.003402\n",
      "Encoder training step: 477/1000\n",
      "[DEBUG] Loss this iteration: 0.003317\n",
      "Encoder training step: 478/1000\n",
      "[DEBUG] Loss this iteration: 0.003410\n",
      "Encoder training step: 479/1000\n",
      "[DEBUG] Loss this iteration: 0.003510\n",
      "Encoder training step: 480/1000\n",
      "[DEBUG] Loss this iteration: 0.003468\n",
      "Encoder training step: 481/1000\n",
      "[DEBUG] Loss this iteration: 0.004006\n",
      "Encoder training step: 482/1000\n",
      "[DEBUG] Loss this iteration: 0.003774\n",
      "Encoder training step: 483/1000\n",
      "[DEBUG] Loss this iteration: 0.003762\n",
      "Encoder training step: 484/1000\n",
      "[DEBUG] Loss this iteration: 0.003236\n",
      "Encoder training step: 485/1000\n",
      "[DEBUG] Loss this iteration: 0.003090\n",
      "Encoder training step: 486/1000\n",
      "[DEBUG] Loss this iteration: 0.002979\n",
      "Encoder training step: 487/1000\n",
      "[DEBUG] Loss this iteration: 0.003334\n",
      "Encoder training step: 488/1000\n",
      "[DEBUG] Loss this iteration: 0.003200\n",
      "Encoder training step: 489/1000\n",
      "[DEBUG] Loss this iteration: 0.003142\n",
      "Encoder training step: 490/1000\n",
      "[DEBUG] Loss this iteration: 0.003105\n",
      "Encoder training step: 491/1000\n",
      "[DEBUG] Loss this iteration: 0.002939\n",
      "Encoder training step: 492/1000\n",
      "[DEBUG] Loss this iteration: 0.003528\n",
      "Encoder training step: 493/1000\n",
      "[DEBUG] Loss this iteration: 0.002869\n",
      "Encoder training step: 494/1000\n",
      "[DEBUG] Loss this iteration: 0.003072\n",
      "Encoder training step: 495/1000\n",
      "[DEBUG] Loss this iteration: 0.002976\n",
      "Encoder training step: 496/1000\n",
      "[DEBUG] Loss this iteration: 0.003382\n",
      "Encoder training step: 497/1000\n",
      "[DEBUG] Loss this iteration: 0.002857\n",
      "Encoder training step: 498/1000\n",
      "[DEBUG] Loss this iteration: 0.003206\n",
      "Encoder training step: 499/1000\n",
      "[DEBUG] Loss this iteration: 0.003275\n",
      "Encoder training step: 500/1000\n",
      "[DEBUG] Loss this iteration: 0.002907\n",
      "Encoder training step: 501/1000\n",
      "[DEBUG] Loss this iteration: 0.003034\n",
      "Encoder training step: 502/1000\n",
      "[DEBUG] Loss this iteration: 0.003294\n",
      "Encoder training step: 503/1000\n",
      "[DEBUG] Loss this iteration: 0.003389\n",
      "Encoder training step: 504/1000\n",
      "[DEBUG] Loss this iteration: 0.002994\n",
      "Encoder training step: 505/1000\n",
      "[DEBUG] Loss this iteration: 0.002720\n",
      "Encoder training step: 506/1000\n",
      "[DEBUG] Loss this iteration: 0.003252\n",
      "Encoder training step: 507/1000\n",
      "[DEBUG] Loss this iteration: 0.003183\n",
      "Encoder training step: 508/1000\n",
      "[DEBUG] Loss this iteration: 0.002933\n",
      "Encoder training step: 509/1000\n",
      "[DEBUG] Loss this iteration: 0.003048\n",
      "Encoder training step: 510/1000\n",
      "[DEBUG] Loss this iteration: 0.003174\n",
      "Encoder training step: 511/1000\n",
      "[DEBUG] Loss this iteration: 0.003294\n",
      "Encoder training step: 512/1000\n",
      "[DEBUG] Loss this iteration: 0.003011\n",
      "Encoder training step: 513/1000\n",
      "[DEBUG] Loss this iteration: 0.003104\n",
      "Encoder training step: 514/1000\n",
      "[DEBUG] Loss this iteration: 0.003176\n",
      "Encoder training step: 515/1000\n",
      "[DEBUG] Loss this iteration: 0.003493\n",
      "Encoder training step: 516/1000\n",
      "[DEBUG] Loss this iteration: 0.003196\n",
      "Encoder training step: 517/1000\n",
      "[DEBUG] Loss this iteration: 0.003240\n",
      "Encoder training step: 518/1000\n",
      "[DEBUG] Loss this iteration: 0.003143\n",
      "Encoder training step: 519/1000\n",
      "[DEBUG] Loss this iteration: 0.003122\n",
      "Encoder training step: 520/1000\n",
      "[DEBUG] Loss this iteration: 0.003144\n",
      "Encoder training step: 521/1000\n",
      "[DEBUG] Loss this iteration: 0.003412\n",
      "Encoder training step: 522/1000\n",
      "[DEBUG] Loss this iteration: 0.003082\n",
      "Encoder training step: 523/1000\n",
      "[DEBUG] Loss this iteration: 0.003274\n",
      "Encoder training step: 524/1000\n",
      "[DEBUG] Loss this iteration: 0.003242\n",
      "Encoder training step: 525/1000\n",
      "[DEBUG] Loss this iteration: 0.003232\n",
      "Encoder training step: 526/1000\n",
      "[DEBUG] Loss this iteration: 0.003404\n",
      "Encoder training step: 527/1000\n",
      "[DEBUG] Loss this iteration: 0.002822\n",
      "Encoder training step: 528/1000\n",
      "[DEBUG] Loss this iteration: 0.003224\n",
      "Encoder training step: 529/1000\n",
      "[DEBUG] Loss this iteration: 0.003004\n",
      "Encoder training step: 530/1000\n",
      "[DEBUG] Loss this iteration: 0.003041\n",
      "Encoder training step: 531/1000\n",
      "[DEBUG] Loss this iteration: 0.003064\n",
      "Encoder training step: 532/1000\n",
      "[DEBUG] Loss this iteration: 0.002956\n",
      "Encoder training step: 533/1000\n",
      "[DEBUG] Loss this iteration: 0.003074\n",
      "Encoder training step: 534/1000\n",
      "[DEBUG] Loss this iteration: 0.003212\n",
      "Encoder training step: 535/1000\n",
      "[DEBUG] Loss this iteration: 0.003704\n",
      "Encoder training step: 536/1000\n",
      "[DEBUG] Loss this iteration: 0.003502\n",
      "Encoder training step: 537/1000\n",
      "[DEBUG] Loss this iteration: 0.003271\n",
      "Encoder training step: 538/1000\n",
      "[DEBUG] Loss this iteration: 0.003281\n",
      "Encoder training step: 539/1000\n",
      "[DEBUG] Loss this iteration: 0.003171\n",
      "Encoder training step: 540/1000\n",
      "[DEBUG] Loss this iteration: 0.003206\n",
      "Encoder training step: 541/1000\n",
      "[DEBUG] Loss this iteration: 0.003010\n",
      "Encoder training step: 542/1000\n",
      "[DEBUG] Loss this iteration: 0.003003\n",
      "Encoder training step: 543/1000\n",
      "[DEBUG] Loss this iteration: 0.003248\n",
      "Encoder training step: 544/1000\n",
      "[DEBUG] Loss this iteration: 0.002989\n",
      "Encoder training step: 545/1000\n",
      "[DEBUG] Loss this iteration: 0.002957\n",
      "Encoder training step: 546/1000\n",
      "[DEBUG] Loss this iteration: 0.003090\n",
      "Encoder training step: 547/1000\n",
      "[DEBUG] Loss this iteration: 0.003116\n",
      "Encoder training step: 548/1000\n",
      "[DEBUG] Loss this iteration: 0.002739\n",
      "Encoder training step: 549/1000\n",
      "[DEBUG] Loss this iteration: 0.002861\n",
      "Encoder training step: 550/1000\n",
      "[DEBUG] Loss this iteration: 0.003011\n",
      "Encoder training step: 551/1000\n",
      "[DEBUG] Loss this iteration: 0.003086\n",
      "Encoder training step: 552/1000\n",
      "[DEBUG] Loss this iteration: 0.002857\n",
      "Encoder training step: 553/1000\n",
      "[DEBUG] Loss this iteration: 0.003033\n",
      "Encoder training step: 554/1000\n",
      "[DEBUG] Loss this iteration: 0.003065\n",
      "Encoder training step: 555/1000\n",
      "[DEBUG] Loss this iteration: 0.002971\n",
      "Encoder training step: 556/1000\n",
      "[DEBUG] Loss this iteration: 0.003574\n",
      "Encoder training step: 557/1000\n",
      "[DEBUG] Loss this iteration: 0.002955\n",
      "Encoder training step: 558/1000\n",
      "[DEBUG] Loss this iteration: 0.003055\n",
      "Encoder training step: 559/1000\n",
      "[DEBUG] Loss this iteration: 0.002747\n",
      "Encoder training step: 560/1000\n",
      "[DEBUG] Loss this iteration: 0.003038\n",
      "Encoder training step: 561/1000\n",
      "[DEBUG] Loss this iteration: 0.003187\n",
      "Encoder training step: 562/1000\n",
      "[DEBUG] Loss this iteration: 0.003010\n",
      "Encoder training step: 563/1000\n",
      "[DEBUG] Loss this iteration: 0.002811\n",
      "Encoder training step: 564/1000\n",
      "[DEBUG] Loss this iteration: 0.002998\n",
      "Encoder training step: 565/1000\n",
      "[DEBUG] Loss this iteration: 0.002839\n",
      "Encoder training step: 566/1000\n",
      "[DEBUG] Loss this iteration: 0.003001\n",
      "Encoder training step: 567/1000\n",
      "[DEBUG] Loss this iteration: 0.003271\n",
      "Encoder training step: 568/1000\n",
      "[DEBUG] Loss this iteration: 0.003158\n",
      "Encoder training step: 569/1000\n",
      "[DEBUG] Loss this iteration: 0.003150\n",
      "Encoder training step: 570/1000\n",
      "[DEBUG] Loss this iteration: 0.003046\n",
      "Encoder training step: 571/1000\n",
      "[DEBUG] Loss this iteration: 0.002988\n",
      "Encoder training step: 572/1000\n",
      "[DEBUG] Loss this iteration: 0.003043\n",
      "Encoder training step: 573/1000\n",
      "[DEBUG] Loss this iteration: 0.003236\n",
      "Encoder training step: 574/1000\n",
      "[DEBUG] Loss this iteration: 0.003245\n",
      "Encoder training step: 575/1000\n",
      "[DEBUG] Loss this iteration: 0.003129\n",
      "Encoder training step: 576/1000\n",
      "[DEBUG] Loss this iteration: 0.003145\n",
      "Encoder training step: 577/1000\n",
      "[DEBUG] Loss this iteration: 0.003090\n",
      "Encoder training step: 578/1000\n",
      "[DEBUG] Loss this iteration: 0.002815\n",
      "Encoder training step: 579/1000\n",
      "[DEBUG] Loss this iteration: 0.002913\n",
      "Encoder training step: 580/1000\n",
      "[DEBUG] Loss this iteration: 0.003167\n",
      "Encoder training step: 581/1000\n",
      "[DEBUG] Loss this iteration: 0.002959\n",
      "Encoder training step: 582/1000\n",
      "[DEBUG] Loss this iteration: 0.002821\n",
      "Encoder training step: 583/1000\n",
      "[DEBUG] Loss this iteration: 0.002822\n",
      "Encoder training step: 584/1000\n",
      "[DEBUG] Loss this iteration: 0.002804\n",
      "Encoder training step: 585/1000\n",
      "[DEBUG] Loss this iteration: 0.002863\n",
      "Encoder training step: 586/1000\n",
      "[DEBUG] Loss this iteration: 0.003079\n",
      "Encoder training step: 587/1000\n",
      "[DEBUG] Loss this iteration: 0.003015\n",
      "Encoder training step: 588/1000\n",
      "[DEBUG] Loss this iteration: 0.002760\n",
      "Encoder training step: 589/1000\n",
      "[DEBUG] Loss this iteration: 0.003120\n",
      "Encoder training step: 590/1000\n",
      "[DEBUG] Loss this iteration: 0.003025\n",
      "Encoder training step: 591/1000\n",
      "[DEBUG] Loss this iteration: 0.002896\n",
      "Encoder training step: 592/1000\n",
      "[DEBUG] Loss this iteration: 0.002829\n",
      "Encoder training step: 593/1000\n",
      "[DEBUG] Loss this iteration: 0.003064\n",
      "Encoder training step: 594/1000\n",
      "[DEBUG] Loss this iteration: 0.003057\n",
      "Encoder training step: 595/1000\n",
      "[DEBUG] Loss this iteration: 0.003046\n",
      "Encoder training step: 596/1000\n",
      "[DEBUG] Loss this iteration: 0.002819\n",
      "Encoder training step: 597/1000\n",
      "[DEBUG] Loss this iteration: 0.002822\n",
      "Encoder training step: 598/1000\n",
      "[DEBUG] Loss this iteration: 0.002847\n",
      "Encoder training step: 599/1000\n",
      "[DEBUG] Loss this iteration: 0.002984\n",
      "Encoder training step: 600/1000\n",
      "[DEBUG] Loss this iteration: 0.002949\n",
      "Encoder training step: 601/1000\n",
      "[DEBUG] Loss this iteration: 0.003167\n",
      "Encoder training step: 602/1000\n",
      "[DEBUG] Loss this iteration: 0.003648\n",
      "Encoder training step: 603/1000\n",
      "[DEBUG] Loss this iteration: 0.003289\n",
      "Encoder training step: 604/1000\n",
      "[DEBUG] Loss this iteration: 0.003234\n",
      "Encoder training step: 605/1000\n",
      "[DEBUG] Loss this iteration: 0.003298\n",
      "Encoder training step: 606/1000\n",
      "[DEBUG] Loss this iteration: 0.003325\n",
      "Encoder training step: 607/1000\n",
      "[DEBUG] Loss this iteration: 0.003053\n",
      "Encoder training step: 608/1000\n",
      "[DEBUG] Loss this iteration: 0.003263\n",
      "Encoder training step: 609/1000\n",
      "[DEBUG] Loss this iteration: 0.003035\n",
      "Encoder training step: 610/1000\n",
      "[DEBUG] Loss this iteration: 0.002820\n",
      "Encoder training step: 611/1000\n",
      "[DEBUG] Loss this iteration: 0.002709\n",
      "Encoder training step: 612/1000\n",
      "[DEBUG] Loss this iteration: 0.003006\n",
      "Encoder training step: 613/1000\n",
      "[DEBUG] Loss this iteration: 0.002828\n",
      "Encoder training step: 614/1000\n",
      "[DEBUG] Loss this iteration: 0.002785\n",
      "Encoder training step: 615/1000\n",
      "[DEBUG] Loss this iteration: 0.002793\n",
      "Encoder training step: 616/1000\n",
      "[DEBUG] Loss this iteration: 0.003164\n",
      "Encoder training step: 617/1000\n",
      "[DEBUG] Loss this iteration: 0.003342\n",
      "Encoder training step: 618/1000\n",
      "[DEBUG] Loss this iteration: 0.002817\n",
      "Encoder training step: 619/1000\n",
      "[DEBUG] Loss this iteration: 0.003187\n",
      "Encoder training step: 620/1000\n",
      "[DEBUG] Loss this iteration: 0.003189\n",
      "Encoder training step: 621/1000\n",
      "[DEBUG] Loss this iteration: 0.003102\n",
      "Encoder training step: 622/1000\n",
      "[DEBUG] Loss this iteration: 0.002836\n",
      "Encoder training step: 623/1000\n",
      "[DEBUG] Loss this iteration: 0.003041\n",
      "Encoder training step: 624/1000\n",
      "[DEBUG] Loss this iteration: 0.002749\n",
      "Encoder training step: 625/1000\n",
      "[DEBUG] Loss this iteration: 0.002732\n",
      "Encoder training step: 626/1000\n",
      "[DEBUG] Loss this iteration: 0.002995\n",
      "Encoder training step: 627/1000\n",
      "[DEBUG] Loss this iteration: 0.003112\n",
      "Encoder training step: 628/1000\n",
      "[DEBUG] Loss this iteration: 0.002934\n",
      "Encoder training step: 629/1000\n",
      "[DEBUG] Loss this iteration: 0.002701\n",
      "Encoder training step: 630/1000\n",
      "[DEBUG] Loss this iteration: 0.002874\n",
      "Encoder training step: 631/1000\n",
      "[DEBUG] Loss this iteration: 0.002657\n",
      "Encoder training step: 632/1000\n",
      "[DEBUG] Loss this iteration: 0.003050\n",
      "Encoder training step: 633/1000\n",
      "[DEBUG] Loss this iteration: 0.002658\n",
      "Encoder training step: 634/1000\n",
      "[DEBUG] Loss this iteration: 0.002795\n",
      "Encoder training step: 635/1000\n",
      "[DEBUG] Loss this iteration: 0.002880\n",
      "Encoder training step: 636/1000\n",
      "[DEBUG] Loss this iteration: 0.002981\n",
      "Encoder training step: 637/1000\n",
      "[DEBUG] Loss this iteration: 0.002649\n",
      "Encoder training step: 638/1000\n",
      "[DEBUG] Loss this iteration: 0.002923\n",
      "Encoder training step: 639/1000\n",
      "[DEBUG] Loss this iteration: 0.003400\n",
      "Encoder training step: 640/1000\n",
      "[DEBUG] Loss this iteration: 0.003463\n",
      "Encoder training step: 641/1000\n",
      "[DEBUG] Loss this iteration: 0.003111\n",
      "Encoder training step: 642/1000\n",
      "[DEBUG] Loss this iteration: 0.003225\n",
      "Encoder training step: 643/1000\n",
      "[DEBUG] Loss this iteration: 0.003149\n",
      "Encoder training step: 644/1000\n",
      "[DEBUG] Loss this iteration: 0.003368\n",
      "Encoder training step: 645/1000\n",
      "[DEBUG] Loss this iteration: 0.003409\n",
      "Encoder training step: 646/1000\n",
      "[DEBUG] Loss this iteration: 0.003386\n",
      "Encoder training step: 647/1000\n",
      "[DEBUG] Loss this iteration: 0.003385\n",
      "Encoder training step: 648/1000\n",
      "[DEBUG] Loss this iteration: 0.003168\n",
      "Encoder training step: 649/1000\n",
      "[DEBUG] Loss this iteration: 0.002886\n",
      "Encoder training step: 650/1000\n",
      "[DEBUG] Loss this iteration: 0.003870\n",
      "Encoder training step: 651/1000\n",
      "[DEBUG] Loss this iteration: 0.004033\n",
      "Encoder training step: 652/1000\n",
      "[DEBUG] Loss this iteration: 0.003601\n",
      "Encoder training step: 653/1000\n",
      "[DEBUG] Loss this iteration: 0.003253\n",
      "Encoder training step: 654/1000\n",
      "[DEBUG] Loss this iteration: 0.002858\n",
      "Encoder training step: 655/1000\n",
      "[DEBUG] Loss this iteration: 0.002774\n",
      "Encoder training step: 656/1000\n",
      "[DEBUG] Loss this iteration: 0.003082\n",
      "Encoder training step: 657/1000\n",
      "[DEBUG] Loss this iteration: 0.002809\n",
      "Encoder training step: 658/1000\n",
      "[DEBUG] Loss this iteration: 0.002945\n",
      "Encoder training step: 659/1000\n",
      "[DEBUG] Loss this iteration: 0.002818\n",
      "Encoder training step: 660/1000\n",
      "[DEBUG] Loss this iteration: 0.003046\n",
      "Encoder training step: 661/1000\n",
      "[DEBUG] Loss this iteration: 0.003083\n",
      "Encoder training step: 662/1000\n",
      "[DEBUG] Loss this iteration: 0.002916\n",
      "Encoder training step: 663/1000\n",
      "[DEBUG] Loss this iteration: 0.002839\n",
      "Encoder training step: 664/1000\n",
      "[DEBUG] Loss this iteration: 0.002911\n",
      "Encoder training step: 665/1000\n",
      "[DEBUG] Loss this iteration: 0.002836\n",
      "Encoder training step: 666/1000\n",
      "[DEBUG] Loss this iteration: 0.002957\n",
      "Encoder training step: 667/1000\n",
      "[DEBUG] Loss this iteration: 0.002684\n",
      "Encoder training step: 668/1000\n",
      "[DEBUG] Loss this iteration: 0.003285\n",
      "Encoder training step: 669/1000\n",
      "[DEBUG] Loss this iteration: 0.003213\n",
      "Encoder training step: 670/1000\n",
      "[DEBUG] Loss this iteration: 0.003140\n",
      "Encoder training step: 671/1000\n",
      "[DEBUG] Loss this iteration: 0.002889\n",
      "Encoder training step: 672/1000\n",
      "[DEBUG] Loss this iteration: 0.002813\n",
      "Encoder training step: 673/1000\n",
      "[DEBUG] Loss this iteration: 0.002998\n",
      "Encoder training step: 674/1000\n",
      "[DEBUG] Loss this iteration: 0.002838\n",
      "Encoder training step: 675/1000\n",
      "[DEBUG] Loss this iteration: 0.002920\n",
      "Encoder training step: 676/1000\n",
      "[DEBUG] Loss this iteration: 0.002698\n",
      "Encoder training step: 677/1000\n",
      "[DEBUG] Loss this iteration: 0.002863\n",
      "Encoder training step: 678/1000\n",
      "[DEBUG] Loss this iteration: 0.002880\n",
      "Encoder training step: 679/1000\n",
      "[DEBUG] Loss this iteration: 0.002715\n",
      "Encoder training step: 680/1000\n",
      "[DEBUG] Loss this iteration: 0.002789\n",
      "Encoder training step: 681/1000\n",
      "[DEBUG] Loss this iteration: 0.002936\n",
      "Encoder training step: 682/1000\n",
      "[DEBUG] Loss this iteration: 0.002835\n",
      "Encoder training step: 683/1000\n",
      "[DEBUG] Loss this iteration: 0.002862\n",
      "Encoder training step: 684/1000\n",
      "[DEBUG] Loss this iteration: 0.002630\n",
      "Encoder training step: 685/1000\n",
      "[DEBUG] Loss this iteration: 0.003108\n",
      "Encoder training step: 686/1000\n",
      "[DEBUG] Loss this iteration: 0.002958\n",
      "Encoder training step: 687/1000\n",
      "[DEBUG] Loss this iteration: 0.002744\n",
      "Encoder training step: 688/1000\n",
      "[DEBUG] Loss this iteration: 0.003079\n",
      "Encoder training step: 689/1000\n",
      "[DEBUG] Loss this iteration: 0.003581\n",
      "Encoder training step: 690/1000\n",
      "[DEBUG] Loss this iteration: 0.003100\n",
      "Encoder training step: 691/1000\n",
      "[DEBUG] Loss this iteration: 0.002824\n",
      "Encoder training step: 692/1000\n",
      "[DEBUG] Loss this iteration: 0.002732\n",
      "Encoder training step: 693/1000\n",
      "[DEBUG] Loss this iteration: 0.002744\n",
      "Encoder training step: 694/1000\n",
      "[DEBUG] Loss this iteration: 0.002679\n",
      "Encoder training step: 695/1000\n",
      "[DEBUG] Loss this iteration: 0.002812\n",
      "Encoder training step: 696/1000\n",
      "[DEBUG] Loss this iteration: 0.003011\n",
      "Encoder training step: 697/1000\n",
      "[DEBUG] Loss this iteration: 0.003011\n",
      "Encoder training step: 698/1000\n",
      "[DEBUG] Loss this iteration: 0.002992\n",
      "Encoder training step: 699/1000\n",
      "[DEBUG] Loss this iteration: 0.002776\n",
      "Encoder training step: 700/1000\n",
      "[DEBUG] Loss this iteration: 0.003099\n",
      "Encoder training step: 701/1000\n",
      "[DEBUG] Loss this iteration: 0.002800\n",
      "Encoder training step: 702/1000\n",
      "[DEBUG] Loss this iteration: 0.003124\n",
      "Encoder training step: 703/1000\n",
      "[DEBUG] Loss this iteration: 0.002737\n",
      "Encoder training step: 704/1000\n",
      "[DEBUG] Loss this iteration: 0.002850\n",
      "Encoder training step: 705/1000\n",
      "[DEBUG] Loss this iteration: 0.002731\n",
      "Encoder training step: 706/1000\n",
      "[DEBUG] Loss this iteration: 0.002902\n",
      "Encoder training step: 707/1000\n",
      "[DEBUG] Loss this iteration: 0.003184\n",
      "Encoder training step: 708/1000\n",
      "[DEBUG] Loss this iteration: 0.002715\n",
      "Encoder training step: 709/1000\n",
      "[DEBUG] Loss this iteration: 0.002799\n",
      "Encoder training step: 710/1000\n",
      "[DEBUG] Loss this iteration: 0.002708\n",
      "Encoder training step: 711/1000\n",
      "[DEBUG] Loss this iteration: 0.002959\n",
      "Encoder training step: 712/1000\n",
      "[DEBUG] Loss this iteration: 0.002806\n",
      "Encoder training step: 713/1000\n",
      "[DEBUG] Loss this iteration: 0.002794\n",
      "Encoder training step: 714/1000\n",
      "[DEBUG] Loss this iteration: 0.002887\n",
      "Encoder training step: 715/1000\n",
      "[DEBUG] Loss this iteration: 0.002848\n",
      "Encoder training step: 716/1000\n",
      "[DEBUG] Loss this iteration: 0.003171\n",
      "Encoder training step: 717/1000\n",
      "[DEBUG] Loss this iteration: 0.002917\n",
      "Encoder training step: 718/1000\n",
      "[DEBUG] Loss this iteration: 0.002799\n",
      "Encoder training step: 719/1000\n",
      "[DEBUG] Loss this iteration: 0.002844\n",
      "Encoder training step: 720/1000\n",
      "[DEBUG] Loss this iteration: 0.002925\n",
      "Encoder training step: 721/1000\n",
      "[DEBUG] Loss this iteration: 0.003000\n",
      "Encoder training step: 722/1000\n",
      "[DEBUG] Loss this iteration: 0.002969\n",
      "Encoder training step: 723/1000\n",
      "[DEBUG] Loss this iteration: 0.002581\n",
      "Encoder training step: 724/1000\n",
      "[DEBUG] Loss this iteration: 0.003061\n",
      "Encoder training step: 725/1000\n",
      "[DEBUG] Loss this iteration: 0.003040\n",
      "Encoder training step: 726/1000\n",
      "[DEBUG] Loss this iteration: 0.002997\n",
      "Encoder training step: 727/1000\n",
      "[DEBUG] Loss this iteration: 0.002583\n",
      "Encoder training step: 728/1000\n",
      "[DEBUG] Loss this iteration: 0.003173\n",
      "Encoder training step: 729/1000\n",
      "[DEBUG] Loss this iteration: 0.003230\n",
      "Encoder training step: 730/1000\n",
      "[DEBUG] Loss this iteration: 0.002883\n",
      "Encoder training step: 731/1000\n",
      "[DEBUG] Loss this iteration: 0.003209\n",
      "Encoder training step: 732/1000\n",
      "[DEBUG] Loss this iteration: 0.002960\n",
      "Encoder training step: 733/1000\n",
      "[DEBUG] Loss this iteration: 0.003096\n",
      "Encoder training step: 734/1000\n",
      "[DEBUG] Loss this iteration: 0.002832\n",
      "Encoder training step: 735/1000\n",
      "[DEBUG] Loss this iteration: 0.002760\n",
      "Encoder training step: 736/1000\n",
      "[DEBUG] Loss this iteration: 0.002941\n",
      "Encoder training step: 737/1000\n",
      "[DEBUG] Loss this iteration: 0.002791\n",
      "Encoder training step: 738/1000\n",
      "[DEBUG] Loss this iteration: 0.002867\n",
      "Encoder training step: 739/1000\n",
      "[DEBUG] Loss this iteration: 0.002689\n",
      "Encoder training step: 740/1000\n",
      "[DEBUG] Loss this iteration: 0.002710\n",
      "Encoder training step: 741/1000\n",
      "[DEBUG] Loss this iteration: 0.002760\n",
      "Encoder training step: 742/1000\n",
      "[DEBUG] Loss this iteration: 0.002799\n",
      "Encoder training step: 743/1000\n",
      "[DEBUG] Loss this iteration: 0.003000\n",
      "Encoder training step: 744/1000\n",
      "[DEBUG] Loss this iteration: 0.003015\n",
      "Encoder training step: 745/1000\n",
      "[DEBUG] Loss this iteration: 0.003035\n",
      "Encoder training step: 746/1000\n",
      "[DEBUG] Loss this iteration: 0.002945\n",
      "Encoder training step: 747/1000\n",
      "[DEBUG] Loss this iteration: 0.002861\n",
      "Encoder training step: 748/1000\n",
      "[DEBUG] Loss this iteration: 0.002858\n",
      "Encoder training step: 749/1000\n",
      "[DEBUG] Loss this iteration: 0.002660\n",
      "Encoder training step: 750/1000\n",
      "[DEBUG] Loss this iteration: 0.002626\n",
      "Encoder training step: 751/1000\n",
      "[DEBUG] Loss this iteration: 0.003054\n",
      "Encoder training step: 752/1000\n",
      "[DEBUG] Loss this iteration: 0.002882\n",
      "Encoder training step: 753/1000\n",
      "[DEBUG] Loss this iteration: 0.002941\n",
      "Encoder training step: 754/1000\n",
      "[DEBUG] Loss this iteration: 0.002743\n",
      "Encoder training step: 755/1000\n",
      "[DEBUG] Loss this iteration: 0.002845\n",
      "Encoder training step: 756/1000\n",
      "[DEBUG] Loss this iteration: 0.002855\n",
      "Encoder training step: 757/1000\n",
      "[DEBUG] Loss this iteration: 0.002752\n",
      "Encoder training step: 758/1000\n",
      "[DEBUG] Loss this iteration: 0.002869\n",
      "Encoder training step: 759/1000\n",
      "[DEBUG] Loss this iteration: 0.003048\n",
      "Encoder training step: 760/1000\n",
      "[DEBUG] Loss this iteration: 0.002995\n",
      "Encoder training step: 761/1000\n",
      "[DEBUG] Loss this iteration: 0.003506\n",
      "Encoder training step: 762/1000\n",
      "[DEBUG] Loss this iteration: 0.002909\n",
      "Encoder training step: 763/1000\n",
      "[DEBUG] Loss this iteration: 0.002692\n",
      "Encoder training step: 764/1000\n",
      "[DEBUG] Loss this iteration: 0.002823\n",
      "Encoder training step: 765/1000\n",
      "[DEBUG] Loss this iteration: 0.002990\n",
      "Encoder training step: 766/1000\n",
      "[DEBUG] Loss this iteration: 0.002881\n",
      "Encoder training step: 767/1000\n",
      "[DEBUG] Loss this iteration: 0.002587\n",
      "Encoder training step: 768/1000\n",
      "[DEBUG] Loss this iteration: 0.002823\n",
      "Encoder training step: 769/1000\n",
      "[DEBUG] Loss this iteration: 0.002779\n",
      "Encoder training step: 770/1000\n",
      "[DEBUG] Loss this iteration: 0.002897\n",
      "Encoder training step: 771/1000\n",
      "[DEBUG] Loss this iteration: 0.002473\n",
      "Encoder training step: 772/1000\n",
      "[DEBUG] Loss this iteration: 0.002787\n",
      "Encoder training step: 773/1000\n",
      "[DEBUG] Loss this iteration: 0.002863\n",
      "Encoder training step: 774/1000\n",
      "[DEBUG] Loss this iteration: 0.002872\n",
      "Encoder training step: 775/1000\n",
      "[DEBUG] Loss this iteration: 0.002867\n",
      "Encoder training step: 776/1000\n",
      "[DEBUG] Loss this iteration: 0.002754\n",
      "Encoder training step: 777/1000\n",
      "[DEBUG] Loss this iteration: 0.002706\n",
      "Encoder training step: 778/1000\n",
      "[DEBUG] Loss this iteration: 0.003028\n",
      "Encoder training step: 779/1000\n",
      "[DEBUG] Loss this iteration: 0.002812\n",
      "Encoder training step: 780/1000\n",
      "[DEBUG] Loss this iteration: 0.002981\n",
      "Encoder training step: 781/1000\n",
      "[DEBUG] Loss this iteration: 0.003080\n",
      "Encoder training step: 782/1000\n",
      "[DEBUG] Loss this iteration: 0.003032\n",
      "Encoder training step: 783/1000\n",
      "[DEBUG] Loss this iteration: 0.002671\n",
      "Encoder training step: 784/1000\n",
      "[DEBUG] Loss this iteration: 0.003001\n",
      "Encoder training step: 785/1000\n",
      "[DEBUG] Loss this iteration: 0.002767\n",
      "Encoder training step: 786/1000\n",
      "[DEBUG] Loss this iteration: 0.002947\n",
      "Encoder training step: 787/1000\n",
      "[DEBUG] Loss this iteration: 0.002876\n",
      "Encoder training step: 788/1000\n",
      "[DEBUG] Loss this iteration: 0.002697\n",
      "Encoder training step: 789/1000\n",
      "[DEBUG] Loss this iteration: 0.003017\n",
      "Encoder training step: 790/1000\n",
      "[DEBUG] Loss this iteration: 0.002683\n",
      "Encoder training step: 791/1000\n",
      "[DEBUG] Loss this iteration: 0.002658\n",
      "Encoder training step: 792/1000\n",
      "[DEBUG] Loss this iteration: 0.002751\n",
      "Encoder training step: 793/1000\n",
      "[DEBUG] Loss this iteration: 0.003207\n",
      "Encoder training step: 794/1000\n",
      "[DEBUG] Loss this iteration: 0.002822\n",
      "Encoder training step: 795/1000\n",
      "[DEBUG] Loss this iteration: 0.002910\n",
      "Encoder training step: 796/1000\n",
      "[DEBUG] Loss this iteration: 0.002553\n",
      "Encoder training step: 797/1000\n",
      "[DEBUG] Loss this iteration: 0.002756\n",
      "Encoder training step: 798/1000\n",
      "[DEBUG] Loss this iteration: 0.002934\n",
      "Encoder training step: 799/1000\n",
      "[DEBUG] Loss this iteration: 0.002949\n",
      "Encoder training step: 800/1000\n",
      "[DEBUG] Loss this iteration: 0.002867\n",
      "Encoder training step: 801/1000\n",
      "[DEBUG] Loss this iteration: 0.002781\n",
      "Encoder training step: 802/1000\n",
      "[DEBUG] Loss this iteration: 0.002920\n",
      "Encoder training step: 803/1000\n",
      "[DEBUG] Loss this iteration: 0.002943\n",
      "Encoder training step: 804/1000\n",
      "[DEBUG] Loss this iteration: 0.003301\n",
      "Encoder training step: 805/1000\n",
      "[DEBUG] Loss this iteration: 0.003066\n",
      "Encoder training step: 806/1000\n",
      "[DEBUG] Loss this iteration: 0.003169\n",
      "Encoder training step: 807/1000\n",
      "[DEBUG] Loss this iteration: 0.003197\n",
      "Encoder training step: 808/1000\n",
      "[DEBUG] Loss this iteration: 0.003090\n",
      "Encoder training step: 809/1000\n",
      "[DEBUG] Loss this iteration: 0.002880\n",
      "Encoder training step: 810/1000\n",
      "[DEBUG] Loss this iteration: 0.002687\n",
      "Encoder training step: 811/1000\n",
      "[DEBUG] Loss this iteration: 0.002889\n",
      "Encoder training step: 812/1000\n",
      "[DEBUG] Loss this iteration: 0.003030\n",
      "Encoder training step: 813/1000\n",
      "[DEBUG] Loss this iteration: 0.002770\n",
      "Encoder training step: 814/1000\n",
      "[DEBUG] Loss this iteration: 0.002638\n",
      "Encoder training step: 815/1000\n",
      "[DEBUG] Loss this iteration: 0.002589\n",
      "Encoder training step: 816/1000\n",
      "[DEBUG] Loss this iteration: 0.002645\n",
      "Encoder training step: 817/1000\n",
      "[DEBUG] Loss this iteration: 0.002875\n",
      "Encoder training step: 818/1000\n",
      "[DEBUG] Loss this iteration: 0.002836\n",
      "Encoder training step: 819/1000\n",
      "[DEBUG] Loss this iteration: 0.002677\n",
      "Encoder training step: 820/1000\n",
      "[DEBUG] Loss this iteration: 0.002454\n",
      "Encoder training step: 821/1000\n",
      "[DEBUG] Loss this iteration: 0.002838\n",
      "Encoder training step: 822/1000\n",
      "[DEBUG] Loss this iteration: 0.002891\n",
      "Encoder training step: 823/1000\n",
      "[DEBUG] Loss this iteration: 0.002595\n",
      "Encoder training step: 824/1000\n",
      "[DEBUG] Loss this iteration: 0.002781\n",
      "Encoder training step: 825/1000\n",
      "[DEBUG] Loss this iteration: 0.002642\n",
      "Encoder training step: 826/1000\n",
      "[DEBUG] Loss this iteration: 0.002700\n",
      "Encoder training step: 827/1000\n",
      "[DEBUG] Loss this iteration: 0.002936\n",
      "Encoder training step: 828/1000\n",
      "[DEBUG] Loss this iteration: 0.002701\n",
      "Encoder training step: 829/1000\n",
      "[DEBUG] Loss this iteration: 0.002718\n",
      "Encoder training step: 830/1000\n",
      "[DEBUG] Loss this iteration: 0.002591\n",
      "Encoder training step: 831/1000\n",
      "[DEBUG] Loss this iteration: 0.002651\n",
      "Encoder training step: 832/1000\n",
      "[DEBUG] Loss this iteration: 0.002985\n",
      "Encoder training step: 833/1000\n",
      "[DEBUG] Loss this iteration: 0.002791\n",
      "Encoder training step: 834/1000\n",
      "[DEBUG] Loss this iteration: 0.002747\n",
      "Encoder training step: 835/1000\n",
      "[DEBUG] Loss this iteration: 0.002835\n",
      "Encoder training step: 836/1000\n",
      "[DEBUG] Loss this iteration: 0.002500\n",
      "Encoder training step: 837/1000\n",
      "[DEBUG] Loss this iteration: 0.002908\n",
      "Encoder training step: 838/1000\n",
      "[DEBUG] Loss this iteration: 0.002603\n",
      "Encoder training step: 839/1000\n",
      "[DEBUG] Loss this iteration: 0.002537\n",
      "Encoder training step: 840/1000\n",
      "[DEBUG] Loss this iteration: 0.002800\n",
      "Encoder training step: 841/1000\n",
      "[DEBUG] Loss this iteration: 0.003174\n",
      "Encoder training step: 842/1000\n",
      "[DEBUG] Loss this iteration: 0.003056\n",
      "Encoder training step: 843/1000\n",
      "[DEBUG] Loss this iteration: 0.002867\n",
      "Encoder training step: 844/1000\n",
      "[DEBUG] Loss this iteration: 0.003007\n",
      "Encoder training step: 845/1000\n",
      "[DEBUG] Loss this iteration: 0.003044\n",
      "Encoder training step: 846/1000\n",
      "[DEBUG] Loss this iteration: 0.003071\n",
      "Encoder training step: 847/1000\n",
      "[DEBUG] Loss this iteration: 0.003230\n",
      "Encoder training step: 848/1000\n",
      "[DEBUG] Loss this iteration: 0.002833\n",
      "Encoder training step: 849/1000\n",
      "[DEBUG] Loss this iteration: 0.002586\n",
      "Encoder training step: 850/1000\n",
      "[DEBUG] Loss this iteration: 0.002664\n",
      "Encoder training step: 851/1000\n",
      "[DEBUG] Loss this iteration: 0.003079\n",
      "Encoder training step: 852/1000\n",
      "[DEBUG] Loss this iteration: 0.002723\n",
      "Encoder training step: 853/1000\n",
      "[DEBUG] Loss this iteration: 0.002914\n",
      "Encoder training step: 854/1000\n",
      "[DEBUG] Loss this iteration: 0.002671\n",
      "Encoder training step: 855/1000\n",
      "[DEBUG] Loss this iteration: 0.002940\n",
      "Encoder training step: 856/1000\n",
      "[DEBUG] Loss this iteration: 0.002627\n",
      "Encoder training step: 857/1000\n",
      "[DEBUG] Loss this iteration: 0.002404\n",
      "Encoder training step: 858/1000\n",
      "[DEBUG] Loss this iteration: 0.002776\n",
      "Encoder training step: 859/1000\n",
      "[DEBUG] Loss this iteration: 0.002696\n",
      "Encoder training step: 860/1000\n",
      "[DEBUG] Loss this iteration: 0.002989\n",
      "Encoder training step: 861/1000\n",
      "[DEBUG] Loss this iteration: 0.003451\n",
      "Encoder training step: 862/1000\n",
      "[DEBUG] Loss this iteration: 0.003077\n",
      "Encoder training step: 863/1000\n",
      "[DEBUG] Loss this iteration: 0.002800\n",
      "Encoder training step: 864/1000\n",
      "[DEBUG] Loss this iteration: 0.002950\n",
      "Encoder training step: 865/1000\n",
      "[DEBUG] Loss this iteration: 0.002919\n",
      "Encoder training step: 866/1000\n",
      "[DEBUG] Loss this iteration: 0.002769\n",
      "Encoder training step: 867/1000\n",
      "[DEBUG] Loss this iteration: 0.003175\n",
      "Encoder training step: 868/1000\n",
      "[DEBUG] Loss this iteration: 0.003045\n",
      "Encoder training step: 869/1000\n",
      "[DEBUG] Loss this iteration: 0.003244\n",
      "Encoder training step: 870/1000\n",
      "[DEBUG] Loss this iteration: 0.002897\n",
      "Encoder training step: 871/1000\n",
      "[DEBUG] Loss this iteration: 0.002727\n",
      "Encoder training step: 872/1000\n",
      "[DEBUG] Loss this iteration: 0.002750\n",
      "Encoder training step: 873/1000\n",
      "[DEBUG] Loss this iteration: 0.002589\n",
      "Encoder training step: 874/1000\n",
      "[DEBUG] Loss this iteration: 0.002544\n",
      "Encoder training step: 875/1000\n",
      "[DEBUG] Loss this iteration: 0.002714\n",
      "Encoder training step: 876/1000\n",
      "[DEBUG] Loss this iteration: 0.002360\n",
      "Encoder training step: 877/1000\n",
      "[DEBUG] Loss this iteration: 0.002415\n",
      "Encoder training step: 878/1000\n",
      "[DEBUG] Loss this iteration: 0.002658\n",
      "Encoder training step: 879/1000\n",
      "[DEBUG] Loss this iteration: 0.002716\n",
      "Encoder training step: 880/1000\n",
      "[DEBUG] Loss this iteration: 0.002790\n",
      "Encoder training step: 881/1000\n",
      "[DEBUG] Loss this iteration: 0.002900\n",
      "Encoder training step: 882/1000\n",
      "[DEBUG] Loss this iteration: 0.002705\n",
      "Encoder training step: 883/1000\n",
      "[DEBUG] Loss this iteration: 0.002582\n",
      "Encoder training step: 884/1000\n",
      "[DEBUG] Loss this iteration: 0.002616\n",
      "Encoder training step: 885/1000\n",
      "[DEBUG] Loss this iteration: 0.002873\n",
      "Encoder training step: 886/1000\n",
      "[DEBUG] Loss this iteration: 0.002547\n",
      "Encoder training step: 887/1000\n",
      "[DEBUG] Loss this iteration: 0.002812\n",
      "Encoder training step: 888/1000\n",
      "[DEBUG] Loss this iteration: 0.002568\n",
      "Encoder training step: 889/1000\n",
      "[DEBUG] Loss this iteration: 0.002845\n",
      "Encoder training step: 890/1000\n",
      "[DEBUG] Loss this iteration: 0.002450\n",
      "Encoder training step: 891/1000\n",
      "[DEBUG] Loss this iteration: 0.002663\n",
      "Encoder training step: 892/1000\n",
      "[DEBUG] Loss this iteration: 0.002529\n",
      "Encoder training step: 893/1000\n",
      "[DEBUG] Loss this iteration: 0.002749\n",
      "Encoder training step: 894/1000\n",
      "[DEBUG] Loss this iteration: 0.002761\n",
      "Encoder training step: 895/1000\n",
      "[DEBUG] Loss this iteration: 0.002515\n",
      "Encoder training step: 896/1000\n",
      "[DEBUG] Loss this iteration: 0.002352\n",
      "Encoder training step: 897/1000\n",
      "[DEBUG] Loss this iteration: 0.002461\n",
      "Encoder training step: 898/1000\n",
      "[DEBUG] Loss this iteration: 0.002672\n",
      "Encoder training step: 899/1000\n",
      "[DEBUG] Loss this iteration: 0.002653\n",
      "Encoder training step: 900/1000\n",
      "[DEBUG] Loss this iteration: 0.002585\n",
      "Encoder training step: 901/1000\n",
      "[DEBUG] Loss this iteration: 0.002385\n",
      "Encoder training step: 902/1000\n",
      "[DEBUG] Loss this iteration: 0.002488\n",
      "Encoder training step: 903/1000\n",
      "[DEBUG] Loss this iteration: 0.002744\n",
      "Encoder training step: 904/1000\n",
      "[DEBUG] Loss this iteration: 0.002588\n",
      "Encoder training step: 905/1000\n",
      "[DEBUG] Loss this iteration: 0.002541\n",
      "Encoder training step: 906/1000\n",
      "[DEBUG] Loss this iteration: 0.002628\n",
      "Encoder training step: 907/1000\n",
      "[DEBUG] Loss this iteration: 0.002705\n",
      "Encoder training step: 908/1000\n",
      "[DEBUG] Loss this iteration: 0.002481\n",
      "Encoder training step: 909/1000\n",
      "[DEBUG] Loss this iteration: 0.002711\n",
      "Encoder training step: 910/1000\n",
      "[DEBUG] Loss this iteration: 0.002724\n",
      "Encoder training step: 911/1000\n",
      "[DEBUG] Loss this iteration: 0.002551\n",
      "Encoder training step: 912/1000\n",
      "[DEBUG] Loss this iteration: 0.002672\n",
      "Encoder training step: 913/1000\n",
      "[DEBUG] Loss this iteration: 0.002573\n",
      "Encoder training step: 914/1000\n",
      "[DEBUG] Loss this iteration: 0.002429\n",
      "Encoder training step: 915/1000\n",
      "[DEBUG] Loss this iteration: 0.002277\n",
      "Encoder training step: 916/1000\n",
      "[DEBUG] Loss this iteration: 0.002334\n",
      "Encoder training step: 917/1000\n",
      "[DEBUG] Loss this iteration: 0.002585\n",
      "Encoder training step: 918/1000\n",
      "[DEBUG] Loss this iteration: 0.002698\n",
      "Encoder training step: 919/1000\n",
      "[DEBUG] Loss this iteration: 0.002592\n",
      "Encoder training step: 920/1000\n",
      "[DEBUG] Loss this iteration: 0.002518\n",
      "Encoder training step: 921/1000\n",
      "[DEBUG] Loss this iteration: 0.002336\n",
      "Encoder training step: 922/1000\n",
      "[DEBUG] Loss this iteration: 0.002599\n",
      "Encoder training step: 923/1000\n",
      "[DEBUG] Loss this iteration: 0.002538\n",
      "Encoder training step: 924/1000\n",
      "[DEBUG] Loss this iteration: 0.002539\n",
      "Encoder training step: 925/1000\n",
      "[DEBUG] Loss this iteration: 0.002592\n",
      "Encoder training step: 926/1000\n",
      "[DEBUG] Loss this iteration: 0.002326\n",
      "Encoder training step: 927/1000\n",
      "[DEBUG] Loss this iteration: 0.002171\n",
      "Encoder training step: 928/1000\n",
      "[DEBUG] Loss this iteration: 0.002348\n",
      "Encoder training step: 929/1000\n",
      "[DEBUG] Loss this iteration: 0.002593\n",
      "Encoder training step: 930/1000\n",
      "[DEBUG] Loss this iteration: 0.002458\n",
      "Encoder training step: 931/1000\n",
      "[DEBUG] Loss this iteration: 0.002788\n",
      "Encoder training step: 932/1000\n",
      "[DEBUG] Loss this iteration: 0.003016\n",
      "Encoder training step: 933/1000\n",
      "[DEBUG] Loss this iteration: 0.002933\n",
      "Encoder training step: 934/1000\n",
      "[DEBUG] Loss this iteration: 0.002584\n",
      "Encoder training step: 935/1000\n",
      "[DEBUG] Loss this iteration: 0.002491\n",
      "Encoder training step: 936/1000\n",
      "[DEBUG] Loss this iteration: 0.002379\n",
      "Encoder training step: 937/1000\n",
      "[DEBUG] Loss this iteration: 0.002246\n",
      "Encoder training step: 938/1000\n",
      "[DEBUG] Loss this iteration: 0.002642\n",
      "Encoder training step: 939/1000\n",
      "[DEBUG] Loss this iteration: 0.002399\n",
      "Encoder training step: 940/1000\n",
      "[DEBUG] Loss this iteration: 0.002398\n",
      "Encoder training step: 941/1000\n",
      "[DEBUG] Loss this iteration: 0.002450\n",
      "Encoder training step: 942/1000\n",
      "[DEBUG] Loss this iteration: 0.002503\n",
      "Encoder training step: 943/1000\n",
      "[DEBUG] Loss this iteration: 0.002536\n",
      "Encoder training step: 944/1000\n",
      "[DEBUG] Loss this iteration: 0.002641\n",
      "Encoder training step: 945/1000\n",
      "[DEBUG] Loss this iteration: 0.002373\n",
      "Encoder training step: 946/1000\n",
      "[DEBUG] Loss this iteration: 0.002560\n",
      "Encoder training step: 947/1000\n",
      "[DEBUG] Loss this iteration: 0.002470\n",
      "Encoder training step: 948/1000\n",
      "[DEBUG] Loss this iteration: 0.002528\n",
      "Encoder training step: 949/1000\n",
      "[DEBUG] Loss this iteration: 0.002525\n",
      "Encoder training step: 950/1000\n",
      "[DEBUG] Loss this iteration: 0.002800\n",
      "Encoder training step: 951/1000\n",
      "[DEBUG] Loss this iteration: 0.002618\n",
      "Encoder training step: 952/1000\n",
      "[DEBUG] Loss this iteration: 0.002550\n",
      "Encoder training step: 953/1000\n",
      "[DEBUG] Loss this iteration: 0.002528\n",
      "Encoder training step: 954/1000\n",
      "[DEBUG] Loss this iteration: 0.002453\n",
      "Encoder training step: 955/1000\n",
      "[DEBUG] Loss this iteration: 0.002731\n",
      "Encoder training step: 956/1000\n",
      "[DEBUG] Loss this iteration: 0.002356\n",
      "Encoder training step: 957/1000\n",
      "[DEBUG] Loss this iteration: 0.002280\n",
      "Encoder training step: 958/1000\n",
      "[DEBUG] Loss this iteration: 0.002179\n",
      "Encoder training step: 959/1000\n",
      "[DEBUG] Loss this iteration: 0.002632\n",
      "Encoder training step: 960/1000\n",
      "[DEBUG] Loss this iteration: 0.002646\n",
      "Encoder training step: 961/1000\n",
      "[DEBUG] Loss this iteration: 0.002618\n",
      "Encoder training step: 962/1000\n",
      "[DEBUG] Loss this iteration: 0.002623\n",
      "Encoder training step: 963/1000\n",
      "[DEBUG] Loss this iteration: 0.002427\n",
      "Encoder training step: 964/1000\n",
      "[DEBUG] Loss this iteration: 0.002407\n",
      "Encoder training step: 965/1000\n",
      "[DEBUG] Loss this iteration: 0.002440\n",
      "Encoder training step: 966/1000\n",
      "[DEBUG] Loss this iteration: 0.002343\n",
      "Encoder training step: 967/1000\n",
      "[DEBUG] Loss this iteration: 0.002321\n",
      "Encoder training step: 968/1000\n",
      "[DEBUG] Loss this iteration: 0.002239\n",
      "Encoder training step: 969/1000\n",
      "[DEBUG] Loss this iteration: 0.002259\n",
      "Encoder training step: 970/1000\n",
      "[DEBUG] Loss this iteration: 0.002360\n",
      "Encoder training step: 971/1000\n",
      "[DEBUG] Loss this iteration: 0.002379\n",
      "Encoder training step: 972/1000\n",
      "[DEBUG] Loss this iteration: 0.002238\n",
      "Encoder training step: 973/1000\n",
      "[DEBUG] Loss this iteration: 0.002444\n",
      "Encoder training step: 974/1000\n",
      "[DEBUG] Loss this iteration: 0.002340\n",
      "Encoder training step: 975/1000\n",
      "[DEBUG] Loss this iteration: 0.002375\n",
      "Encoder training step: 976/1000\n",
      "[DEBUG] Loss this iteration: 0.002565\n",
      "Encoder training step: 977/1000\n",
      "[DEBUG] Loss this iteration: 0.002565\n",
      "Encoder training step: 978/1000\n",
      "[DEBUG] Loss this iteration: 0.002819\n",
      "Encoder training step: 979/1000\n",
      "[DEBUG] Loss this iteration: 0.002976\n",
      "Encoder training step: 980/1000\n",
      "[DEBUG] Loss this iteration: 0.003229\n",
      "Encoder training step: 981/1000\n",
      "[DEBUG] Loss this iteration: 0.002954\n",
      "Encoder training step: 982/1000\n",
      "[DEBUG] Loss this iteration: 0.002564\n",
      "Encoder training step: 983/1000\n",
      "[DEBUG] Loss this iteration: 0.002515\n",
      "Encoder training step: 984/1000\n",
      "[DEBUG] Loss this iteration: 0.002533\n",
      "Encoder training step: 985/1000\n",
      "[DEBUG] Loss this iteration: 0.002442\n",
      "Encoder training step: 986/1000\n",
      "[DEBUG] Loss this iteration: 0.002306\n",
      "Encoder training step: 987/1000\n",
      "[DEBUG] Loss this iteration: 0.002844\n",
      "Encoder training step: 988/1000\n",
      "[DEBUG] Loss this iteration: 0.002885\n",
      "Encoder training step: 989/1000\n",
      "[DEBUG] Loss this iteration: 0.002677\n",
      "Encoder training step: 990/1000\n",
      "[DEBUG] Loss this iteration: 0.002494\n",
      "Encoder training step: 991/1000\n",
      "[DEBUG] Loss this iteration: 0.002324\n",
      "Encoder training step: 992/1000\n",
      "[DEBUG] Loss this iteration: 0.002028\n",
      "Encoder training step: 993/1000\n",
      "[DEBUG] Loss this iteration: 0.002419\n",
      "Encoder training step: 994/1000\n",
      "[DEBUG] Loss this iteration: 0.002757\n",
      "Encoder training step: 995/1000\n",
      "[DEBUG] Loss this iteration: 0.002400\n",
      "Encoder training step: 996/1000\n",
      "[DEBUG] Loss this iteration: 0.002413\n",
      "Encoder training step: 997/1000\n",
      "[DEBUG] Loss this iteration: 0.002202\n",
      "Encoder training step: 998/1000\n",
      "[DEBUG] Loss this iteration: 0.002200\n",
      "Encoder training step: 999/1000\n",
      "Loss S:  tensor(0.0932, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 0/1000\n",
      "Loss S:  tensor(0.0849, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 1/1000\n",
      "Loss S:  tensor(0.0777, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 2/1000\n",
      "Loss S:  tensor(0.0704, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 3/1000\n",
      "Loss S:  tensor(0.0643, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 4/1000\n",
      "Loss S:  tensor(0.0566, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 5/1000\n",
      "Loss S:  tensor(0.0538, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 6/1000\n",
      "Loss S:  tensor(0.0489, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 7/1000\n",
      "Loss S:  tensor(0.0441, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 8/1000\n",
      "Loss S:  tensor(0.0415, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 9/1000\n",
      "Loss S:  tensor(0.0383, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 10/1000\n",
      "Loss S:  tensor(0.0352, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 11/1000\n",
      "Loss S:  tensor(0.0360, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 12/1000\n",
      "Loss S:  tensor(0.0314, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 13/1000\n",
      "Loss S:  tensor(0.0299, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 14/1000\n",
      "Loss S:  tensor(0.0302, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 15/1000\n",
      "Loss S:  tensor(0.0269, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 16/1000\n",
      "Loss S:  tensor(0.0270, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 17/1000\n",
      "Loss S:  tensor(0.0286, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 18/1000\n",
      "Loss S:  tensor(0.0241, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 19/1000\n",
      "Loss S:  tensor(0.0252, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 20/1000\n",
      "Loss S:  tensor(0.0223, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 21/1000\n",
      "Loss S:  tensor(0.0221, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 22/1000\n",
      "Loss S:  tensor(0.0230, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 23/1000\n",
      "Loss S:  tensor(0.0232, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 24/1000\n",
      "Loss S:  tensor(0.0233, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 25/1000\n",
      "Loss S:  tensor(0.0224, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 26/1000\n",
      "Loss S:  tensor(0.0205, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 27/1000\n",
      "Loss S:  tensor(0.0197, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 28/1000\n",
      "Loss S:  tensor(0.0195, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 29/1000\n",
      "Loss S:  tensor(0.0195, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 30/1000\n",
      "Loss S:  tensor(0.0184, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 31/1000\n",
      "Loss S:  tensor(0.0168, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 32/1000\n",
      "Loss S:  tensor(0.0178, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 33/1000\n",
      "Loss S:  tensor(0.0161, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 34/1000\n",
      "Loss S:  tensor(0.0202, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 35/1000\n",
      "Loss S:  tensor(0.0167, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 36/1000\n",
      "Loss S:  tensor(0.0148, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 37/1000\n",
      "Loss S:  tensor(0.0180, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 38/1000\n",
      "Loss S:  tensor(0.0208, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 39/1000\n",
      "Loss S:  tensor(0.0167, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 40/1000\n",
      "Loss S:  tensor(0.0178, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 41/1000\n",
      "Loss S:  tensor(0.0146, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 42/1000\n",
      "Loss S:  tensor(0.0138, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 43/1000\n",
      "Loss S:  tensor(0.0166, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 44/1000\n",
      "Loss S:  tensor(0.0154, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 45/1000\n",
      "Loss S:  tensor(0.0140, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 46/1000\n",
      "Loss S:  tensor(0.0155, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 47/1000\n",
      "Loss S:  tensor(0.0161, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 48/1000\n",
      "Loss S:  tensor(0.0157, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 49/1000\n",
      "Loss S:  tensor(0.0143, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 50/1000\n",
      "Loss S:  tensor(0.0135, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 51/1000\n",
      "Loss S:  tensor(0.0172, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 52/1000\n",
      "Loss S:  tensor(0.0120, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 53/1000\n",
      "Loss S:  tensor(0.0156, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 54/1000\n",
      "Loss S:  tensor(0.0154, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 55/1000\n",
      "Loss S:  tensor(0.0150, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 56/1000\n",
      "Loss S:  tensor(0.0145, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 57/1000\n",
      "Loss S:  tensor(0.0167, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 58/1000\n",
      "Loss S:  tensor(0.0131, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 59/1000\n",
      "Loss S:  tensor(0.0141, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 60/1000\n",
      "Loss S:  tensor(0.0147, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 61/1000\n",
      "Loss S:  tensor(0.0136, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 62/1000\n",
      "Loss S:  tensor(0.0155, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 63/1000\n",
      "Loss S:  tensor(0.0121, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 64/1000\n",
      "Loss S:  tensor(0.0141, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 65/1000\n",
      "Loss S:  tensor(0.0148, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 66/1000\n",
      "Loss S:  tensor(0.0121, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 67/1000\n",
      "Loss S:  tensor(0.0143, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 68/1000\n",
      "Loss S:  tensor(0.0118, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 69/1000\n",
      "Loss S:  tensor(0.0123, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 70/1000\n",
      "Loss S:  tensor(0.0120, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 71/1000\n",
      "Loss S:  tensor(0.0140, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 72/1000\n",
      "Loss S:  tensor(0.0123, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 73/1000\n",
      "Loss S:  tensor(0.0140, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 74/1000\n",
      "Loss S:  tensor(0.0116, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 75/1000\n",
      "Loss S:  tensor(0.0147, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 76/1000\n",
      "Loss S:  tensor(0.0112, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 77/1000\n",
      "Loss S:  tensor(0.0126, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 78/1000\n",
      "Loss S:  tensor(0.0137, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 79/1000\n",
      "Loss S:  tensor(0.0167, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 80/1000\n",
      "Loss S:  tensor(0.0162, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 81/1000\n",
      "Loss S:  tensor(0.0142, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 82/1000\n",
      "Loss S:  tensor(0.0110, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 83/1000\n",
      "Loss S:  tensor(0.0135, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 84/1000\n",
      "Loss S:  tensor(0.0115, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 85/1000\n",
      "Loss S:  tensor(0.0116, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 86/1000\n",
      "Loss S:  tensor(0.0131, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 87/1000\n",
      "Loss S:  tensor(0.0117, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 88/1000\n",
      "Loss S:  tensor(0.0159, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 89/1000\n",
      "Loss S:  tensor(0.0141, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 90/1000\n",
      "Loss S:  tensor(0.0129, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 91/1000\n",
      "Loss S:  tensor(0.0098, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 92/1000\n",
      "Loss S:  tensor(0.0112, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 93/1000\n",
      "Loss S:  tensor(0.0103, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 94/1000\n",
      "Loss S:  tensor(0.0149, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 95/1000\n",
      "Loss S:  tensor(0.0170, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 96/1000\n",
      "Loss S:  tensor(0.0141, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 97/1000\n",
      "Loss S:  tensor(0.0151, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 98/1000\n",
      "Loss S:  tensor(0.0136, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 99/1000\n",
      "Loss S:  tensor(0.0131, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 100/1000\n",
      "Loss S:  tensor(0.0126, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 101/1000\n",
      "Loss S:  tensor(0.0117, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 102/1000\n",
      "Loss S:  tensor(0.0110, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 103/1000\n",
      "Loss S:  tensor(0.0105, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 104/1000\n",
      "Loss S:  tensor(0.0121, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 105/1000\n",
      "Loss S:  tensor(0.0133, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 106/1000\n",
      "Loss S:  tensor(0.0129, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 107/1000\n",
      "Loss S:  tensor(0.0131, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 108/1000\n",
      "Loss S:  tensor(0.0101, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 109/1000\n",
      "Loss S:  tensor(0.0098, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 110/1000\n",
      "Loss S:  tensor(0.0113, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 111/1000\n",
      "Loss S:  tensor(0.0129, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 112/1000\n",
      "Loss S:  tensor(0.0123, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 113/1000\n",
      "Loss S:  tensor(0.0112, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 114/1000\n",
      "Loss S:  tensor(0.0126, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 115/1000\n",
      "Loss S:  tensor(0.0140, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 116/1000\n",
      "Loss S:  tensor(0.0144, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 117/1000\n",
      "Loss S:  tensor(0.0091, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 118/1000\n",
      "Loss S:  tensor(0.0128, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 119/1000\n",
      "Loss S:  tensor(0.0101, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 120/1000\n",
      "Loss S:  tensor(0.0110, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 121/1000\n",
      "Loss S:  tensor(0.0096, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 122/1000\n",
      "Loss S:  tensor(0.0125, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 123/1000\n",
      "Loss S:  tensor(0.0110, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 124/1000\n",
      "Loss S:  tensor(0.0114, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 125/1000\n",
      "Loss S:  tensor(0.0137, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 126/1000\n",
      "Loss S:  tensor(0.0113, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 127/1000\n",
      "Loss S:  tensor(0.0104, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 128/1000\n",
      "Loss S:  tensor(0.0127, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 129/1000\n",
      "Loss S:  tensor(0.0125, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 130/1000\n",
      "Loss S:  tensor(0.0127, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 131/1000\n",
      "Loss S:  tensor(0.0125, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 132/1000\n",
      "Loss S:  tensor(0.0116, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 133/1000\n",
      "Loss S:  tensor(0.0087, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 134/1000\n",
      "Loss S:  tensor(0.0112, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 135/1000\n",
      "Loss S:  tensor(0.0088, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 136/1000\n",
      "Loss S:  tensor(0.0107, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 137/1000\n",
      "Loss S:  tensor(0.0106, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 138/1000\n",
      "Loss S:  tensor(0.0105, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 139/1000\n",
      "Loss S:  tensor(0.0127, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 140/1000\n",
      "Loss S:  tensor(0.0104, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 141/1000\n",
      "Loss S:  tensor(0.0099, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 142/1000\n",
      "Loss S:  tensor(0.0112, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 143/1000\n",
      "Loss S:  tensor(0.0100, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 144/1000\n",
      "Loss S:  tensor(0.0119, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 145/1000\n",
      "Loss S:  tensor(0.0114, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 146/1000\n",
      "Loss S:  tensor(0.0131, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 147/1000\n",
      "Loss S:  tensor(0.0108, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 148/1000\n",
      "Loss S:  tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 149/1000\n",
      "Loss S:  tensor(0.0085, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 150/1000\n",
      "Loss S:  tensor(0.0087, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 151/1000\n",
      "Loss S:  tensor(0.0106, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 152/1000\n",
      "Loss S:  tensor(0.0086, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 153/1000\n",
      "Loss S:  tensor(0.0095, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 154/1000\n",
      "Loss S:  tensor(0.0119, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 155/1000\n",
      "Loss S:  tensor(0.0095, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 156/1000\n",
      "Loss S:  tensor(0.0083, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 157/1000\n",
      "Loss S:  tensor(0.0091, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 158/1000\n",
      "Loss S:  tensor(0.0105, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 159/1000\n",
      "Loss S:  tensor(0.0086, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 160/1000\n",
      "Loss S:  tensor(0.0082, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 161/1000\n",
      "Loss S:  tensor(0.0100, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 162/1000\n",
      "Loss S:  tensor(0.0085, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 163/1000\n",
      "Loss S:  tensor(0.0084, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 164/1000\n",
      "Loss S:  tensor(0.0069, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 165/1000\n",
      "Loss S:  tensor(0.0093, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 166/1000\n",
      "Loss S:  tensor(0.0087, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 167/1000\n",
      "Loss S:  tensor(0.0086, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 168/1000\n",
      "Loss S:  tensor(0.0082, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 169/1000\n",
      "Loss S:  tensor(0.0092, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 170/1000\n",
      "Loss S:  tensor(0.0081, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 171/1000\n",
      "Loss S:  tensor(0.0078, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 172/1000\n",
      "Loss S:  tensor(0.0099, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 173/1000\n",
      "Loss S:  tensor(0.0087, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 174/1000\n",
      "Loss S:  tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 175/1000\n",
      "Loss S:  tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 176/1000\n",
      "Loss S:  tensor(0.0085, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 177/1000\n",
      "Loss S:  tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 178/1000\n",
      "Loss S:  tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 179/1000\n",
      "Loss S:  tensor(0.0070, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 180/1000\n",
      "Loss S:  tensor(0.0078, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 181/1000\n",
      "Loss S:  tensor(0.0073, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 182/1000\n",
      "Loss S:  tensor(0.0077, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 183/1000\n",
      "Loss S:  tensor(0.0065, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 184/1000\n",
      "Loss S:  tensor(0.0070, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 185/1000\n",
      "Loss S:  tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 186/1000\n",
      "Loss S:  tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 187/1000\n",
      "Loss S:  tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 188/1000\n",
      "Loss S:  tensor(0.0073, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 189/1000\n",
      "Loss S:  tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 190/1000\n",
      "Loss S:  tensor(0.0069, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 191/1000\n",
      "Loss S:  tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 192/1000\n",
      "Loss S:  tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 193/1000\n",
      "Loss S:  tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 194/1000\n",
      "Loss S:  tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 195/1000\n",
      "Loss S:  tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 196/1000\n",
      "Loss S:  tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 197/1000\n",
      "Loss S:  tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 198/1000\n",
      "Loss S:  tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 199/1000\n",
      "Loss S:  tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 200/1000\n",
      "Loss S:  tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 201/1000\n",
      "Loss S:  tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 202/1000\n",
      "Loss S:  tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 203/1000\n",
      "Loss S:  tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 204/1000\n",
      "Loss S:  tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 205/1000\n",
      "Loss S:  tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 206/1000\n",
      "Loss S:  tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 207/1000\n",
      "Loss S:  tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 208/1000\n",
      "Loss S:  tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 209/1000\n",
      "Loss S:  tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 210/1000\n",
      "Loss S:  tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 211/1000\n",
      "Loss S:  tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 212/1000\n",
      "Loss S:  tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 213/1000\n",
      "Loss S:  tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 214/1000\n",
      "Loss S:  tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 215/1000\n",
      "Loss S:  tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 216/1000\n",
      "Loss S:  tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 217/1000\n",
      "Loss S:  tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 218/1000\n",
      "Loss S:  tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 219/1000\n",
      "Loss S:  tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 220/1000\n",
      "Loss S:  tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 221/1000\n",
      "Loss S:  tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 222/1000\n",
      "Loss S:  tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 223/1000\n",
      "Loss S:  tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 224/1000\n",
      "Loss S:  tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 225/1000\n",
      "Loss S:  tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 226/1000\n",
      "Loss S:  tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 227/1000\n",
      "Loss S:  tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 228/1000\n",
      "Loss S:  tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 229/1000\n",
      "Loss S:  tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 230/1000\n",
      "Loss S:  tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 231/1000\n",
      "Loss S:  tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 232/1000\n",
      "Loss S:  tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 233/1000\n",
      "Loss S:  tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 234/1000\n",
      "Loss S:  tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 235/1000\n",
      "Loss S:  tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 236/1000\n",
      "Loss S:  tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 237/1000\n",
      "Loss S:  tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 238/1000\n",
      "Loss S:  tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 239/1000\n",
      "Loss S:  tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 240/1000\n",
      "Loss S:  tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 241/1000\n",
      "Loss S:  tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 242/1000\n",
      "Loss S:  tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 243/1000\n",
      "Loss S:  tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 244/1000\n",
      "Loss S:  tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 245/1000\n",
      "Loss S:  tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 246/1000\n",
      "Loss S:  tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 247/1000\n",
      "Loss S:  tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 248/1000\n",
      "Loss S:  tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 249/1000\n",
      "Loss S:  tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 250/1000\n",
      "Loss S:  tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 251/1000\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 252/1000\n",
      "Loss S:  tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 253/1000\n",
      "Loss S:  tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 254/1000\n",
      "Loss S:  tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 255/1000\n",
      "Loss S:  tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 256/1000\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 257/1000\n",
      "Loss S:  tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 258/1000\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 259/1000\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 260/1000\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 261/1000\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 262/1000\n",
      "Loss S:  tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 263/1000\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 264/1000\n",
      "Loss S:  tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 265/1000\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 266/1000\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 267/1000\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 268/1000\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 269/1000\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 270/1000\n",
      "Loss S:  tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 271/1000\n",
      "Loss S:  tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 272/1000\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 273/1000\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 274/1000\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 275/1000\n",
      "Loss S:  tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 276/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 277/1000\n",
      "Loss S:  tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 278/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 279/1000\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 280/1000\n",
      "Loss S:  tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 281/1000\n",
      "Loss S:  tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 282/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 283/1000\n",
      "Loss S:  tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 284/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 285/1000\n",
      "Loss S:  tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 286/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 287/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 288/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 289/1000\n",
      "Loss S:  tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 290/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 291/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 292/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 293/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 294/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 295/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 296/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 297/1000\n",
      "Loss S:  tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 298/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 299/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 300/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 301/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 302/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 303/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 304/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 305/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 306/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 307/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 308/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 309/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 310/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 311/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 312/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 313/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 314/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 315/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 316/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 317/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 318/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 319/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 320/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 321/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 322/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 323/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 324/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 325/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 326/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 327/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 328/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 329/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 330/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 331/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 332/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 333/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 334/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 335/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 336/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 337/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 338/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 339/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 340/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 341/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 342/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 343/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 344/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 345/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 346/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 347/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 348/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 349/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 350/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 351/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 352/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 353/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 354/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 355/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 356/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 357/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 358/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 359/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 360/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 361/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 362/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 363/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 364/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 365/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 366/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 367/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 368/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 369/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 370/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 371/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 372/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 373/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 374/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 375/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 376/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 377/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 378/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 379/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 380/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 381/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 382/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 383/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 384/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 385/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 386/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 387/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 388/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 389/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 390/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 391/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 392/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 393/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 394/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 395/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 396/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 397/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 398/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 399/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 400/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 401/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 402/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 403/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 404/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 405/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 406/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 407/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 408/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 409/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 410/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 411/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 412/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 413/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 414/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 415/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 416/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 417/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 418/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 419/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 420/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 421/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 422/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 423/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 424/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 425/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 426/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 427/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 428/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 429/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 430/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 431/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 432/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 433/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 434/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 435/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 436/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 437/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 438/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 439/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 440/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 441/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 442/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 443/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 444/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 445/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 446/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 447/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 448/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 449/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 450/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 451/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 452/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 453/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 454/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 455/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 456/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 457/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 458/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 459/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 460/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 461/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 462/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 463/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 464/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 465/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 466/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 467/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 468/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 469/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 470/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 471/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 472/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 473/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 474/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 475/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 476/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 477/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 478/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 479/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 480/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 481/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 482/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 483/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 484/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 485/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 486/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 487/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 488/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 489/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 490/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 491/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 492/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 493/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 494/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 495/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 496/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 497/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 498/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 499/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 500/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 501/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 502/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 503/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 504/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 505/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 506/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 507/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 508/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 509/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 510/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 511/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 512/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 513/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 514/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 515/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 516/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 517/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 518/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 519/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 520/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 521/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 522/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 523/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 524/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 525/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 526/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 527/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 528/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 529/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 530/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 531/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 532/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 533/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 534/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 535/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 536/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 537/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 538/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 539/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 540/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 541/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 542/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 543/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 544/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 545/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 546/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 547/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 548/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 549/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 550/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 551/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 552/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 553/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 554/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 555/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 556/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 557/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 558/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 559/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 560/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 561/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 562/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 563/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 564/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 565/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 566/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 567/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 568/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 569/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 570/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 571/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 572/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 573/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 574/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 575/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 576/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 577/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 578/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 579/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 580/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 581/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 582/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 583/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 584/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 585/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 586/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 587/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 588/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 589/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 590/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 591/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 592/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 593/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 594/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 595/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 596/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 597/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 598/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 599/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 600/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 601/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 602/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 603/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 604/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 605/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 606/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 607/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 608/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 609/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 610/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 611/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 612/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 613/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 614/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 615/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 616/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 617/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 618/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 619/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 620/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 621/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 622/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 623/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 624/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 625/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 626/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 627/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 628/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 629/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 630/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 631/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 632/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 633/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 634/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 635/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 636/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 637/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 638/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 639/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 640/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 641/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 642/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 643/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 644/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 645/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 646/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 647/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 648/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 649/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 650/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 651/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 652/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 653/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 654/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 655/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 656/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 657/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 658/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 659/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 660/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 661/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 662/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 663/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 664/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 665/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 666/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 667/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 668/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 669/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 670/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 671/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 672/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 673/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 674/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 675/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 676/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 677/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 678/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 679/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 680/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 681/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 682/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 683/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 684/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 685/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 686/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 687/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 688/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 689/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 690/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 691/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 692/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 693/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 694/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 695/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 696/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 697/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 698/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 699/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 700/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 701/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 702/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 703/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 704/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 705/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 706/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 707/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 708/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 709/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 710/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 711/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 712/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 713/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 714/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 715/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 716/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 717/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 718/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 719/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 720/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 721/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 722/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 723/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 724/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 725/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 726/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 727/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 728/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 729/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 730/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 731/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 732/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 733/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 734/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 735/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 736/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 737/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 738/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 739/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 740/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 741/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 742/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 743/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 744/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 745/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 746/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 747/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 748/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 749/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 750/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 751/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 752/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 753/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 754/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 755/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 756/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 757/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 758/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 759/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 760/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 761/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 762/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 763/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 764/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 765/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 766/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 767/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 768/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 769/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 770/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 771/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 772/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 773/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 774/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 775/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 776/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 777/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 778/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 779/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 780/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 781/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 782/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 783/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 784/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 785/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 786/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 787/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 788/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 789/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 790/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 791/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 792/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 793/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 794/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 795/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 796/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 797/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 798/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 799/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 800/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 801/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 802/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 803/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 804/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 805/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 806/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 807/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 808/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 809/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 810/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 811/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 812/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 813/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 814/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 815/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 816/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 817/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 818/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 819/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 820/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 821/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 822/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 823/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 824/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 825/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 826/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 827/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 828/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 829/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 830/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 831/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 832/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 833/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 834/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 835/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 836/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 837/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 838/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 839/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 840/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 841/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 842/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 843/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 844/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 845/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 846/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 847/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 848/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 849/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 850/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 851/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 852/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 853/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 854/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 855/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 856/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 857/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 858/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 859/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 860/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 861/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 862/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 863/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 864/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 865/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 866/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 867/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 868/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 869/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 870/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 871/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 872/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 873/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 874/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 875/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 876/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 877/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 878/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 879/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 880/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 881/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 882/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 883/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 884/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 885/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 886/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 887/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 888/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 889/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 890/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 891/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 892/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 893/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 894/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 895/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 896/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 897/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 898/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 899/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 900/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 901/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 902/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 903/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 904/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 905/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 906/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 907/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 908/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 909/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 910/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 911/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 912/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 913/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 914/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 915/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 916/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 917/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 918/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 919/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 920/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 921/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 922/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 923/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 924/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 925/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 926/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 927/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 928/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 929/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 930/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 931/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 932/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 933/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 934/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 935/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 936/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 937/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 938/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 939/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 940/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 941/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 942/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 943/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 944/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 945/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 946/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 947/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 948/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 949/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 950/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 951/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 952/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 953/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 954/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 955/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 956/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 957/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 958/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 959/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 960/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 961/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 962/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 963/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 964/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 965/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 966/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 967/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 968/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 969/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 970/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 971/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 972/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 973/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 974/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 975/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 976/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 977/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 978/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 979/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 980/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 981/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 982/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 983/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 984/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 985/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 986/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 987/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 988/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 989/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 990/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 991/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 992/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 993/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 994/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 995/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 996/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 997/1000\n",
      "Loss S:  tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 998/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 999/1000\n",
      "Loss G (total):  tensor(11.0918, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 0/1000\n",
      "Loss G (total):  tensor(19.3010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 1/1000\n",
      "Loss G (total):  tensor(14.2374, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 2/1000\n",
      "Loss G (total):  tensor(13.6584, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 3/1000\n",
      "Loss G (total):  tensor(13.0992, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 4/1000\n",
      "Loss G (total):  tensor(11.8047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 5/1000\n",
      "Loss G (total):  tensor(14.0933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 6/1000\n",
      "Loss G (total):  tensor(15.2149, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 7/1000\n",
      "Loss G (total):  tensor(14.9517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 8/1000\n",
      "Loss G (total):  tensor(14.9822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 9/1000\n",
      "Loss G (total):  tensor(15.7884, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 10/1000\n",
      "Loss G (total):  tensor(17.5909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 11/1000\n",
      "Loss G (total):  tensor(18.0896, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 12/1000\n",
      "Loss G (total):  tensor(19.1980, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 13/1000\n",
      "Loss G (total):  tensor(21.4702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 14/1000\n",
      "Loss G (total):  tensor(20.1019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 15/1000\n",
      "Loss G (total):  tensor(21.0759, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 16/1000\n",
      "Loss G (total):  tensor(22.0163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 17/1000\n",
      "Loss G (total):  tensor(21.3839, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 18/1000\n",
      "Loss G (total):  tensor(21.9360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 19/1000\n",
      "Loss G (total):  tensor(22.9428, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 20/1000\n",
      "Loss G (total):  tensor(25.5511, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 21/1000\n",
      "Loss G (total):  tensor(23.8968, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 22/1000\n",
      "Loss G (total):  tensor(23.8479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 23/1000\n",
      "Loss G (total):  tensor(22.8173, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 24/1000\n",
      "Loss G (total):  tensor(24.4038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 25/1000\n",
      "Loss G (total):  tensor(26.4587, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 26/1000\n",
      "Loss G (total):  tensor(25.5229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 27/1000\n",
      "Loss G (total):  tensor(25.6955, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 28/1000\n",
      "Loss G (total):  tensor(26.1699, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 29/1000\n",
      "Loss G (total):  tensor(25.2621, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 30/1000\n",
      "Loss G (total):  tensor(26.5434, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 31/1000\n",
      "Loss G (total):  tensor(26.1169, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 32/1000\n",
      "Loss G (total):  tensor(24.9415, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 33/1000\n",
      "Loss G (total):  tensor(25.0569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 34/1000\n",
      "Loss G (total):  tensor(27.0151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 35/1000\n",
      "Loss G (total):  tensor(26.5577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 36/1000\n",
      "Loss G (total):  tensor(25.2638, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 37/1000\n",
      "Loss G (total):  tensor(26.5605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 38/1000\n",
      "Loss G (total):  tensor(26.8513, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 39/1000\n",
      "Loss G (total):  tensor(29.3545, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 40/1000\n",
      "Loss G (total):  tensor(26.4378, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 41/1000\n",
      "Loss G (total):  tensor(24.9212, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 42/1000\n",
      "Loss G (total):  tensor(26.1152, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 43/1000\n",
      "Loss G (total):  tensor(26.3767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 44/1000\n",
      "Loss G (total):  tensor(26.4155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 45/1000\n",
      "Loss G (total):  tensor(26.5769, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 46/1000\n",
      "Loss G (total):  tensor(27.5328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 47/1000\n",
      "Loss G (total):  tensor(26.4695, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 48/1000\n",
      "Loss G (total):  tensor(27.0402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 49/1000\n",
      "Loss G (total):  tensor(28.2357, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 50/1000\n",
      "Loss G (total):  tensor(27.2938, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 51/1000\n",
      "Loss G (total):  tensor(27.9419, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 52/1000\n",
      "Loss G (total):  tensor(28.9502, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 53/1000\n",
      "Loss G (total):  tensor(27.4972, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 54/1000\n",
      "Loss G (total):  tensor(26.8642, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 55/1000\n",
      "Loss G (total):  tensor(30.4782, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 56/1000\n",
      "Loss G (total):  tensor(27.7302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 57/1000\n",
      "Loss G (total):  tensor(28.5786, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 58/1000\n",
      "Loss G (total):  tensor(29.5373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 59/1000\n",
      "Loss G (total):  tensor(30.5124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 60/1000\n",
      "Loss G (total):  tensor(29.5372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 61/1000\n",
      "Loss G (total):  tensor(28.4703, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 62/1000\n",
      "Loss G (total):  tensor(28.9165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 63/1000\n",
      "Loss G (total):  tensor(31.8186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 64/1000\n",
      "Loss G (total):  tensor(29.2876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 65/1000\n",
      "Loss G (total):  tensor(29.4889, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 66/1000\n",
      "Loss G (total):  tensor(29.4260, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 67/1000\n",
      "Loss G (total):  tensor(29.1088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 68/1000\n",
      "Loss G (total):  tensor(30.0070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 69/1000\n",
      "Loss G (total):  tensor(29.9565, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 70/1000\n",
      "Loss G (total):  tensor(33.1590, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 71/1000\n",
      "Loss G (total):  tensor(29.9797, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 72/1000\n",
      "Loss G (total):  tensor(29.4255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 73/1000\n",
      "Loss G (total):  tensor(29.3908, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 74/1000\n",
      "Loss G (total):  tensor(31.0851, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 75/1000\n",
      "Loss G (total):  tensor(30.6077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 76/1000\n",
      "Loss G (total):  tensor(30.6599, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 77/1000\n",
      "Loss G (total):  tensor(30.0459, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 78/1000\n",
      "Loss G (total):  tensor(30.6591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 79/1000\n",
      "Loss G (total):  tensor(31.4559, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 80/1000\n",
      "Loss G (total):  tensor(30.8713, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 81/1000\n",
      "Loss G (total):  tensor(30.5064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 82/1000\n",
      "Loss G (total):  tensor(31.5620, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 83/1000\n",
      "Loss G (total):  tensor(32.4066, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 84/1000\n",
      "Loss G (total):  tensor(32.3230, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 85/1000\n",
      "Loss G (total):  tensor(31.4150, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 86/1000\n",
      "Loss G (total):  tensor(30.1437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 87/1000\n",
      "Loss G (total):  tensor(33.7915, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 88/1000\n",
      "Loss G (total):  tensor(34.1170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 89/1000\n",
      "Loss G (total):  tensor(31.4901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 90/1000\n",
      "Loss G (total):  tensor(32.6003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 91/1000\n",
      "Loss G (total):  tensor(34.1840, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 92/1000\n",
      "Loss G (total):  tensor(31.7697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 93/1000\n",
      "Loss G (total):  tensor(33.0404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 94/1000\n",
      "Loss G (total):  tensor(33.5522, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 95/1000\n",
      "Loss G (total):  tensor(32.4478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 96/1000\n",
      "Loss G (total):  tensor(35.6678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 97/1000\n",
      "Loss G (total):  tensor(33.3667, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 98/1000\n",
      "Loss G (total):  tensor(34.3402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 99/1000\n",
      "Loss G (total):  tensor(35.1036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 100/1000\n",
      "Loss G (total):  tensor(35.7277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 101/1000\n",
      "Loss G (total):  tensor(33.7806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 102/1000\n",
      "Loss G (total):  tensor(36.4899, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 103/1000\n",
      "Loss G (total):  tensor(35.8710, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 104/1000\n",
      "Loss G (total):  tensor(36.2778, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 105/1000\n",
      "Loss G (total):  tensor(34.1412, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 106/1000\n",
      "Loss G (total):  tensor(36.8137, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 107/1000\n",
      "Loss G (total):  tensor(35.3664, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 108/1000\n",
      "Loss G (total):  tensor(35.7605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 109/1000\n",
      "Loss G (total):  tensor(34.9186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 110/1000\n",
      "Loss G (total):  tensor(35.9764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 111/1000\n",
      "Loss G (total):  tensor(38.2618, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 112/1000\n",
      "Loss G (total):  tensor(38.2314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 113/1000\n",
      "Loss G (total):  tensor(37.2246, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 114/1000\n",
      "Loss G (total):  tensor(35.8645, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 115/1000\n",
      "Loss G (total):  tensor(35.9636, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 116/1000\n",
      "Loss G (total):  tensor(37.3093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 117/1000\n",
      "Loss G (total):  tensor(37.7116, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 118/1000\n",
      "Loss G (total):  tensor(37.3559, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 119/1000\n",
      "Loss G (total):  tensor(40.2775, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 120/1000\n",
      "Loss G (total):  tensor(38.2597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 121/1000\n",
      "Loss G (total):  tensor(36.9405, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 122/1000\n",
      "Loss G (total):  tensor(38.2539, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 123/1000\n",
      "Loss G (total):  tensor(38.6850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 124/1000\n",
      "Loss G (total):  tensor(39.2931, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 125/1000\n",
      "Loss G (total):  tensor(38.1431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 126/1000\n",
      "Loss G (total):  tensor(39.4755, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 127/1000\n",
      "Loss G (total):  tensor(38.3951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 128/1000\n",
      "Loss G (total):  tensor(40.9457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 129/1000\n",
      "Loss G (total):  tensor(39.9207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 130/1000\n",
      "Loss G (total):  tensor(39.3005, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 131/1000\n",
      "Loss G (total):  tensor(39.1325, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 132/1000\n",
      "Loss G (total):  tensor(40.4902, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 133/1000\n",
      "Loss G (total):  tensor(39.7707, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 134/1000\n",
      "Loss G (total):  tensor(41.1810, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 135/1000\n",
      "Loss G (total):  tensor(40.2100, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 136/1000\n",
      "Loss G (total):  tensor(40.0423, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 137/1000\n",
      "Loss G (total):  tensor(40.9757, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 138/1000\n",
      "Loss G (total):  tensor(42.8215, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 139/1000\n",
      "Loss G (total):  tensor(43.4268, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 140/1000\n",
      "Loss G (total):  tensor(41.5122, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 141/1000\n",
      "Loss G (total):  tensor(39.8934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 142/1000\n",
      "Loss G (total):  tensor(41.6850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 143/1000\n",
      "Loss G (total):  tensor(43.8500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 144/1000\n",
      "Loss G (total):  tensor(42.3585, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 145/1000\n",
      "Loss G (total):  tensor(43.1227, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 146/1000\n",
      "Loss G (total):  tensor(43.6322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 147/1000\n",
      "Loss G (total):  tensor(44.1070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 148/1000\n",
      "Loss G (total):  tensor(43.6252, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 149/1000\n",
      "Loss G (total):  tensor(45.5582, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 150/1000\n",
      "Loss G (total):  tensor(45.1674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 151/1000\n",
      "Loss G (total):  tensor(46.1202, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 152/1000\n",
      "Loss G (total):  tensor(44.6813, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 153/1000\n",
      "Loss G (total):  tensor(45.0565, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 154/1000\n",
      "Loss G (total):  tensor(47.0555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 155/1000\n",
      "Loss G (total):  tensor(43.5448, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 156/1000\n",
      "Loss G (total):  tensor(44.7998, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 157/1000\n",
      "Loss G (total):  tensor(46.2645, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 158/1000\n",
      "Loss G (total):  tensor(45.0877, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 159/1000\n",
      "Loss G (total):  tensor(46.6804, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 160/1000\n",
      "Loss G (total):  tensor(47.1909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 161/1000\n",
      "Loss G (total):  tensor(47.2257, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 162/1000\n",
      "Loss G (total):  tensor(47.5160, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 163/1000\n",
      "Loss G (total):  tensor(47.6738, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 164/1000\n",
      "Loss G (total):  tensor(47.1957, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 165/1000\n",
      "Loss G (total):  tensor(46.9180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 166/1000\n",
      "Loss G (total):  tensor(47.2056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 167/1000\n",
      "Loss G (total):  tensor(47.9152, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 168/1000\n",
      "Loss G (total):  tensor(47.2255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 169/1000\n",
      "Loss G (total):  tensor(48.6232, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 170/1000\n",
      "Loss G (total):  tensor(49.2157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 171/1000\n",
      "Loss G (total):  tensor(50.4699, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 172/1000\n",
      "Loss G (total):  tensor(50.1756, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 173/1000\n",
      "Loss G (total):  tensor(49.9388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 174/1000\n",
      "Loss G (total):  tensor(49.7602, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 175/1000\n",
      "Loss G (total):  tensor(49.5955, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 176/1000\n",
      "Loss G (total):  tensor(50.8499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 177/1000\n",
      "Loss G (total):  tensor(51.0014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 178/1000\n",
      "Loss G (total):  tensor(51.2096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 179/1000\n",
      "Loss G (total):  tensor(50.6855, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 180/1000\n",
      "Loss G (total):  tensor(52.0766, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 181/1000\n",
      "Loss G (total):  tensor(51.1713, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 182/1000\n",
      "Loss G (total):  tensor(52.2924, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 183/1000\n",
      "Loss G (total):  tensor(52.1803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 184/1000\n",
      "Loss G (total):  tensor(51.6245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 185/1000\n",
      "Loss G (total):  tensor(52.8624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 186/1000\n",
      "Loss G (total):  tensor(51.7307, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 187/1000\n",
      "Loss G (total):  tensor(53.1076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 188/1000\n",
      "Loss G (total):  tensor(54.2774, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 189/1000\n",
      "Loss G (total):  tensor(54.3105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 190/1000\n",
      "Loss G (total):  tensor(54.8650, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 191/1000\n",
      "Loss G (total):  tensor(53.6552, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 192/1000\n",
      "Loss G (total):  tensor(53.9533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 193/1000\n",
      "Loss G (total):  tensor(54.4736, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 194/1000\n",
      "Loss G (total):  tensor(54.3717, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 195/1000\n",
      "Loss G (total):  tensor(54.1685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 196/1000\n",
      "Loss G (total):  tensor(54.6042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 197/1000\n",
      "Loss G (total):  tensor(57.9244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 198/1000\n",
      "Loss G (total):  tensor(57.1614, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 199/1000\n",
      "Loss G (total):  tensor(57.1012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 200/1000\n",
      "Loss G (total):  tensor(56.3067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 201/1000\n",
      "Loss G (total):  tensor(56.1833, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 202/1000\n",
      "Loss G (total):  tensor(54.6790, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 203/1000\n",
      "Loss G (total):  tensor(54.8075, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 204/1000\n",
      "Loss G (total):  tensor(57.4306, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 205/1000\n",
      "Loss G (total):  tensor(58.4073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 206/1000\n",
      "Loss G (total):  tensor(57.9528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 207/1000\n",
      "Loss G (total):  tensor(59.1876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 208/1000\n",
      "Loss G (total):  tensor(60.0413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 209/1000\n",
      "Loss G (total):  tensor(58.4656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 210/1000\n",
      "Loss G (total):  tensor(58.5226, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 211/1000\n",
      "Loss G (total):  tensor(60.8609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 212/1000\n",
      "Loss G (total):  tensor(60.8900, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 213/1000\n",
      "Loss G (total):  tensor(60.8542, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 214/1000\n",
      "Loss G (total):  tensor(60.5923, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 215/1000\n",
      "Loss G (total):  tensor(60.4521, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 216/1000\n",
      "Loss G (total):  tensor(62.3068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 217/1000\n",
      "Loss G (total):  tensor(60.8821, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 218/1000\n",
      "Loss G (total):  tensor(61.9630, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 219/1000\n",
      "Loss G (total):  tensor(62.0430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 220/1000\n",
      "Loss G (total):  tensor(61.9267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 221/1000\n",
      "Loss G (total):  tensor(63.4665, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 222/1000\n",
      "Loss G (total):  tensor(60.8250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 223/1000\n",
      "Loss G (total):  tensor(63.1185, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 224/1000\n",
      "Loss G (total):  tensor(63.5136, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 225/1000\n",
      "Loss G (total):  tensor(63.4345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 226/1000\n",
      "Loss G (total):  tensor(63.3504, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 227/1000\n",
      "Loss G (total):  tensor(63.6736, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 228/1000\n",
      "Loss G (total):  tensor(65.6748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 229/1000\n",
      "Loss G (total):  tensor(65.1133, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 230/1000\n",
      "Loss G (total):  tensor(67.5050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 231/1000\n",
      "Loss G (total):  tensor(65.8495, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 232/1000\n",
      "Loss G (total):  tensor(67.4827, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 233/1000\n",
      "Loss G (total):  tensor(67.4862, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 234/1000\n",
      "Loss G (total):  tensor(66.8456, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 235/1000\n",
      "Loss G (total):  tensor(66.5986, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 236/1000\n",
      "Loss G (total):  tensor(68.3063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 237/1000\n",
      "Loss G (total):  tensor(68.5436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 238/1000\n",
      "Loss G (total):  tensor(69.4228, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 239/1000\n",
      "Loss G (total):  tensor(68.2973, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 240/1000\n",
      "Loss G (total):  tensor(68.4038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 241/1000\n",
      "Loss G (total):  tensor(69.5673, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 242/1000\n",
      "Loss G (total):  tensor(69.2353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 243/1000\n",
      "Loss G (total):  tensor(69.4047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 244/1000\n",
      "Loss G (total):  tensor(70.5298, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 245/1000\n",
      "Loss G (total):  tensor(70.6890, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 246/1000\n",
      "Loss G (total):  tensor(69.7525, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 247/1000\n",
      "Loss G (total):  tensor(72.5118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 248/1000\n",
      "Loss G (total):  tensor(70.2255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 249/1000\n",
      "Loss G (total):  tensor(73.4927, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 250/1000\n",
      "Loss G (total):  tensor(70.4882, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 251/1000\n",
      "Loss G (total):  tensor(70.1897, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 252/1000\n",
      "Loss G (total):  tensor(72.9391, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 253/1000\n",
      "Loss G (total):  tensor(73.0481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 254/1000\n",
      "Loss G (total):  tensor(73.8051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 255/1000\n",
      "Loss G (total):  tensor(73.0628, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 256/1000\n",
      "Loss G (total):  tensor(73.7259, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 257/1000\n",
      "Loss G (total):  tensor(74.0181, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 258/1000\n",
      "Loss G (total):  tensor(75.8753, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 259/1000\n",
      "Loss G (total):  tensor(75.3142, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 260/1000\n",
      "Loss G (total):  tensor(75.4251, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 261/1000\n",
      "Loss G (total):  tensor(75.3339, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 262/1000\n",
      "Loss G (total):  tensor(76.8332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 263/1000\n",
      "Loss G (total):  tensor(76.9145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 264/1000\n",
      "Loss G (total):  tensor(76.1587, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 265/1000\n",
      "Loss G (total):  tensor(75.5682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 266/1000\n",
      "Loss G (total):  tensor(78.0412, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 267/1000\n",
      "Loss G (total):  tensor(76.6844, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 268/1000\n",
      "Loss G (total):  tensor(79.3617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 269/1000\n",
      "Loss G (total):  tensor(78.2188, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 270/1000\n",
      "Loss G (total):  tensor(79.2324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 271/1000\n",
      "Loss G (total):  tensor(79.6835, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 272/1000\n",
      "Loss G (total):  tensor(79.5771, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 273/1000\n",
      "Loss G (total):  tensor(79.3932, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 274/1000\n",
      "Loss G (total):  tensor(80.3724, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 275/1000\n",
      "Loss G (total):  tensor(81.1110, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 276/1000\n",
      "Loss G (total):  tensor(81.2000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 277/1000\n",
      "Loss G (total):  tensor(80.8630, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 278/1000\n",
      "Loss G (total):  tensor(82.3121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 279/1000\n",
      "Loss G (total):  tensor(81.9829, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 280/1000\n",
      "Loss G (total):  tensor(82.3520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 281/1000\n",
      "Loss G (total):  tensor(84.1580, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 282/1000\n",
      "Loss G (total):  tensor(84.1737, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 283/1000\n",
      "Loss G (total):  tensor(84.7517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 284/1000\n",
      "Loss G (total):  tensor(83.3637, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 285/1000\n",
      "Loss G (total):  tensor(82.4047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 286/1000\n",
      "Loss G (total):  tensor(84.7101, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 287/1000\n",
      "Loss G (total):  tensor(85.2107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 288/1000\n",
      "Loss G (total):  tensor(85.9688, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 289/1000\n",
      "Loss G (total):  tensor(86.2540, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 290/1000\n",
      "Loss G (total):  tensor(85.0463, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 291/1000\n",
      "Loss G (total):  tensor(88.7934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 292/1000\n",
      "Loss G (total):  tensor(87.8569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 293/1000\n",
      "Loss G (total):  tensor(86.8952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 294/1000\n",
      "Loss G (total):  tensor(85.1131, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 295/1000\n",
      "Loss G (total):  tensor(88.4626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 296/1000\n",
      "Loss G (total):  tensor(89.6537, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 297/1000\n",
      "Loss G (total):  tensor(88.4696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 298/1000\n",
      "Loss G (total):  tensor(88.3920, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 299/1000\n",
      "Loss G (total):  tensor(90.4062, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 300/1000\n",
      "Loss G (total):  tensor(89.7240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 301/1000\n",
      "Loss G (total):  tensor(90.3515, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 302/1000\n",
      "Loss G (total):  tensor(92.2591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 303/1000\n",
      "Loss G (total):  tensor(90.2817, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 304/1000\n",
      "Loss G (total):  tensor(90.3265, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 305/1000\n",
      "Loss G (total):  tensor(91.4352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 306/1000\n",
      "Loss G (total):  tensor(92.5843, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 307/1000\n",
      "Loss G (total):  tensor(93.6780, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 308/1000\n",
      "Loss G (total):  tensor(93.0535, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 309/1000\n",
      "Loss G (total):  tensor(93.9389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 310/1000\n",
      "Loss G (total):  tensor(96.3939, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 311/1000\n",
      "Loss G (total):  tensor(94.0655, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 312/1000\n",
      "Loss G (total):  tensor(94.3971, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 313/1000\n",
      "Loss G (total):  tensor(94.9257, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 314/1000\n",
      "Loss G (total):  tensor(96.3938, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 315/1000\n",
      "Loss G (total):  tensor(95.3163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 316/1000\n",
      "Loss G (total):  tensor(96.2431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 317/1000\n",
      "Loss G (total):  tensor(99.2482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 318/1000\n",
      "Loss G (total):  tensor(98.8128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 319/1000\n",
      "Loss G (total):  tensor(97.6820, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 320/1000\n",
      "Loss G (total):  tensor(98.1393, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 321/1000\n",
      "Loss G (total):  tensor(98.7727, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 322/1000\n",
      "Loss G (total):  tensor(98.0648, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 323/1000\n",
      "Loss G (total):  tensor(99.7811, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 324/1000\n",
      "Loss G (total):  tensor(100.4249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 325/1000\n",
      "Loss G (total):  tensor(100.1426, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 326/1000\n",
      "Loss G (total):  tensor(100.9356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 327/1000\n",
      "Loss G (total):  tensor(101.0718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 328/1000\n",
      "Loss G (total):  tensor(101.0398, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 329/1000\n",
      "Loss G (total):  tensor(101.8473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 330/1000\n",
      "Loss G (total):  tensor(101.2318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 331/1000\n",
      "Loss G (total):  tensor(103.3394, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 332/1000\n",
      "Loss G (total):  tensor(101.9544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 333/1000\n",
      "Loss G (total):  tensor(104.6475, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 334/1000\n",
      "Loss G (total):  tensor(104.2013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 335/1000\n",
      "Loss G (total):  tensor(105.2979, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 336/1000\n",
      "Loss G (total):  tensor(103.0250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 337/1000\n",
      "Loss G (total):  tensor(105.0434, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 338/1000\n",
      "Loss G (total):  tensor(105.5554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 339/1000\n",
      "Loss G (total):  tensor(106.1326, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 340/1000\n",
      "Loss G (total):  tensor(108.9006, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 341/1000\n",
      "Loss G (total):  tensor(106.1739, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 342/1000\n",
      "Loss G (total):  tensor(106.9873, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 343/1000\n",
      "Loss G (total):  tensor(107.8447, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 344/1000\n",
      "Loss G (total):  tensor(107.3950, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 345/1000\n",
      "Loss G (total):  tensor(107.1425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 346/1000\n",
      "Loss G (total):  tensor(108.9941, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 347/1000\n",
      "Loss G (total):  tensor(108.5649, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 348/1000\n",
      "Loss G (total):  tensor(109.2505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 349/1000\n",
      "Loss G (total):  tensor(109.8193, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 350/1000\n",
      "Loss G (total):  tensor(109.9463, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 351/1000\n",
      "Loss G (total):  tensor(111.4519, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 352/1000\n",
      "Loss G (total):  tensor(111.6245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 353/1000\n",
      "Loss G (total):  tensor(112.4959, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 354/1000\n",
      "Loss G (total):  tensor(112.9086, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 355/1000\n",
      "Loss G (total):  tensor(113.5339, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 356/1000\n",
      "Loss G (total):  tensor(112.7440, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 357/1000\n",
      "Loss G (total):  tensor(115.3608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 358/1000\n",
      "Loss G (total):  tensor(114.9582, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 359/1000\n",
      "Loss G (total):  tensor(114.3698, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 360/1000\n",
      "Loss G (total):  tensor(114.6607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 361/1000\n",
      "Loss G (total):  tensor(114.9063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 362/1000\n",
      "Loss G (total):  tensor(115.6248, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 363/1000\n",
      "Loss G (total):  tensor(115.8028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 364/1000\n",
      "Loss G (total):  tensor(116.4366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 365/1000\n",
      "Loss G (total):  tensor(116.9933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 366/1000\n",
      "Loss G (total):  tensor(117.0579, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 367/1000\n",
      "Loss G (total):  tensor(118.8141, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 368/1000\n",
      "Loss G (total):  tensor(118.4457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 369/1000\n",
      "Loss G (total):  tensor(118.1201, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 370/1000\n",
      "Loss G (total):  tensor(120.3854, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 371/1000\n",
      "Loss G (total):  tensor(119.9374, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 372/1000\n",
      "Loss G (total):  tensor(119.3060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 373/1000\n",
      "Loss G (total):  tensor(121.7287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 374/1000\n",
      "Loss G (total):  tensor(121.0968, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 375/1000\n",
      "Loss G (total):  tensor(122.6441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 376/1000\n",
      "Loss G (total):  tensor(123.2045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 377/1000\n",
      "Loss G (total):  tensor(123.8849, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 378/1000\n",
      "Loss G (total):  tensor(123.2988, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 379/1000\n",
      "Loss G (total):  tensor(122.7513, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 380/1000\n",
      "Loss G (total):  tensor(124.3108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 381/1000\n",
      "Loss G (total):  tensor(124.0503, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 382/1000\n",
      "Loss G (total):  tensor(125.2041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 383/1000\n",
      "Loss G (total):  tensor(126.1479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 384/1000\n",
      "Loss G (total):  tensor(126.5333, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 385/1000\n",
      "Loss G (total):  tensor(127.0746, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 386/1000\n",
      "Loss G (total):  tensor(127.3271, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 387/1000\n",
      "Loss G (total):  tensor(126.4324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 388/1000\n",
      "Loss G (total):  tensor(128.0714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 389/1000\n",
      "Loss G (total):  tensor(129.7677, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 390/1000\n",
      "Loss G (total):  tensor(128.5108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 391/1000\n",
      "Loss G (total):  tensor(129.2656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 392/1000\n",
      "Loss G (total):  tensor(130.6062, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 393/1000\n",
      "Loss G (total):  tensor(129.7398, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 394/1000\n",
      "Loss G (total):  tensor(131.6105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 395/1000\n",
      "Loss G (total):  tensor(131.3587, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 396/1000\n",
      "Loss G (total):  tensor(132.7981, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 397/1000\n",
      "Loss G (total):  tensor(133.2564, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 398/1000\n",
      "Loss G (total):  tensor(133.1678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 399/1000\n",
      "Loss G (total):  tensor(133.1481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 400/1000\n",
      "Loss G (total):  tensor(134.1796, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 401/1000\n",
      "Loss G (total):  tensor(131.9646, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 402/1000\n",
      "Loss G (total):  tensor(133.8617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 403/1000\n",
      "Loss G (total):  tensor(134.4588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 404/1000\n",
      "Loss G (total):  tensor(136.2985, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 405/1000\n",
      "Loss G (total):  tensor(136.1636, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 406/1000\n",
      "Loss G (total):  tensor(136.4559, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 407/1000\n",
      "Loss G (total):  tensor(136.7878, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 408/1000\n",
      "Loss G (total):  tensor(137.0733, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 409/1000\n",
      "Loss G (total):  tensor(137.8727, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 410/1000\n",
      "Loss G (total):  tensor(138.8122, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 411/1000\n",
      "Loss G (total):  tensor(137.7429, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 412/1000\n",
      "Loss G (total):  tensor(138.8765, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 413/1000\n",
      "Loss G (total):  tensor(140.8634, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 414/1000\n",
      "Loss G (total):  tensor(141.0275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 415/1000\n",
      "Loss G (total):  tensor(140.4581, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 416/1000\n",
      "Loss G (total):  tensor(141.1731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 417/1000\n",
      "Loss G (total):  tensor(142.1043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 418/1000\n",
      "Loss G (total):  tensor(142.9416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 419/1000\n",
      "Loss G (total):  tensor(145.6431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 420/1000\n",
      "Loss G (total):  tensor(145.8656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 421/1000\n",
      "Loss G (total):  tensor(144.6918, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 422/1000\n",
      "Loss G (total):  tensor(145.1792, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 423/1000\n",
      "Loss G (total):  tensor(145.2076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 424/1000\n",
      "Loss G (total):  tensor(145.5913, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 425/1000\n",
      "Loss G (total):  tensor(146.1808, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 426/1000\n",
      "Loss G (total):  tensor(146.9902, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 427/1000\n",
      "Loss G (total):  tensor(147.7515, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 428/1000\n",
      "Loss G (total):  tensor(147.9750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 429/1000\n",
      "Loss G (total):  tensor(148.9399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 430/1000\n",
      "Loss G (total):  tensor(149.3135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 431/1000\n",
      "Loss G (total):  tensor(149.3412, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 432/1000\n",
      "Loss G (total):  tensor(151.6544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 433/1000\n",
      "Loss G (total):  tensor(150.2899, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 434/1000\n",
      "Loss G (total):  tensor(150.3518, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 435/1000\n",
      "Loss G (total):  tensor(151.2161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 436/1000\n",
      "Loss G (total):  tensor(151.8693, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 437/1000\n",
      "Loss G (total):  tensor(152.9073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 438/1000\n",
      "Loss G (total):  tensor(153.6080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 439/1000\n",
      "Loss G (total):  tensor(154.0269, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 440/1000\n",
      "Loss G (total):  tensor(154.8988, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 441/1000\n",
      "Loss G (total):  tensor(154.4934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 442/1000\n",
      "Loss G (total):  tensor(154.5004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 443/1000\n",
      "Loss G (total):  tensor(155.7948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 444/1000\n",
      "Loss G (total):  tensor(155.9472, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 445/1000\n",
      "Loss G (total):  tensor(156.8132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 446/1000\n",
      "Loss G (total):  tensor(158.1396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 447/1000\n",
      "Loss G (total):  tensor(157.1718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 448/1000\n",
      "Loss G (total):  tensor(158.4989, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 449/1000\n",
      "Loss G (total):  tensor(159.6878, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 450/1000\n",
      "Loss G (total):  tensor(158.9034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 451/1000\n",
      "Loss G (total):  tensor(162.1056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 452/1000\n",
      "Loss G (total):  tensor(160.9858, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 453/1000\n",
      "Loss G (total):  tensor(162.0040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 454/1000\n",
      "Loss G (total):  tensor(161.9421, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 455/1000\n",
      "Loss G (total):  tensor(159.8481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 456/1000\n",
      "Loss G (total):  tensor(162.7570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 457/1000\n",
      "Loss G (total):  tensor(162.8032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 458/1000\n",
      "Loss G (total):  tensor(163.8245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 459/1000\n",
      "Loss G (total):  tensor(163.6045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 460/1000\n",
      "Loss G (total):  tensor(164.2860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 461/1000\n",
      "Loss G (total):  tensor(169.5651, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 462/1000\n",
      "Loss G (total):  tensor(167.2581, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 463/1000\n",
      "Loss G (total):  tensor(168.0902, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 464/1000\n",
      "Loss G (total):  tensor(167.2553, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 465/1000\n",
      "Loss G (total):  tensor(167.0606, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 466/1000\n",
      "Loss G (total):  tensor(168.7235, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 467/1000\n",
      "Loss G (total):  tensor(169.5815, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 468/1000\n",
      "Loss G (total):  tensor(171.8336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 469/1000\n",
      "Loss G (total):  tensor(171.3879, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 470/1000\n",
      "Loss G (total):  tensor(171.5045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 471/1000\n",
      "Loss G (total):  tensor(169.5295, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 472/1000\n",
      "Loss G (total):  tensor(172.3716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 473/1000\n",
      "Loss G (total):  tensor(173.2122, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 474/1000\n",
      "Loss G (total):  tensor(171.1631, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 475/1000\n",
      "Loss G (total):  tensor(171.7146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 476/1000\n",
      "Loss G (total):  tensor(173.5120, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 477/1000\n",
      "Loss G (total):  tensor(174.2956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 478/1000\n",
      "Loss G (total):  tensor(175.1307, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 479/1000\n",
      "Loss G (total):  tensor(174.4096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 480/1000\n",
      "Loss G (total):  tensor(176.1862, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 481/1000\n",
      "Loss G (total):  tensor(176.7045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 482/1000\n",
      "Loss G (total):  tensor(179.6994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 483/1000\n",
      "Loss G (total):  tensor(176.6106, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 484/1000\n",
      "Loss G (total):  tensor(178.4425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 485/1000\n",
      "Loss G (total):  tensor(179.4633, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 486/1000\n",
      "Loss G (total):  tensor(179.0855, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 487/1000\n",
      "Loss G (total):  tensor(180.2919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 488/1000\n",
      "Loss G (total):  tensor(180.7910, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 489/1000\n",
      "Loss G (total):  tensor(180.2595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 490/1000\n",
      "Loss G (total):  tensor(181.9351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 491/1000\n",
      "Loss G (total):  tensor(182.5099, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 492/1000\n",
      "Loss G (total):  tensor(183.0531, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 493/1000\n",
      "Loss G (total):  tensor(186.1569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 494/1000\n",
      "Loss G (total):  tensor(184.0270, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 495/1000\n",
      "Loss G (total):  tensor(184.0266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 496/1000\n",
      "Loss G (total):  tensor(186.3628, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 497/1000\n",
      "Loss G (total):  tensor(186.1660, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 498/1000\n",
      "Loss G (total):  tensor(187.1854, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 499/1000\n",
      "Loss G (total):  tensor(186.4205, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 500/1000\n",
      "Loss G (total):  tensor(189.0838, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 501/1000\n",
      "Loss G (total):  tensor(189.3126, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 502/1000\n",
      "Loss G (total):  tensor(189.2346, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 503/1000\n",
      "Loss G (total):  tensor(190.8320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 504/1000\n",
      "Loss G (total):  tensor(189.7768, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 505/1000\n",
      "Loss G (total):  tensor(189.9365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 506/1000\n",
      "Loss G (total):  tensor(192.1846, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 507/1000\n",
      "Loss G (total):  tensor(192.6199, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 508/1000\n",
      "Loss G (total):  tensor(193.9080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 509/1000\n",
      "Loss G (total):  tensor(192.5166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 510/1000\n",
      "Loss G (total):  tensor(193.7208, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 511/1000\n",
      "Loss G (total):  tensor(195.4962, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 512/1000\n",
      "Loss G (total):  tensor(194.5854, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 513/1000\n",
      "Loss G (total):  tensor(195.4720, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 514/1000\n",
      "Loss G (total):  tensor(195.3632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 515/1000\n",
      "Loss G (total):  tensor(196.9955, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 516/1000\n",
      "Loss G (total):  tensor(197.3887, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 517/1000\n",
      "Loss G (total):  tensor(196.5248, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 518/1000\n",
      "Loss G (total):  tensor(198.5437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 519/1000\n",
      "Loss G (total):  tensor(199.0410, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 520/1000\n",
      "Loss G (total):  tensor(200.1545, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 521/1000\n",
      "Loss G (total):  tensor(201.4149, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 522/1000\n",
      "Loss G (total):  tensor(202.2593, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 523/1000\n",
      "Loss G (total):  tensor(201.9772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 524/1000\n",
      "Loss G (total):  tensor(201.9437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 525/1000\n",
      "Loss G (total):  tensor(203.1080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 526/1000\n",
      "Loss G (total):  tensor(202.8335, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 527/1000\n",
      "Loss G (total):  tensor(203.4319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 528/1000\n",
      "Loss G (total):  tensor(204.2156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 529/1000\n",
      "Loss G (total):  tensor(202.6278, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 530/1000\n",
      "Loss G (total):  tensor(206.0592, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 531/1000\n",
      "Loss G (total):  tensor(207.2843, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 532/1000\n",
      "Loss G (total):  tensor(207.1330, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 533/1000\n",
      "Loss G (total):  tensor(207.4857, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 534/1000\n",
      "Loss G (total):  tensor(208.0710, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 535/1000\n",
      "Loss G (total):  tensor(209.1026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 536/1000\n",
      "Loss G (total):  tensor(209.4493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 537/1000\n",
      "Loss G (total):  tensor(211.1864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 538/1000\n",
      "Loss G (total):  tensor(211.1178, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 539/1000\n",
      "Loss G (total):  tensor(210.1299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 540/1000\n",
      "Loss G (total):  tensor(211.2790, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 541/1000\n",
      "Loss G (total):  tensor(212.5433, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 542/1000\n",
      "Loss G (total):  tensor(213.7199, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 543/1000\n",
      "Loss G (total):  tensor(214.2445, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 544/1000\n",
      "Loss G (total):  tensor(214.4717, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 545/1000\n",
      "Loss G (total):  tensor(215.1681, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 546/1000\n",
      "Loss G (total):  tensor(216.3205, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 547/1000\n",
      "Loss G (total):  tensor(215.6396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 548/1000\n",
      "Loss G (total):  tensor(216.4349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 549/1000\n",
      "Loss G (total):  tensor(216.8683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 550/1000\n",
      "Loss G (total):  tensor(218.7839, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 551/1000\n",
      "Loss G (total):  tensor(219.2222, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 552/1000\n",
      "Loss G (total):  tensor(219.8142, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 553/1000\n",
      "Loss G (total):  tensor(219.5816, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 554/1000\n",
      "Loss G (total):  tensor(219.9709, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 555/1000\n",
      "Loss G (total):  tensor(221.3328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 556/1000\n",
      "Loss G (total):  tensor(221.2314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 557/1000\n",
      "Loss G (total):  tensor(223.4091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 558/1000\n",
      "Loss G (total):  tensor(224.0432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 559/1000\n",
      "Loss G (total):  tensor(222.3649, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 560/1000\n",
      "Loss G (total):  tensor(225.1911, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 561/1000\n",
      "Loss G (total):  tensor(226.9990, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 562/1000\n",
      "Loss G (total):  tensor(225.2396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 563/1000\n",
      "Loss G (total):  tensor(228.1869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 564/1000\n",
      "Loss G (total):  tensor(227.8685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 565/1000\n",
      "Loss G (total):  tensor(226.3524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 566/1000\n",
      "Loss G (total):  tensor(228.0469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 567/1000\n",
      "Loss G (total):  tensor(229.7623, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 568/1000\n",
      "Loss G (total):  tensor(229.1478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 569/1000\n",
      "Loss G (total):  tensor(231.7385, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 570/1000\n",
      "Loss G (total):  tensor(230.9477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 571/1000\n",
      "Loss G (total):  tensor(231.0859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 572/1000\n",
      "Loss G (total):  tensor(233.1843, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 573/1000\n",
      "Loss G (total):  tensor(234.6193, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 574/1000\n",
      "Loss G (total):  tensor(232.9830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 575/1000\n",
      "Loss G (total):  tensor(233.8200, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 576/1000\n",
      "Loss G (total):  tensor(236.2019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 577/1000\n",
      "Loss G (total):  tensor(235.7481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 578/1000\n",
      "Loss G (total):  tensor(235.5747, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 579/1000\n",
      "Loss G (total):  tensor(238.4708, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 580/1000\n",
      "Loss G (total):  tensor(237.4942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 581/1000\n",
      "Loss G (total):  tensor(239.0970, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 582/1000\n",
      "Loss G (total):  tensor(238.5042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 583/1000\n",
      "Loss G (total):  tensor(239.7043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 584/1000\n",
      "Loss G (total):  tensor(239.6948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 585/1000\n",
      "Loss G (total):  tensor(241.4065, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 586/1000\n",
      "Loss G (total):  tensor(241.9511, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 587/1000\n",
      "Loss G (total):  tensor(241.6236, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 588/1000\n",
      "Loss G (total):  tensor(242.1991, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 589/1000\n",
      "Loss G (total):  tensor(241.5851, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 590/1000\n",
      "Loss G (total):  tensor(243.6601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 591/1000\n",
      "Loss G (total):  tensor(243.9731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 592/1000\n",
      "Loss G (total):  tensor(246.4093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 593/1000\n",
      "Loss G (total):  tensor(246.1683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 594/1000\n",
      "Loss G (total):  tensor(247.8588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 595/1000\n",
      "Loss G (total):  tensor(248.4388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 596/1000\n",
      "Loss G (total):  tensor(247.2804, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 597/1000\n",
      "Loss G (total):  tensor(248.1770, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 598/1000\n",
      "Loss G (total):  tensor(249.9969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 599/1000\n",
      "Loss G (total):  tensor(250.1941, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 600/1000\n",
      "Loss G (total):  tensor(252.3510, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 601/1000\n",
      "Loss G (total):  tensor(250.2137, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 602/1000\n",
      "Loss G (total):  tensor(253.6071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 603/1000\n",
      "Loss G (total):  tensor(252.7302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 604/1000\n",
      "Loss G (total):  tensor(253.5484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 605/1000\n",
      "Loss G (total):  tensor(253.9933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 606/1000\n",
      "Loss G (total):  tensor(254.7360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 607/1000\n",
      "Loss G (total):  tensor(256.5136, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 608/1000\n",
      "Loss G (total):  tensor(256.6750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 609/1000\n",
      "Loss G (total):  tensor(256.2443, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 610/1000\n",
      "Loss G (total):  tensor(256.8922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 611/1000\n",
      "Loss G (total):  tensor(257.1631, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 612/1000\n",
      "Loss G (total):  tensor(259.7025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 613/1000\n",
      "Loss G (total):  tensor(259.6773, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 614/1000\n",
      "Loss G (total):  tensor(259.6945, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 615/1000\n",
      "Loss G (total):  tensor(260.1044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 616/1000\n",
      "Loss G (total):  tensor(260.2946, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 617/1000\n",
      "Loss G (total):  tensor(263.8064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 618/1000\n",
      "Loss G (total):  tensor(263.7975, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 619/1000\n",
      "Loss G (total):  tensor(262.9424, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 620/1000\n",
      "Loss G (total):  tensor(264.5878, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 621/1000\n",
      "Loss G (total):  tensor(264.3320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 622/1000\n",
      "Loss G (total):  tensor(264.3819, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 623/1000\n",
      "Loss G (total):  tensor(267.4105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 624/1000\n",
      "Loss G (total):  tensor(266.0407, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 625/1000\n",
      "Loss G (total):  tensor(268.5110, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 626/1000\n",
      "Loss G (total):  tensor(268.4930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 627/1000\n",
      "Loss G (total):  tensor(269.0650, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 628/1000\n",
      "Loss G (total):  tensor(268.8380, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 629/1000\n",
      "Loss G (total):  tensor(271.0121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 630/1000\n",
      "Loss G (total):  tensor(271.8444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 631/1000\n",
      "Loss G (total):  tensor(271.5377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 632/1000\n",
      "Loss G (total):  tensor(272.8113, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 633/1000\n",
      "Loss G (total):  tensor(272.8102, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 634/1000\n",
      "Loss G (total):  tensor(275.2167, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 635/1000\n",
      "Loss G (total):  tensor(273.4511, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 636/1000\n",
      "Loss G (total):  tensor(277.4455, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 637/1000\n",
      "Loss G (total):  tensor(276.8375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 638/1000\n",
      "Loss G (total):  tensor(276.4132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 639/1000\n",
      "Loss G (total):  tensor(278.8143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 640/1000\n",
      "Loss G (total):  tensor(277.0714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 641/1000\n",
      "Loss G (total):  tensor(279.0762, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 642/1000\n",
      "Loss G (total):  tensor(279.9376, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 643/1000\n",
      "Loss G (total):  tensor(280.8087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 644/1000\n",
      "Loss G (total):  tensor(280.8453, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 645/1000\n",
      "Loss G (total):  tensor(282.1123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 646/1000\n",
      "Loss G (total):  tensor(281.2530, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 647/1000\n",
      "Loss G (total):  tensor(283.8386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 648/1000\n",
      "Loss G (total):  tensor(284.8489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 649/1000\n",
      "Loss G (total):  tensor(284.4794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 650/1000\n",
      "Loss G (total):  tensor(285.9789, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 651/1000\n",
      "Loss G (total):  tensor(285.7034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 652/1000\n",
      "Loss G (total):  tensor(287.7493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 653/1000\n",
      "Loss G (total):  tensor(287.5847, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 654/1000\n",
      "Loss G (total):  tensor(288.2238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 655/1000\n",
      "Loss G (total):  tensor(289.0462, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 656/1000\n",
      "Loss G (total):  tensor(289.9869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 657/1000\n",
      "Loss G (total):  tensor(289.8058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 658/1000\n",
      "Loss G (total):  tensor(292.1389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 659/1000\n",
      "Loss G (total):  tensor(292.5558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 660/1000\n",
      "Loss G (total):  tensor(292.6296, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 661/1000\n",
      "Loss G (total):  tensor(292.2276, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 662/1000\n",
      "Loss G (total):  tensor(295.4712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 663/1000\n",
      "Loss G (total):  tensor(294.5442, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 664/1000\n",
      "Loss G (total):  tensor(295.2130, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 665/1000\n",
      "Loss G (total):  tensor(295.2089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 666/1000\n",
      "Loss G (total):  tensor(296.1371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 667/1000\n",
      "Loss G (total):  tensor(297.4607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 668/1000\n",
      "Loss G (total):  tensor(297.4215, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 669/1000\n",
      "Loss G (total):  tensor(299.3166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 670/1000\n",
      "Loss G (total):  tensor(301.3233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 671/1000\n",
      "Loss G (total):  tensor(300.0838, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 672/1000\n",
      "Loss G (total):  tensor(300.7768, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 673/1000\n",
      "Loss G (total):  tensor(302.7210, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 674/1000\n",
      "Loss G (total):  tensor(302.4788, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 675/1000\n",
      "Loss G (total):  tensor(303.6349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 676/1000\n",
      "Loss G (total):  tensor(304.6667, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 677/1000\n",
      "Loss G (total):  tensor(306.1347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 678/1000\n",
      "Loss G (total):  tensor(305.1999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 679/1000\n",
      "Loss G (total):  tensor(306.5763, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 680/1000\n",
      "Loss G (total):  tensor(307.3132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 681/1000\n",
      "Loss G (total):  tensor(308.5171, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 682/1000\n",
      "Loss G (total):  tensor(308.0371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 683/1000\n",
      "Loss G (total):  tensor(309.3698, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 684/1000\n",
      "Loss G (total):  tensor(310.4570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 685/1000\n",
      "Loss G (total):  tensor(309.9951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 686/1000\n",
      "Loss G (total):  tensor(312.1736, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 687/1000\n",
      "Loss G (total):  tensor(312.8666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 688/1000\n",
      "Loss G (total):  tensor(313.6903, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 689/1000\n",
      "Loss G (total):  tensor(313.3152, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 690/1000\n",
      "Loss G (total):  tensor(313.2635, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 691/1000\n",
      "Loss G (total):  tensor(315.4343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 692/1000\n",
      "Loss G (total):  tensor(314.9888, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 693/1000\n",
      "Loss G (total):  tensor(316.8693, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 694/1000\n",
      "Loss G (total):  tensor(317.1436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 695/1000\n",
      "Loss G (total):  tensor(318.7885, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 696/1000\n",
      "Loss G (total):  tensor(319.4888, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 697/1000\n",
      "Loss G (total):  tensor(319.6599, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 698/1000\n",
      "Loss G (total):  tensor(320.4659, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 699/1000\n",
      "Loss G (total):  tensor(321.6599, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 700/1000\n",
      "Loss G (total):  tensor(322.5316, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 701/1000\n",
      "Loss G (total):  tensor(323.4619, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 702/1000\n",
      "Loss G (total):  tensor(323.5474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 703/1000\n",
      "Loss G (total):  tensor(324.2696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 704/1000\n",
      "Loss G (total):  tensor(326.1163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 705/1000\n",
      "Loss G (total):  tensor(325.4408, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 706/1000\n",
      "Loss G (total):  tensor(325.7139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 707/1000\n",
      "Loss G (total):  tensor(328.2209, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 708/1000\n",
      "Loss G (total):  tensor(328.0619, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 709/1000\n",
      "Loss G (total):  tensor(327.8672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 710/1000\n",
      "Loss G (total):  tensor(330.9609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 711/1000\n",
      "Loss G (total):  tensor(330.0505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 712/1000\n",
      "Loss G (total):  tensor(331.7855, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 713/1000\n",
      "Loss G (total):  tensor(333.1509, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 714/1000\n",
      "Loss G (total):  tensor(332.5690, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 715/1000\n",
      "Loss G (total):  tensor(334.1269, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 716/1000\n",
      "Loss G (total):  tensor(335.0864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 717/1000\n",
      "Loss G (total):  tensor(334.9259, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 718/1000\n",
      "Loss G (total):  tensor(334.9777, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 719/1000\n",
      "Loss G (total):  tensor(335.9145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 720/1000\n",
      "Loss G (total):  tensor(338.7632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 721/1000\n",
      "Loss G (total):  tensor(338.0473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 722/1000\n",
      "Loss G (total):  tensor(337.6129, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 723/1000\n",
      "Loss G (total):  tensor(339.3362, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 724/1000\n",
      "Loss G (total):  tensor(340.5253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 725/1000\n",
      "Loss G (total):  tensor(340.8303, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 726/1000\n",
      "Loss G (total):  tensor(341.7323, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 727/1000\n",
      "Loss G (total):  tensor(343.3224, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 728/1000\n",
      "Loss G (total):  tensor(343.1704, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 729/1000\n",
      "Loss G (total):  tensor(345.7030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 730/1000\n",
      "Loss G (total):  tensor(346.2837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 731/1000\n",
      "Loss G (total):  tensor(345.5756, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 732/1000\n",
      "Loss G (total):  tensor(345.0706, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 733/1000\n",
      "Loss G (total):  tensor(344.9370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 734/1000\n",
      "Loss G (total):  tensor(347.7876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 735/1000\n",
      "Loss G (total):  tensor(349.6256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 736/1000\n",
      "Loss G (total):  tensor(349.8502, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 737/1000\n",
      "Loss G (total):  tensor(350.4880, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 738/1000\n",
      "Loss G (total):  tensor(351.6462, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 739/1000\n",
      "Loss G (total):  tensor(352.5153, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 740/1000\n",
      "Loss G (total):  tensor(352.9852, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 741/1000\n",
      "Loss G (total):  tensor(353.6549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 742/1000\n",
      "Loss G (total):  tensor(354.2744, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 743/1000\n",
      "Loss G (total):  tensor(354.9379, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 744/1000\n",
      "Loss G (total):  tensor(355.5580, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 745/1000\n",
      "Loss G (total):  tensor(355.8079, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 746/1000\n",
      "Loss G (total):  tensor(357.1674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 747/1000\n",
      "Loss G (total):  tensor(357.9626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 748/1000\n",
      "Loss G (total):  tensor(360.8782, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 749/1000\n",
      "Loss G (total):  tensor(359.9164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 750/1000\n",
      "Loss G (total):  tensor(360.6549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 751/1000\n",
      "Loss G (total):  tensor(361.0081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 752/1000\n",
      "Loss G (total):  tensor(362.3803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 753/1000\n",
      "Loss G (total):  tensor(361.6895, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 754/1000\n",
      "Loss G (total):  tensor(363.7231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 755/1000\n",
      "Loss G (total):  tensor(365.5640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 756/1000\n",
      "Loss G (total):  tensor(365.9915, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 757/1000\n",
      "Loss G (total):  tensor(366.9544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 758/1000\n",
      "Loss G (total):  tensor(367.0408, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 759/1000\n",
      "Loss G (total):  tensor(367.8267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 760/1000\n",
      "Loss G (total):  tensor(369.1905, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 761/1000\n",
      "Loss G (total):  tensor(370.5409, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 762/1000\n",
      "Loss G (total):  tensor(370.7369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 763/1000\n",
      "Loss G (total):  tensor(372.0039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 764/1000\n",
      "Loss G (total):  tensor(371.5570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 765/1000\n",
      "Loss G (total):  tensor(372.0718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 766/1000\n",
      "Loss G (total):  tensor(372.8797, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 767/1000\n",
      "Loss G (total):  tensor(373.6227, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 768/1000\n",
      "Loss G (total):  tensor(376.6686, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 769/1000\n",
      "Loss G (total):  tensor(376.4233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 770/1000\n",
      "Loss G (total):  tensor(376.5328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 771/1000\n",
      "Loss G (total):  tensor(376.5638, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 772/1000\n",
      "Loss G (total):  tensor(377.8441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 773/1000\n",
      "Loss G (total):  tensor(378.4058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 774/1000\n",
      "Loss G (total):  tensor(379.3649, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 775/1000\n",
      "Loss G (total):  tensor(382.1280, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 776/1000\n",
      "Loss G (total):  tensor(383.4043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 777/1000\n",
      "Loss G (total):  tensor(382.6753, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 778/1000\n",
      "Loss G (total):  tensor(382.0429, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 779/1000\n",
      "Loss G (total):  tensor(383.2575, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 780/1000\n",
      "Loss G (total):  tensor(386.8947, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 781/1000\n",
      "Loss G (total):  tensor(385.1353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 782/1000\n",
      "Loss G (total):  tensor(387.1624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 783/1000\n",
      "Loss G (total):  tensor(387.4859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 784/1000\n",
      "Loss G (total):  tensor(388.6497, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 785/1000\n",
      "Loss G (total):  tensor(389.4051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 786/1000\n",
      "Loss G (total):  tensor(390.8760, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 787/1000\n",
      "Loss G (total):  tensor(389.9125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 788/1000\n",
      "Loss G (total):  tensor(391.7177, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 789/1000\n",
      "Loss G (total):  tensor(391.0268, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 790/1000\n",
      "Loss G (total):  tensor(393.7393, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 791/1000\n",
      "Loss G (total):  tensor(393.2147, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 792/1000\n",
      "Loss G (total):  tensor(393.9313, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 793/1000\n",
      "Loss G (total):  tensor(396.3424, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 794/1000\n",
      "Loss G (total):  tensor(396.8632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 795/1000\n",
      "Loss G (total):  tensor(396.2474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 796/1000\n",
      "Loss G (total):  tensor(398.7292, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 797/1000\n",
      "Loss G (total):  tensor(397.8470, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 798/1000\n",
      "Loss G (total):  tensor(401.4430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 799/1000\n",
      "Loss G (total):  tensor(400.5222, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 800/1000\n",
      "Loss G (total):  tensor(399.9307, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 801/1000\n",
      "Loss G (total):  tensor(402.3284, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 802/1000\n",
      "Loss G (total):  tensor(402.5552, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 803/1000\n",
      "Loss G (total):  tensor(405.1541, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 804/1000\n",
      "Loss G (total):  tensor(404.5654, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 805/1000\n",
      "Loss G (total):  tensor(405.2970, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 806/1000\n",
      "Loss G (total):  tensor(406.0136, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 807/1000\n",
      "Loss G (total):  tensor(407.3043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 808/1000\n",
      "Loss G (total):  tensor(406.8806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 809/1000\n",
      "Loss G (total):  tensor(409.7921, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 810/1000\n",
      "Loss G (total):  tensor(409.7179, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 811/1000\n",
      "Loss G (total):  tensor(410.1643, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 812/1000\n",
      "Loss G (total):  tensor(411.6938, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 813/1000\n",
      "Loss G (total):  tensor(411.4177, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 814/1000\n",
      "Loss G (total):  tensor(412.2728, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 815/1000\n",
      "Loss G (total):  tensor(412.9308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 816/1000\n",
      "Loss G (total):  tensor(414.5047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 817/1000\n",
      "Loss G (total):  tensor(416.4706, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 818/1000\n",
      "Loss G (total):  tensor(416.2915, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 819/1000\n",
      "Loss G (total):  tensor(417.0408, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 820/1000\n",
      "Loss G (total):  tensor(418.2213, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 821/1000\n",
      "Loss G (total):  tensor(419.1790, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 822/1000\n",
      "Loss G (total):  tensor(419.2529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 823/1000\n",
      "Loss G (total):  tensor(420.1774, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 824/1000\n",
      "Loss G (total):  tensor(420.6068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 825/1000\n",
      "Loss G (total):  tensor(422.4853, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 826/1000\n",
      "Loss G (total):  tensor(424.3632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 827/1000\n",
      "Loss G (total):  tensor(425.1351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 828/1000\n",
      "Loss G (total):  tensor(424.7356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 829/1000\n",
      "Loss G (total):  tensor(425.8453, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 830/1000\n",
      "Loss G (total):  tensor(426.4377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 831/1000\n",
      "Loss G (total):  tensor(426.6664, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 832/1000\n",
      "Loss G (total):  tensor(427.5244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 833/1000\n",
      "Loss G (total):  tensor(428.4574, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 834/1000\n",
      "Loss G (total):  tensor(430.3538, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 835/1000\n",
      "Loss G (total):  tensor(431.2451, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 836/1000\n",
      "Loss G (total):  tensor(430.5020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 837/1000\n",
      "Loss G (total):  tensor(432.2313, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 838/1000\n",
      "Loss G (total):  tensor(433.4741, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 839/1000\n",
      "Loss G (total):  tensor(434.6655, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 840/1000\n",
      "Loss G (total):  tensor(434.8053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 841/1000\n",
      "Loss G (total):  tensor(435.8872, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 842/1000\n",
      "Loss G (total):  tensor(436.3077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 843/1000\n",
      "Loss G (total):  tensor(437.1817, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 844/1000\n",
      "Loss G (total):  tensor(437.2033, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 845/1000\n",
      "Loss G (total):  tensor(438.8976, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 846/1000\n",
      "Loss G (total):  tensor(440.1399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 847/1000\n",
      "Loss G (total):  tensor(441.1115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 848/1000\n",
      "Loss G (total):  tensor(443.0178, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 849/1000\n",
      "Loss G (total):  tensor(443.3744, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 850/1000\n",
      "Loss G (total):  tensor(445.4772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 851/1000\n",
      "Loss G (total):  tensor(444.8841, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 852/1000\n",
      "Loss G (total):  tensor(445.6999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 853/1000\n",
      "Loss G (total):  tensor(446.2180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 854/1000\n",
      "Loss G (total):  tensor(448.6528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 855/1000\n",
      "Loss G (total):  tensor(447.8258, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 856/1000\n",
      "Loss G (total):  tensor(447.7518, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 857/1000\n",
      "Loss G (total):  tensor(452.0055, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 858/1000\n",
      "Loss G (total):  tensor(451.3195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 859/1000\n",
      "Loss G (total):  tensor(452.0705, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 860/1000\n",
      "Loss G (total):  tensor(451.4959, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 861/1000\n",
      "Loss G (total):  tensor(454.3947, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 862/1000\n",
      "Loss G (total):  tensor(454.4776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 863/1000\n",
      "Loss G (total):  tensor(454.9764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 864/1000\n",
      "Loss G (total):  tensor(455.0550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 865/1000\n",
      "Loss G (total):  tensor(456.3859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 866/1000\n",
      "Loss G (total):  tensor(457.6352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 867/1000\n",
      "Loss G (total):  tensor(457.7165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 868/1000\n",
      "Loss G (total):  tensor(459.1870, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 869/1000\n",
      "Loss G (total):  tensor(461.7495, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 870/1000\n",
      "Loss G (total):  tensor(460.3736, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 871/1000\n",
      "Loss G (total):  tensor(461.9480, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 872/1000\n",
      "Loss G (total):  tensor(463.3903, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 873/1000\n",
      "Loss G (total):  tensor(465.5350, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 874/1000\n",
      "Loss G (total):  tensor(465.0253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 875/1000\n",
      "Loss G (total):  tensor(466.2436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 876/1000\n",
      "Loss G (total):  tensor(465.5912, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 877/1000\n",
      "Loss G (total):  tensor(466.8793, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 878/1000\n",
      "Loss G (total):  tensor(468.0013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 879/1000\n",
      "Loss G (total):  tensor(468.5623, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 880/1000\n",
      "Loss G (total):  tensor(470.8443, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 881/1000\n",
      "Loss G (total):  tensor(470.6994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 882/1000\n",
      "Loss G (total):  tensor(471.9153, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 883/1000\n",
      "Loss G (total):  tensor(472.0066, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 884/1000\n",
      "Loss G (total):  tensor(472.3901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 885/1000\n",
      "Loss G (total):  tensor(473.5065, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 886/1000\n",
      "Loss G (total):  tensor(475.4135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 887/1000\n",
      "Loss G (total):  tensor(476.4482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 888/1000\n",
      "Loss G (total):  tensor(476.4836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 889/1000\n",
      "Loss G (total):  tensor(478.4101, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 890/1000\n",
      "Loss G (total):  tensor(477.5846, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 891/1000\n",
      "Loss G (total):  tensor(478.9829, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 892/1000\n",
      "Loss G (total):  tensor(480.7138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 893/1000\n",
      "Loss G (total):  tensor(480.9828, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 894/1000\n",
      "Loss G (total):  tensor(483.6764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 895/1000\n",
      "Loss G (total):  tensor(484.3196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 896/1000\n",
      "Loss G (total):  tensor(483.1024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 897/1000\n",
      "Loss G (total):  tensor(485.3930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 898/1000\n",
      "Loss G (total):  tensor(485.7175, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 899/1000\n",
      "Loss G (total):  tensor(487.5089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 900/1000\n",
      "Loss G (total):  tensor(487.0579, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 901/1000\n",
      "Loss G (total):  tensor(490.2951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 902/1000\n",
      "Loss G (total):  tensor(489.8308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 903/1000\n",
      "Loss G (total):  tensor(491.5072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 904/1000\n",
      "Loss G (total):  tensor(490.0834, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 905/1000\n",
      "Loss G (total):  tensor(492.5140, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 906/1000\n",
      "Loss G (total):  tensor(492.2683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 907/1000\n",
      "Loss G (total):  tensor(493.8074, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 908/1000\n",
      "Loss G (total):  tensor(494.7685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 909/1000\n",
      "Loss G (total):  tensor(495.4006, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 910/1000\n",
      "Loss G (total):  tensor(496.8176, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 911/1000\n",
      "Loss G (total):  tensor(497.3806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 912/1000\n",
      "Loss G (total):  tensor(499.6739, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 913/1000\n",
      "Loss G (total):  tensor(499.2959, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 914/1000\n",
      "Loss G (total):  tensor(500.6228, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 915/1000\n",
      "Loss G (total):  tensor(501.6172, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 916/1000\n",
      "Loss G (total):  tensor(501.6627, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 917/1000\n",
      "Loss G (total):  tensor(502.6776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 918/1000\n",
      "Loss G (total):  tensor(502.6227, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 919/1000\n",
      "Loss G (total):  tensor(504.9601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 920/1000\n",
      "Loss G (total):  tensor(505.5308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 921/1000\n",
      "Loss G (total):  tensor(506.2478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 922/1000\n",
      "Loss G (total):  tensor(507.7240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 923/1000\n",
      "Loss G (total):  tensor(509.0190, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 924/1000\n",
      "Loss G (total):  tensor(510.3509, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 925/1000\n",
      "Loss G (total):  tensor(511.4953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 926/1000\n",
      "Loss G (total):  tensor(510.2139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 927/1000\n",
      "Loss G (total):  tensor(511.8584, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 928/1000\n",
      "Loss G (total):  tensor(512.7007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 929/1000\n",
      "Loss G (total):  tensor(514.5134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 930/1000\n",
      "Loss G (total):  tensor(515.4803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 931/1000\n",
      "Loss G (total):  tensor(515.7666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 932/1000\n",
      "Loss G (total):  tensor(516.8395, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 933/1000\n",
      "Loss G (total):  tensor(517.2275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 934/1000\n",
      "Loss G (total):  tensor(518.9627, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 935/1000\n",
      "Loss G (total):  tensor(519.7305, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 936/1000\n",
      "Loss G (total):  tensor(520.5660, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 937/1000\n",
      "Loss G (total):  tensor(521.6552, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 938/1000\n",
      "Loss G (total):  tensor(522.3347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 939/1000\n",
      "Loss G (total):  tensor(522.9011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 940/1000\n",
      "Loss G (total):  tensor(522.2131, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 941/1000\n",
      "Loss G (total):  tensor(525.7112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 942/1000\n",
      "Loss G (total):  tensor(526.4313, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 943/1000\n",
      "Loss G (total):  tensor(525.8901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 944/1000\n",
      "Loss G (total):  tensor(527.8282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 945/1000\n",
      "Loss G (total):  tensor(529.7822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 946/1000\n",
      "Loss G (total):  tensor(529.4275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 947/1000\n",
      "Loss G (total):  tensor(530.3231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 948/1000\n",
      "Loss G (total):  tensor(530.6760, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 949/1000\n",
      "Loss G (total):  tensor(531.5319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 950/1000\n",
      "Loss G (total):  tensor(533.9389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 951/1000\n",
      "Loss G (total):  tensor(533.4485, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 952/1000\n",
      "Loss G (total):  tensor(535.6818, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 953/1000\n",
      "Loss G (total):  tensor(536.6674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 954/1000\n",
      "Loss G (total):  tensor(536.6476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 955/1000\n",
      "Loss G (total):  tensor(537.4313, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 956/1000\n",
      "Loss G (total):  tensor(539.6948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 957/1000\n",
      "Loss G (total):  tensor(539.4672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 958/1000\n",
      "Loss G (total):  tensor(540.5034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 959/1000\n",
      "Loss G (total):  tensor(541.9424, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 960/1000\n",
      "Loss G (total):  tensor(545.4765, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 961/1000\n",
      "Loss G (total):  tensor(543.4477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 962/1000\n",
      "Loss G (total):  tensor(544.4248, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 963/1000\n",
      "Loss G (total):  tensor(546.0554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 964/1000\n",
      "Loss G (total):  tensor(548.0372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 965/1000\n",
      "Loss G (total):  tensor(547.6790, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 966/1000\n",
      "Loss G (total):  tensor(548.4760, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 967/1000\n",
      "Loss G (total):  tensor(549.9103, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 968/1000\n",
      "Loss G (total):  tensor(550.2151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 969/1000\n",
      "Loss G (total):  tensor(551.3837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 970/1000\n",
      "Loss G (total):  tensor(551.1709, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 971/1000\n",
      "Loss G (total):  tensor(552.3362, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 972/1000\n",
      "Loss G (total):  tensor(553.5479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 973/1000\n",
      "Loss G (total):  tensor(554.1252, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 974/1000\n",
      "Loss G (total):  tensor(555.9039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 975/1000\n",
      "Loss G (total):  tensor(557.9500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 976/1000\n",
      "Loss G (total):  tensor(557.7731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 977/1000\n",
      "Loss G (total):  tensor(559.6464, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 978/1000\n",
      "Loss G (total):  tensor(559.6904, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 979/1000\n",
      "Loss G (total):  tensor(560.3668, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 980/1000\n",
      "Loss G (total):  tensor(560.6845, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 981/1000\n",
      "Loss G (total):  tensor(561.7769, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 982/1000\n",
      "Loss G (total):  tensor(564.3510, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 983/1000\n",
      "Loss G (total):  tensor(566.3044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 984/1000\n",
      "Loss G (total):  tensor(565.4359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 985/1000\n",
      "Loss G (total):  tensor(567.8041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 986/1000\n",
      "Loss G (total):  tensor(568.6324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 987/1000\n",
      "Loss G (total):  tensor(568.9792, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 988/1000\n",
      "Loss G (total):  tensor(569.4731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 989/1000\n",
      "Loss G (total):  tensor(572.0790, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 990/1000\n",
      "Loss G (total):  tensor(570.5953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 991/1000\n",
      "Loss G (total):  tensor(572.2308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 992/1000\n",
      "Loss G (total):  tensor(573.9521, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 993/1000\n",
      "Loss G (total):  tensor(573.5358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 994/1000\n",
      "Loss G (total):  tensor(575.1038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 995/1000\n",
      "Loss G (total):  tensor(575.3387, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 996/1000\n",
      "Loss G (total):  tensor(577.6833, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 997/1000\n",
      "Loss G (total):  tensor(578.0742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 998/1000\n",
      "Loss G (total):  tensor(579.5226, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 999/1000\n",
      "Finish Synthetic Data Generation\n"
     ]
    }
   ],
   "source": [
    "from options_TGAN import Options\n",
    "from lib.TimeGAN import TimeGAN\n",
    "\n",
    "# 1. Options\n",
    "opt = Options().parse()\n",
    "\n",
    "# 2. Settings\n",
    "opt.seq_len = seq_len            # 256, mismo de arriba\n",
    "opt.iteration = 1000             # pocas iteraciones para probar\n",
    "opt.batch_size = 32              # batch peque√±o\n",
    "opt.n_critic = 5                 # ya lo definiste para WGAN-GP\n",
    "opt.gp_lambda = 10.0             # peso del gradient penalty\n",
    "opt.name = \"TimeGAN_real_small\"  # nombre del experimento (carpeta de salida)\n",
    "\n",
    "# 3. Create model\n",
    "model = TimeGAN(opt, ori_data)\n",
    "\n",
    "# 4. Train\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a6c8e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© Generating 50 samples in 1 batches of 64...\n",
      "  ‚úÖ Batch 1/1 generated (50 samples)\n"
     ]
    }
   ],
   "source": [
    "from generation_TGAN import safe_generation\n",
    "generated_data = safe_generation(model, num_samples=50, batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b7ec313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåÄ Ejecutando t-SNE (puede tardar 20‚Äì40s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dario\\Desktop\\ThesiS JBP\\jordan_venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_vis = 1000  # target\n",
    "\n",
    "n_real = min(n_vis, len(ori_data))\n",
    "n_fake = min(n_vis, len(generated_data))\n",
    "\n",
    "idx_real = np.random.choice(len(ori_data), n_real, replace=False)\n",
    "idx_fake = np.random.choice(len(generated_data), n_fake, replace=False)\n",
    "\n",
    "ori_vis = [ori_data[i] for i in idx_real]\n",
    "gen_vis = [generated_data[i] for i in idx_fake]\n",
    "\n",
    "from visualization_TGAN import visualization\n",
    "\n",
    "visualization(ori_vis, gen_vis, 'pca')\n",
    "visualization(ori_vis, gen_vis, 'tsne')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification 1 : \n",
    "# Discriminator - Change the discriminator to WGAN-GP, delete Spectral Norm, no sigmoid  and add layernorm. \n",
    "# We did this in ordert to avoid saturation and collapse in discriminator. Normal TIMEGAN uses sigmoid + BCE  to classify real/fake, is classifficator not critic regression.\n",
    "# We delete the sigmoid to get real scores, dsicriminatori not classify now estiamte Wasserstein distance, delete spectral norm, and add layernorm to give stability.\n",
    "# We did this change because BCE tends to collapse, WGAN-GP produces smooth gradients, more stable. Is ideal for vibrations .\n",
    "\n",
    "# Modification 2: \n",
    "# We dont include LSTM , instead we add LayerNorm to all sub networks (5 networks also modified its forward functions accordign to this) in order to reduce the instability and collapse, specially in signals viration.\n",
    "#TimeGAN only has GRU raw, without normalziation, so high variation of activations , noise + lenght seq generates collapse and inestbaility.\n",
    "# So we add LayerNorm in each GRU Encoder, Recovery, Generator, Supervisor, Discriminator. Layernorm stabilize each step of time, GRU becomes more stable wiht real noise, training smooth. Improve convergence.\n",
    "# \n",
    "# \n",
    "#  Modification 3: \n",
    "# Add Gradient Penalty TimeGAN, and reeplce all the backward_d of discriminator to WGAN-GP. \n",
    "# Here th timegan original used the BCE which measures probabilities true or false, and have gradients 0 o 1. So we use WGAN, , and by force GRADIENT PENALTY.\n",
    "# WGAN measures the real distance between distributions, not probailities. Avoid collapse, is more stable for continuous signals, and and produce smooth training.\n",
    "\n",
    "# Modification 4:\n",
    "# Trainning Loop WGAN, in TIMEGAN it train 1:1 G y D, is bad for WGAN generates poor gradients. New modification in training loop we train the critic 5 times more than generator.\n",
    "# This is better because , critic is strong, gradients are high quality when send to generator. Convergence is stable.\n",
    "\n",
    "\n",
    "# Modification 5 : change the real data loading indata pre processing, now it overlap 75% of information. As vibration signals change fast, is not o stable. Considering overlaping 75% we are sure will take all infroamtion, noramlly papers consider 50,75 and 90%, in order to make \n",
    "# the model learn correctly. for example if u have 1000 points a windows of 200, and failure occurs between 350-400, u will have small infromation o nly one window, but if u apply 75% , u will have 6-7 windows. so will have more relvant infroamtion. So this help to catch transitions better.BaseExceptionimprove GAN trainning , increase the number of windows \n",
    "# and dont loose important parts per window.\n",
    "# Original timegan dont use overlapping, so have low windows, less transition, so bad representation of virbation. Now we use overlapping of 75%, so generate 4 times more windows, captures more information .\n",
    "# so more infromatio is better cause enrich timegan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4aa3d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Adjusted opt.z_dim to match data feature size: 6\n",
      "[DEBUG] Loss this iteration: 0.145167\n",
      "Encoder training step: 0/100\n",
      "[DEBUG] Loss this iteration: 0.111384\n",
      "Encoder training step: 1/100\n",
      "[DEBUG] Loss this iteration: 0.091704\n",
      "Encoder training step: 2/100\n",
      "[DEBUG] Loss this iteration: 0.061466\n",
      "Encoder training step: 3/100\n",
      "[DEBUG] Loss this iteration: 0.046996\n",
      "Encoder training step: 4/100\n",
      "[DEBUG] Loss this iteration: 0.039331\n",
      "Encoder training step: 5/100\n",
      "[DEBUG] Loss this iteration: 0.026369\n",
      "Encoder training step: 6/100\n",
      "[DEBUG] Loss this iteration: 0.018642\n",
      "Encoder training step: 7/100\n",
      "[DEBUG] Loss this iteration: 0.017502\n",
      "Encoder training step: 8/100\n",
      "[DEBUG] Loss this iteration: 0.016454\n",
      "Encoder training step: 9/100\n",
      "[DEBUG] Loss this iteration: 0.017662\n",
      "Encoder training step: 10/100\n",
      "[DEBUG] Loss this iteration: 0.018016\n",
      "Encoder training step: 11/100\n",
      "[DEBUG] Loss this iteration: 0.015132\n",
      "Encoder training step: 12/100\n",
      "[DEBUG] Loss this iteration: 0.019403\n",
      "Encoder training step: 13/100\n",
      "[DEBUG] Loss this iteration: 0.018092\n",
      "Encoder training step: 14/100\n",
      "[DEBUG] Loss this iteration: 0.016845\n",
      "Encoder training step: 15/100\n",
      "[DEBUG] Loss this iteration: 0.017196\n",
      "Encoder training step: 16/100\n",
      "[DEBUG] Loss this iteration: 0.017374\n",
      "Encoder training step: 17/100\n",
      "[DEBUG] Loss this iteration: 0.018453\n",
      "Encoder training step: 18/100\n",
      "[DEBUG] Loss this iteration: 0.018084\n",
      "Encoder training step: 19/100\n",
      "[DEBUG] Loss this iteration: 0.017100\n",
      "Encoder training step: 20/100\n",
      "[DEBUG] Loss this iteration: 0.017156\n",
      "Encoder training step: 21/100\n",
      "[DEBUG] Loss this iteration: 0.017134\n",
      "Encoder training step: 22/100\n",
      "[DEBUG] Loss this iteration: 0.019035\n",
      "Encoder training step: 23/100\n",
      "[DEBUG] Loss this iteration: 0.016850\n",
      "Encoder training step: 24/100\n",
      "[DEBUG] Loss this iteration: 0.015904\n",
      "Encoder training step: 25/100\n",
      "[DEBUG] Loss this iteration: 0.017111\n",
      "Encoder training step: 26/100\n",
      "[DEBUG] Loss this iteration: 0.015386\n",
      "Encoder training step: 27/100\n",
      "[DEBUG] Loss this iteration: 0.016466\n",
      "Encoder training step: 28/100\n",
      "[DEBUG] Loss this iteration: 0.016668\n",
      "Encoder training step: 29/100\n",
      "[DEBUG] Loss this iteration: 0.016192\n",
      "Encoder training step: 30/100\n",
      "[DEBUG] Loss this iteration: 0.016431\n",
      "Encoder training step: 31/100\n",
      "[DEBUG] Loss this iteration: 0.014446\n",
      "Encoder training step: 32/100\n",
      "[DEBUG] Loss this iteration: 0.016496\n",
      "Encoder training step: 33/100\n",
      "[DEBUG] Loss this iteration: 0.019337\n",
      "Encoder training step: 34/100\n",
      "[DEBUG] Loss this iteration: 0.018813\n",
      "Encoder training step: 35/100\n",
      "[DEBUG] Loss this iteration: 0.018294\n",
      "Encoder training step: 36/100\n",
      "[DEBUG] Loss this iteration: 0.011522\n",
      "Encoder training step: 37/100\n",
      "[DEBUG] Loss this iteration: 0.015841\n",
      "Encoder training step: 38/100\n",
      "[DEBUG] Loss this iteration: 0.016682\n",
      "Encoder training step: 39/100\n",
      "[DEBUG] Loss this iteration: 0.015683\n",
      "Encoder training step: 40/100\n",
      "[DEBUG] Loss this iteration: 0.017322\n",
      "Encoder training step: 41/100\n",
      "[DEBUG] Loss this iteration: 0.016898\n",
      "Encoder training step: 42/100\n",
      "[DEBUG] Loss this iteration: 0.013775\n",
      "Encoder training step: 43/100\n",
      "[DEBUG] Loss this iteration: 0.015852\n",
      "Encoder training step: 44/100\n",
      "[DEBUG] Loss this iteration: 0.016330\n",
      "Encoder training step: 45/100\n",
      "[DEBUG] Loss this iteration: 0.015097\n",
      "Encoder training step: 46/100\n",
      "[DEBUG] Loss this iteration: 0.014832\n",
      "Encoder training step: 47/100\n",
      "[DEBUG] Loss this iteration: 0.018974\n",
      "Encoder training step: 48/100\n",
      "[DEBUG] Loss this iteration: 0.016693\n",
      "Encoder training step: 49/100\n",
      "[DEBUG] Loss this iteration: 0.016586\n",
      "Encoder training step: 50/100\n",
      "[DEBUG] Loss this iteration: 0.013908\n",
      "Encoder training step: 51/100\n",
      "[DEBUG] Loss this iteration: 0.015968\n",
      "Encoder training step: 52/100\n",
      "[DEBUG] Loss this iteration: 0.015532\n",
      "Encoder training step: 53/100\n",
      "[DEBUG] Loss this iteration: 0.014824\n",
      "Encoder training step: 54/100\n",
      "[DEBUG] Loss this iteration: 0.013629\n",
      "Encoder training step: 55/100\n",
      "[DEBUG] Loss this iteration: 0.015570\n",
      "Encoder training step: 56/100\n",
      "[DEBUG] Loss this iteration: 0.017907\n",
      "Encoder training step: 57/100\n",
      "[DEBUG] Loss this iteration: 0.014926\n",
      "Encoder training step: 58/100\n",
      "[DEBUG] Loss this iteration: 0.015097\n",
      "Encoder training step: 59/100\n",
      "[DEBUG] Loss this iteration: 0.014314\n",
      "Encoder training step: 60/100\n",
      "[DEBUG] Loss this iteration: 0.015698\n",
      "Encoder training step: 61/100\n",
      "[DEBUG] Loss this iteration: 0.018378\n",
      "Encoder training step: 62/100\n",
      "[DEBUG] Loss this iteration: 0.015575\n",
      "Encoder training step: 63/100\n",
      "[DEBUG] Loss this iteration: 0.013087\n",
      "Encoder training step: 64/100\n",
      "[DEBUG] Loss this iteration: 0.020018\n",
      "Encoder training step: 65/100\n",
      "[DEBUG] Loss this iteration: 0.015409\n",
      "Encoder training step: 66/100\n",
      "[DEBUG] Loss this iteration: 0.016484\n",
      "Encoder training step: 67/100\n",
      "[DEBUG] Loss this iteration: 0.012810\n",
      "Encoder training step: 68/100\n",
      "[DEBUG] Loss this iteration: 0.015825\n",
      "Encoder training step: 69/100\n",
      "[DEBUG] Loss this iteration: 0.013685\n",
      "Encoder training step: 70/100\n",
      "[DEBUG] Loss this iteration: 0.016921\n",
      "Encoder training step: 71/100\n",
      "[DEBUG] Loss this iteration: 0.016561\n",
      "Encoder training step: 72/100\n",
      "[DEBUG] Loss this iteration: 0.017592\n",
      "Encoder training step: 73/100\n",
      "[DEBUG] Loss this iteration: 0.019224\n",
      "Encoder training step: 74/100\n",
      "[DEBUG] Loss this iteration: 0.016958\n",
      "Encoder training step: 75/100\n",
      "[DEBUG] Loss this iteration: 0.015233\n",
      "Encoder training step: 76/100\n",
      "[DEBUG] Loss this iteration: 0.012857\n",
      "Encoder training step: 77/100\n",
      "[DEBUG] Loss this iteration: 0.012212\n",
      "Encoder training step: 78/100\n",
      "[DEBUG] Loss this iteration: 0.014869\n",
      "Encoder training step: 79/100\n",
      "[DEBUG] Loss this iteration: 0.020314\n",
      "Encoder training step: 80/100\n",
      "[DEBUG] Loss this iteration: 0.011303\n",
      "Encoder training step: 81/100\n",
      "[DEBUG] Loss this iteration: 0.015507\n",
      "Encoder training step: 82/100\n",
      "[DEBUG] Loss this iteration: 0.014472\n",
      "Encoder training step: 83/100\n",
      "[DEBUG] Loss this iteration: 0.013920\n",
      "Encoder training step: 84/100\n",
      "[DEBUG] Loss this iteration: 0.012748\n",
      "Encoder training step: 85/100\n",
      "[DEBUG] Loss this iteration: 0.016033\n",
      "Encoder training step: 86/100\n",
      "[DEBUG] Loss this iteration: 0.014145\n",
      "Encoder training step: 87/100\n",
      "[DEBUG] Loss this iteration: 0.014375\n",
      "Encoder training step: 88/100\n",
      "[DEBUG] Loss this iteration: 0.015415\n",
      "Encoder training step: 89/100\n",
      "[DEBUG] Loss this iteration: 0.013344\n",
      "Encoder training step: 90/100\n",
      "[DEBUG] Loss this iteration: 0.016890\n",
      "Encoder training step: 91/100\n",
      "[DEBUG] Loss this iteration: 0.015113\n",
      "Encoder training step: 92/100\n",
      "[DEBUG] Loss this iteration: 0.013972\n",
      "Encoder training step: 93/100\n",
      "[DEBUG] Loss this iteration: 0.011942\n",
      "Encoder training step: 94/100\n",
      "[DEBUG] Loss this iteration: 0.016158\n",
      "Encoder training step: 95/100\n",
      "[DEBUG] Loss this iteration: 0.009467\n",
      "Encoder training step: 96/100\n",
      "[DEBUG] Loss this iteration: 0.015565\n",
      "Encoder training step: 97/100\n",
      "[DEBUG] Loss this iteration: 0.012770\n",
      "Encoder training step: 98/100\n",
      "[DEBUG] Loss this iteration: 0.015531\n",
      "Encoder training step: 99/100\n",
      "Loss S:  tensor(0.0846, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 0/100\n",
      "Loss S:  tensor(0.0809, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 1/100\n",
      "Loss S:  tensor(0.0758, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 2/100\n",
      "Loss S:  tensor(0.0723, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 3/100\n",
      "Loss S:  tensor(0.0680, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 4/100\n",
      "Loss S:  tensor(0.0639, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 5/100\n",
      "Loss S:  tensor(0.0611, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 6/100\n",
      "Loss S:  tensor(0.0580, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 7/100\n",
      "Loss S:  tensor(0.0535, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 8/100\n",
      "Loss S:  tensor(0.0497, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 9/100\n",
      "Loss S:  tensor(0.0473, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 10/100\n",
      "Loss S:  tensor(0.0441, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 11/100\n",
      "Loss S:  tensor(0.0409, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 12/100\n",
      "Loss S:  tensor(0.0382, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 13/100\n",
      "Loss S:  tensor(0.0345, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 14/100\n",
      "Loss S:  tensor(0.0320, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 15/100\n",
      "Loss S:  tensor(0.0294, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 16/100\n",
      "Loss S:  tensor(0.0272, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 17/100\n",
      "Loss S:  tensor(0.0247, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 18/100\n",
      "Loss S:  tensor(0.0224, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 19/100\n",
      "Loss S:  tensor(0.0216, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 20/100\n",
      "Loss S:  tensor(0.0199, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 21/100\n",
      "Loss S:  tensor(0.0189, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 22/100\n",
      "Loss S:  tensor(0.0175, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 23/100\n",
      "Loss S:  tensor(0.0177, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 24/100\n",
      "Loss S:  tensor(0.0164, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 25/100\n",
      "Loss S:  tensor(0.0144, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 26/100\n",
      "Loss S:  tensor(0.0144, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 27/100\n",
      "Loss S:  tensor(0.0126, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 28/100\n",
      "Loss S:  tensor(0.0120, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 29/100\n",
      "Loss S:  tensor(0.0113, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 30/100\n",
      "Loss S:  tensor(0.0107, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 31/100\n",
      "Loss S:  tensor(0.0103, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 32/100\n",
      "Loss S:  tensor(0.0094, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 33/100\n",
      "Loss S:  tensor(0.0100, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 34/100\n",
      "Loss S:  tensor(0.0094, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 35/100\n",
      "Loss S:  tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 36/100\n",
      "Loss S:  tensor(0.0081, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 37/100\n",
      "Loss S:  tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 38/100\n",
      "Loss S:  tensor(0.0073, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 39/100\n",
      "Loss S:  tensor(0.0078, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 40/100\n",
      "Loss S:  tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 41/100\n",
      "Loss S:  tensor(0.0075, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 42/100\n",
      "Loss S:  tensor(0.0063, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 43/100\n",
      "Loss S:  tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 44/100\n",
      "Loss S:  tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 45/100\n",
      "Loss S:  tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 46/100\n",
      "Loss S:  tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 47/100\n",
      "Loss S:  tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 48/100\n",
      "Loss S:  tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 49/100\n",
      "Loss S:  tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 50/100\n",
      "Loss S:  tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 51/100\n",
      "Loss S:  tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 52/100\n",
      "Loss S:  tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 53/100\n",
      "Loss S:  tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 54/100\n",
      "Loss S:  tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 55/100\n",
      "Loss S:  tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 56/100\n",
      "Loss S:  tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 57/100\n",
      "Loss S:  tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 58/100\n",
      "Loss S:  tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 59/100\n",
      "Loss S:  tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 60/100\n",
      "Loss S:  tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 61/100\n",
      "Loss S:  tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 62/100\n",
      "Loss S:  tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 63/100\n",
      "Loss S:  tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 64/100\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 65/100\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 66/100\n",
      "Loss S:  tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 67/100\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 68/100\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 69/100\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 70/100\n",
      "Loss S:  tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 71/100\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 72/100\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 73/100\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 74/100\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 75/100\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 76/100\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 77/100\n",
      "Loss S:  tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 78/100\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 79/100\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 80/100\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 81/100\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 82/100\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 83/100\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 84/100\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 85/100\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 86/100\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 87/100\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 88/100\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 89/100\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 90/100\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 91/100\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 92/100\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 93/100\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 94/100\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 95/100\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 96/100\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 97/100\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 98/100\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 99/100\n",
      "Loss G (total):  tensor(16.3235, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 0/100\n",
      "Loss G (total):  tensor(19.7423, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 1/100\n",
      "Loss G (total):  tensor(15.7866, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 2/100\n",
      "Loss G (total):  tensor(21.3697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 3/100\n",
      "Loss G (total):  tensor(18.1681, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 4/100\n",
      "Loss G (total):  tensor(19.9594, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 5/100\n",
      "Loss G (total):  tensor(19.8694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 6/100\n",
      "Loss G (total):  tensor(19.1765, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 7/100\n",
      "Loss G (total):  tensor(19.4721, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 8/100\n",
      "Loss G (total):  tensor(18.6358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 9/100\n",
      "Loss G (total):  tensor(18.7292, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 10/100\n",
      "Loss G (total):  tensor(20.7057, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 11/100\n",
      "Loss G (total):  tensor(20.2190, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 12/100\n",
      "Loss G (total):  tensor(22.9548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 13/100\n",
      "Loss G (total):  tensor(25.1890, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 14/100\n",
      "Loss G (total):  tensor(25.4858, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 15/100\n",
      "Loss G (total):  tensor(26.7764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 16/100\n",
      "Loss G (total):  tensor(26.2669, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 17/100\n",
      "Loss G (total):  tensor(27.2694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 18/100\n",
      "Loss G (total):  tensor(25.6228, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 19/100\n",
      "Loss G (total):  tensor(26.5173, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 20/100\n",
      "Loss G (total):  tensor(26.1450, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 21/100\n",
      "Loss G (total):  tensor(28.2852, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 22/100\n",
      "Loss G (total):  tensor(27.6358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 23/100\n",
      "Loss G (total):  tensor(25.5721, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 24/100\n",
      "Loss G (total):  tensor(26.4249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 25/100\n",
      "Loss G (total):  tensor(29.9946, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 26/100\n",
      "Loss G (total):  tensor(28.5015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 27/100\n",
      "Loss G (total):  tensor(29.9626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 28/100\n",
      "Loss G (total):  tensor(30.5901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 29/100\n",
      "Loss G (total):  tensor(29.8999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 30/100\n",
      "Loss G (total):  tensor(31.2858, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 31/100\n",
      "Loss G (total):  tensor(29.8102, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 32/100\n",
      "Loss G (total):  tensor(29.2996, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 33/100\n",
      "Loss G (total):  tensor(29.5732, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 34/100\n",
      "Loss G (total):  tensor(29.0503, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 35/100\n",
      "Loss G (total):  tensor(26.8042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 36/100\n",
      "Loss G (total):  tensor(26.2797, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 37/100\n",
      "Loss G (total):  tensor(28.6235, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 38/100\n",
      "Loss G (total):  tensor(27.5391, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 39/100\n",
      "Loss G (total):  tensor(27.4253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 40/100\n",
      "Loss G (total):  tensor(29.6392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 41/100\n",
      "Loss G (total):  tensor(30.0542, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 42/100\n",
      "Loss G (total):  tensor(29.8631, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 43/100\n",
      "Loss G (total):  tensor(27.7532, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 44/100\n",
      "Loss G (total):  tensor(28.9554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 45/100\n",
      "Loss G (total):  tensor(27.3791, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 46/100\n",
      "Loss G (total):  tensor(28.2923, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 47/100\n",
      "Loss G (total):  tensor(26.4764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 48/100\n",
      "Loss G (total):  tensor(30.0299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 49/100\n",
      "Loss G (total):  tensor(28.7686, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 50/100\n",
      "Loss G (total):  tensor(29.5390, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 51/100\n",
      "Loss G (total):  tensor(30.0304, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 52/100\n",
      "Loss G (total):  tensor(28.7914, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 53/100\n",
      "Loss G (total):  tensor(29.5031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 54/100\n",
      "Loss G (total):  tensor(30.1065, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 55/100\n",
      "Loss G (total):  tensor(28.5960, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 56/100\n",
      "Loss G (total):  tensor(28.8949, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 57/100\n",
      "Loss G (total):  tensor(32.2188, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 58/100\n",
      "Loss G (total):  tensor(29.3073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 59/100\n",
      "Loss G (total):  tensor(31.9266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 60/100\n",
      "Loss G (total):  tensor(33.9547, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 61/100\n",
      "Loss G (total):  tensor(31.0046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 62/100\n",
      "Loss G (total):  tensor(31.2922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 63/100\n",
      "Loss G (total):  tensor(29.7139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 64/100\n",
      "Loss G (total):  tensor(30.6888, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 65/100\n",
      "Loss G (total):  tensor(33.1409, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 66/100\n",
      "Loss G (total):  tensor(30.1718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 67/100\n",
      "Loss G (total):  tensor(31.8798, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 68/100\n",
      "Loss G (total):  tensor(33.8873, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 69/100\n",
      "Loss G (total):  tensor(30.7093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 70/100\n",
      "Loss G (total):  tensor(33.5979, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 71/100\n",
      "Loss G (total):  tensor(31.2871, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 72/100\n",
      "Loss G (total):  tensor(32.0288, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 73/100\n",
      "Loss G (total):  tensor(30.0682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 74/100\n",
      "Loss G (total):  tensor(29.6672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 75/100\n",
      "Loss G (total):  tensor(29.2027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 76/100\n",
      "Loss G (total):  tensor(32.8708, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 77/100\n",
      "Loss G (total):  tensor(30.4226, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 78/100\n",
      "Loss G (total):  tensor(33.6742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 79/100\n",
      "Loss G (total):  tensor(29.4959, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 80/100\n",
      "Loss G (total):  tensor(30.6367, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 81/100\n",
      "Loss G (total):  tensor(33.2570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 82/100\n",
      "Loss G (total):  tensor(31.7886, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 83/100\n",
      "Loss G (total):  tensor(30.3431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 84/100\n",
      "Loss G (total):  tensor(30.4008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 85/100\n",
      "Loss G (total):  tensor(33.1589, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 86/100\n",
      "Loss G (total):  tensor(31.3342, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 87/100\n",
      "Loss G (total):  tensor(30.9817, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 88/100\n",
      "Loss G (total):  tensor(34.3133, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 89/100\n",
      "Loss G (total):  tensor(32.3316, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 90/100\n",
      "Loss G (total):  tensor(35.7262, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 91/100\n",
      "Loss G (total):  tensor(32.0027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 92/100\n",
      "Loss G (total):  tensor(38.2840, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 93/100\n",
      "Loss G (total):  tensor(33.9397, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 94/100\n",
      "Loss G (total):  tensor(33.1172, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 95/100\n",
      "Loss G (total):  tensor(35.1080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 96/100\n",
      "Loss G (total):  tensor(31.1690, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 97/100\n",
      "Loss G (total):  tensor(31.3048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 98/100\n",
      "Loss G (total):  tensor(35.1494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 99/100\n",
      "Finish Synthetic Data Generation\n"
     ]
    }
   ],
   "source": [
    "# This point we modified the PCA to make multivariable comparisson . To properly compare real and synthetic sequences, you must use all variables and all time steps.\n",
    "# Before visualization averaged all variables, not reliable for evaluating. now we flatten the full multivariate time-series (all variables included) so PCA and t-SNE compare complete real and generated sequences correctly\n",
    "\n",
    "from options_TGAN import Options\n",
    "from lib.TimeGAN import TimeGAN\n",
    "\n",
    "# 1. Options\n",
    "opt = Options().parse()\n",
    "\n",
    "# 2. Set paper-style hyperparameters\n",
    "opt.seq_len = seq_len            # 256\n",
    "opt.lr = 1e-4                    # Learning rate paper\n",
    "opt.beta1 = 0.5                  # Momentum recomend\n",
    "opt.batch_size = 64              # Batch paper\n",
    "opt.iteration = 100               # Iteraciones \n",
    "opt.n_critic = 5                 # WGAN-GP \n",
    "opt.gp_lambda = 10.0             # Weight penalty\n",
    "opt.name = \"TimeGAN_real_paper_settings\"\n",
    "\n",
    "# 3. Create model\n",
    "model = TimeGAN(opt, ori_data)\n",
    "\n",
    "# 4. Train\n",
    "model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3aa7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© Generating 50 samples in 1 batches of 64...\n",
      "  ‚úÖ Batch 1/1 generated (50 samples)\n"
     ]
    }
   ],
   "source": [
    "from generation_TGAN import safe_generation\n",
    "generated_data = safe_generation(model, num_samples=50, batch_size=64)\n",
    "\n",
    "n_vis = 5000  # target\n",
    "\n",
    "n_real = min(n_vis, len(ori_data))\n",
    "n_fake = min(n_vis, len(generated_data))\n",
    "\n",
    "idx_real = np.random.choice(len(ori_data), n_real, replace=False)\n",
    "idx_fake = np.random.choice(len(generated_data), n_fake, replace=False)\n",
    "\n",
    "ori_vis = [ori_data[i] for i in idx_real]\n",
    "gen_vis = [generated_data[i] for i in idx_fake]\n",
    "\n",
    "from visualization_TGAN import visualization\n",
    "\n",
    "visualization(ori_vis, gen_vis, 'pca')\n",
    "visualization(ori_vis, gen_vis, 'tsne')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6483ae22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåÄ Ejecutando t-SNE (puede tardar unos segundos)...\n",
      "[t-SNE] Computing 99 nearest neighbors...\n",
      "[t-SNE] Indexed 100 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 100 samples in 0.031s...\n",
      "[t-SNE] Computed conditional probabilities for sample 100 / 100\n",
      "[t-SNE] Mean sigma: 1.271805\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 45.873119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dario\\Desktop\\ThesiS JBP\\jordan_venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] KL divergence after 500 iterations: 0.057000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jordan_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
