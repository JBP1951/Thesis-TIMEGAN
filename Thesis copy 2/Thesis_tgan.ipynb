{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f024853",
   "metadata": {},
   "source": [
    "IMPLEMENTATION\n",
    "- LSTM/RNN AND THEN TRANSFORMERS INSTEAD RNN\n",
    "- MODIFY THE GENERATOR AND INTRODUCE A WASSERSTEIN + GRADIENT PENALTY\n",
    "- IMPLEMENTATION IN THE ORIGINAL TIME GAN CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b98c7df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrados 1 archivos:\n",
      "C:\\Users\\Dario\\Desktop\\ThesiS JBP\\Data\\signals_for_GAN_01.mat\n",
      "‚úÖ Concatenated data shape: (6144001, 6)\n",
      "Tama√±os por test: [(6144001, 6)]\n",
      "‚úÖ All tests has same length\n",
      "‚úÖ Created 24000 sequences (each shape = (256, 6))\n",
      "üíæ Saved 'data_sequences.npy' for next runs.\n"
     ]
    }
   ],
   "source": [
    "#Import Data\n",
    "\n",
    "#Import libraries\n",
    "import scipy.io as sio\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "\n",
    "# Look for file signals_for_GAN_XX.mat ---\n",
    "data_dir = r\"C:\\Users\\Dario\\Desktop\\ThesiS JBP\\Data\"  # la r evita problemas con \\\n",
    "file_list = sorted(glob.glob(f\"{data_dir}/signals_for_GAN_*.mat\"))\n",
    "\n",
    "print(f\"Encontrados {len(file_list)} archivos:\")\n",
    "for f in file_list:\n",
    "    print(f)\n",
    "\n",
    "if len(file_list) == 0:\n",
    "    raise FileNotFoundError(\"Not found file 'signals_for_GAN_*.mat'.\")\n",
    "\n",
    "# Concatenate Data vertically\n",
    "# Before normalization we need to organize the global data\n",
    "all_data_concat = []\n",
    "for file in file_list:\n",
    "    mat = sio.loadmat(file)\n",
    "    data = mat['data_all']  # [N x 6]\n",
    "    all_data_concat.append(data)\n",
    "\n",
    "data_global = np.vstack(all_data_concat)\n",
    "print(f\"‚úÖ Concatenated data shape: {data_global.shape}\")\n",
    "\n",
    "#Check size length\n",
    "shapes = [d.shape for d in all_data_concat]\n",
    "print(\"Tama√±os por test:\", shapes)\n",
    "assert len(set([s[0] for s in shapes])) == 1, \"‚ùå Different lengths in test\"\n",
    "print(\"‚úÖ All tests has same length\")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from lib.data_preprocess import load_data\n",
    "# ------------------------------------------------------------\n",
    "# üîÑ Create fixed-length sequences for TimeGAN\n",
    "# ------------------------------------------------------------\n",
    "seq_len = 256  # HERE MODIFY ACCORDINT TO THE PAPER SELECTED AS REFERENCE\n",
    "num_seq = data_global.shape[0] // seq_len\n",
    "\n",
    "# cortar para que sea m√∫ltiplo exacto\n",
    "data_global = data_global[:num_seq * seq_len]\n",
    "\n",
    "# crear las secuencias 3D (num_seq, seq_len, features)\n",
    "data_sequences = data_global.reshape(num_seq, seq_len, -1).astype(np.float32)\n",
    "\n",
    "print(f\"‚úÖ Created {len(data_sequences)} sequences (each shape = {data_sequences[0].shape})\")\n",
    "\n",
    "# üíæ save for next runs\n",
    "np.save(\"data_sequences.npy\", data_sequences)\n",
    "print(\"üíæ Saved 'data_sequences.npy' for next runs.\")\n",
    "\n",
    "def safe_generation(model, num_samples, batch_size=64):\n",
    "    \"\"\"\n",
    "    Genera datos sint√©ticos en bloques peque√±os para evitar errores de √≠ndice o memoria.\n",
    "    Compatible con el c√≥digo original de TimeGAN sin modificarlo.\n",
    "    \"\"\"\n",
    "    generated_all = []\n",
    "    n_batches = (num_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    print(f\"üß© Generando {num_samples} muestras en {n_batches} mini-batches de {batch_size}...\")\n",
    "    for b in range(n_batches):\n",
    "        start = b * batch_size\n",
    "        end = min((b + 1) * batch_size, num_samples)\n",
    "        current_n = end - start\n",
    "\n",
    "        # usar longitudes de ventana reales solo para este bloque\n",
    "        model.T = model.ori_time[start:end]\n",
    "\n",
    "        # generar bloque actual (usa el m√©todo original del modelo)\n",
    "        gen_batch = model.generation(num_samples=current_n)\n",
    "        generated_all.extend(gen_batch)\n",
    "\n",
    "        print(f\"  ‚úÖ Bloque {b+1}/{n_batches} generado ({current_n} muestras)\")\n",
    "\n",
    "    return generated_all\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================\n",
    "#  Full TimeGAN Training (All Phases)\n",
    "# =============================================\n",
    "\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from options_TGAN import Options\n",
    "from lib.TimeGAN import TimeGAN  # üëà make sure you import the right class\n",
    "from generation_TGAN import safe_generation\n",
    "from visualization_TGAN import visualization\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1Ô∏è‚É£ Load your preprocessed rotor data\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "data_sequences = np.load(\"data_sequences.npy\", allow_pickle=True)\n",
    "print(f\"‚úÖ Loaded {len(data_sequences)} sequences (each shape = {data_sequences[0].shape})\")\n",
    "\n",
    "opt_parser = Options()\n",
    "opt = opt_parser.parser.parse_args(args=[])\n",
    "# ------------------------------------------------------------\n",
    "# 3Ô∏è‚É£ Initialize and train the full TimeGAN\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüß© Initializing TimeGAN model...\")\n",
    "model = TimeGAN(opt, ori_data=data_sequences)\n",
    "print(\"‚úÖ Model initialized successfully.\\n\")\n",
    "\n",
    "print(\"üöÄ Starting FULL TimeGAN training (all phases)...\\n\")\n",
    "model.train()  # üëà this automatically runs ER ‚Üí S ‚Üí Joint (G+D)\n",
    "print(\"\\n‚úÖ Full TimeGAN training completed successfully.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4Ô∏è‚É£ Generate synthetic sequences\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüé® Generating synthetic sequences...\")\n",
    "generated_data = safe_generation(model, num_samples=2000, batch_size=64)\n",
    "print(f\"‚úÖ Generated {len(generated_data)} synthetic sequences.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5Ô∏è‚É£ Visualization (PCA + t-SNE)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüìä Visualizing results (PCA & t-SNE)...\")\n",
    "visualization(data_sequences[:1000], generated_data[:1000], analysis='pca')\n",
    "visualization(data_sequences[:1000], generated_data[:1000], analysis='tsne')\n",
    "\n",
    "print(\"\\nüéâ All phases completed successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification 1 : \n",
    "# Discriminator - Change the discriminator to WGAN-GP, delete Spectral Norm, no sigmoid  and add layernorm. \n",
    "# We did this in ordert to avoid saturation and collapse in discriminator. Normal TIMEGAN uses sigmoid + BCE  to classify real/fake, is classifficator not critic regression.\n",
    "# We delete the sigmoid to get real scores, dsicriminatori not classify now estiamte Wasserstein distance, delete spectral norm, and add layernorm to give stability.\n",
    "# We did this change because BCE tends to collapse, WGAN-GP produces smooth gradients, more stable. Is ideal for vibrations .\n",
    "\n",
    "# Modification 2: \n",
    "# We dont include LSTM , instead we add LayerNorm to all sub networks (5 networks also modified its forward functions accordign to this) in order to reduce the instability and collapse, specially in signals viration.\n",
    "#TimeGAN only has GRU raw, without normalziation, so high variation of activations , noise + lenght seq generates collapse and inestbaility.\n",
    "# So we add LayerNorm in each GRU Encoder, Recovery, Generator, Supervisor, Discriminator. Layernorm stabilize each step of time, GRU becomes more stable wiht real noise, training smooth. Improve convergence.\n",
    "# \n",
    "# \n",
    "#  Modification 3: \n",
    "# Add Gradient Penalty TimeGAN, and reeplce all the backward_d of discriminator to WGAN-GP. \n",
    "# Here th timegan original used the BCE which measures probabilities true or false, and have gradients 0 o 1. So we use WGAN, , and by force GRADIENT PENALTY.\n",
    "# WGAN measures the real distance between distributions, not probailities. Avoid collapse, is more stable for continuous signals, and and produce smooth training.\n",
    "\n",
    "# Modification 4:\n",
    "# Trainning Loop WGAN, in TIMEGAN it train 1:1 G y D, is bad for WGAN generates poor gradients. New modification in training loop we train the critic 5 times more than generator.\n",
    "# This is better because , critic is strong, gradients are high quality when send to generator. Convergence is stable.\n",
    "\n",
    "\n",
    "# Modification 5 : change the real data loading indata pre processing, now it overlap 75% of information. As vibration signals change fast, is not o stable. Considering overlaping 75% we are sure will take all infroamtion, noramlly papers consider 50,75 and 90%, in order to make \n",
    "# the model learn correctly. for example if u have 1000 points a windows of 200, and failure occurs between 350-400, u will have small infromation o nly one window, but if u apply 75% , u will have 6-7 windows. so will have more relvant infroamtion. So this help to catch transitions better.BaseExceptionimprove GAN trainning , increase the number of windows \n",
    "# and dont loose important parts per window.\n",
    "# Original timegan dont use overlapping, so have low windows, less transition, so bad representation of virbation. Now we use overlapping of 75%, so generate 4 times more windows, captures more information .\n",
    "# so more infromatio is better cause enrich timegan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa3d07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jordan_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
