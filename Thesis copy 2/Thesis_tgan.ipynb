{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f024853",
   "metadata": {},
   "source": [
    "IMPLEMENTATION\n",
    "- LSTM/RNN AND THEN TRANSFORMERS INSTEAD RNN\n",
    "- MODIFY THE GENERATOR AND INTRODUCE A WASSERSTEIN + GRADIENT PENALTY\n",
    "- IMPLEMENTATION IN THE ORIGINAL TIME GAN CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b98c7df3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_data() got an unexpected keyword argument 'step'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m file_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/signals_for_GAN_*.mat\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      7\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[1;32m----> 9\u001b[0m ori_data, scaler \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmytests\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# <--- AQUÍ DEFINES OVERLAP\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# <--- Límite para evitar 70GB RAM\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ori_data), ori_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mTypeError\u001b[0m: load_data() got an unexpected keyword argument 'step'"
     ]
    }
   ],
   "source": [
    "from lib.data_preprocess import load_data\n",
    "import glob\n",
    "\n",
    "data_dir = r\"C:\\Users\\Dario\\Desktop\\ThesiS JBP\\Data\"\n",
    "file_list = sorted(glob.glob(f\"{data_dir}/signals_for_GAN_*.mat\"))\n",
    "\n",
    "seq_len = 256\n",
    "\n",
    "ori_data, scaler = load_data(\n",
    "    \"mytests\",\n",
    "    seq_len,\n",
    "    file_list=file_list,\n",
    "    step=128,           # <--- AQUÍ DEFINES OVERLAP\n",
    "    max_sequences=50000 # <--- Límite para evitar 70GB RAM\n",
    ")\n",
    "\n",
    "print(len(ori_data), ori_data[0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5245412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Adjusted opt.z_dim to match data feature size: 6\n",
      "[DEBUG] Loss this iteration: 0.077676\n",
      "Encoder training step: 0/1000\n",
      "[DEBUG] Loss this iteration: 0.048913\n",
      "Encoder training step: 1/1000\n",
      "[DEBUG] Loss this iteration: 0.023869\n",
      "Encoder training step: 2/1000\n",
      "[DEBUG] Loss this iteration: 0.017437\n",
      "Encoder training step: 3/1000\n",
      "[DEBUG] Loss this iteration: 0.019619\n",
      "Encoder training step: 4/1000\n",
      "[DEBUG] Loss this iteration: 0.017665\n",
      "Encoder training step: 5/1000\n",
      "[DEBUG] Loss this iteration: 0.017516\n",
      "Encoder training step: 6/1000\n",
      "[DEBUG] Loss this iteration: 0.021335\n",
      "Encoder training step: 7/1000\n",
      "[DEBUG] Loss this iteration: 0.018692\n",
      "Encoder training step: 8/1000\n",
      "[DEBUG] Loss this iteration: 0.017317\n",
      "Encoder training step: 9/1000\n",
      "[DEBUG] Loss this iteration: 0.016043\n",
      "Encoder training step: 10/1000\n",
      "[DEBUG] Loss this iteration: 0.020323\n",
      "Encoder training step: 11/1000\n",
      "[DEBUG] Loss this iteration: 0.017608\n",
      "Encoder training step: 12/1000\n",
      "[DEBUG] Loss this iteration: 0.017456\n",
      "Encoder training step: 13/1000\n",
      "[DEBUG] Loss this iteration: 0.016025\n",
      "Encoder training step: 14/1000\n",
      "[DEBUG] Loss this iteration: 0.018847\n",
      "Encoder training step: 15/1000\n",
      "[DEBUG] Loss this iteration: 0.015174\n",
      "Encoder training step: 16/1000\n",
      "[DEBUG] Loss this iteration: 0.016838\n",
      "Encoder training step: 17/1000\n",
      "[DEBUG] Loss this iteration: 0.019010\n",
      "Encoder training step: 18/1000\n",
      "[DEBUG] Loss this iteration: 0.020684\n",
      "Encoder training step: 19/1000\n",
      "[DEBUG] Loss this iteration: 0.013892\n",
      "Encoder training step: 20/1000\n",
      "[DEBUG] Loss this iteration: 0.021231\n",
      "Encoder training step: 21/1000\n",
      "[DEBUG] Loss this iteration: 0.017575\n",
      "Encoder training step: 22/1000\n",
      "[DEBUG] Loss this iteration: 0.014296\n",
      "Encoder training step: 23/1000\n",
      "[DEBUG] Loss this iteration: 0.022679\n",
      "Encoder training step: 24/1000\n",
      "[DEBUG] Loss this iteration: 0.014775\n",
      "Encoder training step: 25/1000\n",
      "[DEBUG] Loss this iteration: 0.015689\n",
      "Encoder training step: 26/1000\n",
      "[DEBUG] Loss this iteration: 0.023458\n",
      "Encoder training step: 27/1000\n",
      "[DEBUG] Loss this iteration: 0.015014\n",
      "Encoder training step: 28/1000\n",
      "[DEBUG] Loss this iteration: 0.018761\n",
      "Encoder training step: 29/1000\n",
      "[DEBUG] Loss this iteration: 0.018017\n",
      "Encoder training step: 30/1000\n",
      "[DEBUG] Loss this iteration: 0.013424\n",
      "Encoder training step: 31/1000\n",
      "[DEBUG] Loss this iteration: 0.023403\n",
      "Encoder training step: 32/1000\n",
      "[DEBUG] Loss this iteration: 0.025310\n",
      "Encoder training step: 33/1000\n",
      "[DEBUG] Loss this iteration: 0.018610\n",
      "Encoder training step: 34/1000\n",
      "[DEBUG] Loss this iteration: 0.017655\n",
      "Encoder training step: 35/1000\n",
      "[DEBUG] Loss this iteration: 0.015916\n",
      "Encoder training step: 36/1000\n",
      "[DEBUG] Loss this iteration: 0.014520\n",
      "Encoder training step: 37/1000\n",
      "[DEBUG] Loss this iteration: 0.021069\n",
      "Encoder training step: 38/1000\n",
      "[DEBUG] Loss this iteration: 0.014581\n",
      "Encoder training step: 39/1000\n",
      "[DEBUG] Loss this iteration: 0.015367\n",
      "Encoder training step: 40/1000\n",
      "[DEBUG] Loss this iteration: 0.018136\n",
      "Encoder training step: 41/1000\n",
      "[DEBUG] Loss this iteration: 0.016017\n",
      "Encoder training step: 42/1000\n",
      "[DEBUG] Loss this iteration: 0.016633\n",
      "Encoder training step: 43/1000\n",
      "[DEBUG] Loss this iteration: 0.015995\n",
      "Encoder training step: 44/1000\n",
      "[DEBUG] Loss this iteration: 0.020708\n",
      "Encoder training step: 45/1000\n",
      "[DEBUG] Loss this iteration: 0.022845\n",
      "Encoder training step: 46/1000\n",
      "[DEBUG] Loss this iteration: 0.020032\n",
      "Encoder training step: 47/1000\n",
      "[DEBUG] Loss this iteration: 0.017155\n",
      "Encoder training step: 48/1000\n",
      "[DEBUG] Loss this iteration: 0.016358\n",
      "Encoder training step: 49/1000\n",
      "[DEBUG] Loss this iteration: 0.015952\n",
      "Encoder training step: 50/1000\n",
      "[DEBUG] Loss this iteration: 0.018470\n",
      "Encoder training step: 51/1000\n",
      "[DEBUG] Loss this iteration: 0.017605\n",
      "Encoder training step: 52/1000\n",
      "[DEBUG] Loss this iteration: 0.015061\n",
      "Encoder training step: 53/1000\n",
      "[DEBUG] Loss this iteration: 0.015442\n",
      "Encoder training step: 54/1000\n",
      "[DEBUG] Loss this iteration: 0.017501\n",
      "Encoder training step: 55/1000\n",
      "[DEBUG] Loss this iteration: 0.015984\n",
      "Encoder training step: 56/1000\n",
      "[DEBUG] Loss this iteration: 0.018660\n",
      "Encoder training step: 57/1000\n",
      "[DEBUG] Loss this iteration: 0.016457\n",
      "Encoder training step: 58/1000\n",
      "[DEBUG] Loss this iteration: 0.015061\n",
      "Encoder training step: 59/1000\n",
      "[DEBUG] Loss this iteration: 0.018989\n",
      "Encoder training step: 60/1000\n",
      "[DEBUG] Loss this iteration: 0.023355\n",
      "Encoder training step: 61/1000\n",
      "[DEBUG] Loss this iteration: 0.019558\n",
      "Encoder training step: 62/1000\n",
      "[DEBUG] Loss this iteration: 0.015507\n",
      "Encoder training step: 63/1000\n",
      "[DEBUG] Loss this iteration: 0.016901\n",
      "Encoder training step: 64/1000\n",
      "[DEBUG] Loss this iteration: 0.018447\n",
      "Encoder training step: 65/1000\n",
      "[DEBUG] Loss this iteration: 0.014850\n",
      "Encoder training step: 66/1000\n",
      "[DEBUG] Loss this iteration: 0.014679\n",
      "Encoder training step: 67/1000\n",
      "[DEBUG] Loss this iteration: 0.019528\n",
      "Encoder training step: 68/1000\n",
      "[DEBUG] Loss this iteration: 0.017936\n",
      "Encoder training step: 69/1000\n",
      "[DEBUG] Loss this iteration: 0.017247\n",
      "Encoder training step: 70/1000\n",
      "[DEBUG] Loss this iteration: 0.019088\n",
      "Encoder training step: 71/1000\n",
      "[DEBUG] Loss this iteration: 0.017860\n",
      "Encoder training step: 72/1000\n",
      "[DEBUG] Loss this iteration: 0.016066\n",
      "Encoder training step: 73/1000\n",
      "[DEBUG] Loss this iteration: 0.016474\n",
      "Encoder training step: 74/1000\n",
      "[DEBUG] Loss this iteration: 0.014574\n",
      "Encoder training step: 75/1000\n",
      "[DEBUG] Loss this iteration: 0.018398\n",
      "Encoder training step: 76/1000\n",
      "[DEBUG] Loss this iteration: 0.019086\n",
      "Encoder training step: 77/1000\n",
      "[DEBUG] Loss this iteration: 0.017633\n",
      "Encoder training step: 78/1000\n",
      "[DEBUG] Loss this iteration: 0.021125\n",
      "Encoder training step: 79/1000\n",
      "[DEBUG] Loss this iteration: 0.023005\n",
      "Encoder training step: 80/1000\n",
      "[DEBUG] Loss this iteration: 0.017437\n",
      "Encoder training step: 81/1000\n",
      "[DEBUG] Loss this iteration: 0.019248\n",
      "Encoder training step: 82/1000\n",
      "[DEBUG] Loss this iteration: 0.022322\n",
      "Encoder training step: 83/1000\n",
      "[DEBUG] Loss this iteration: 0.015348\n",
      "Encoder training step: 84/1000\n",
      "[DEBUG] Loss this iteration: 0.014019\n",
      "Encoder training step: 85/1000\n",
      "[DEBUG] Loss this iteration: 0.015090\n",
      "Encoder training step: 86/1000\n",
      "[DEBUG] Loss this iteration: 0.016711\n",
      "Encoder training step: 87/1000\n",
      "[DEBUG] Loss this iteration: 0.020103\n",
      "Encoder training step: 88/1000\n",
      "[DEBUG] Loss this iteration: 0.015604\n",
      "Encoder training step: 89/1000\n",
      "[DEBUG] Loss this iteration: 0.019821\n",
      "Encoder training step: 90/1000\n",
      "[DEBUG] Loss this iteration: 0.016275\n",
      "Encoder training step: 91/1000\n",
      "[DEBUG] Loss this iteration: 0.016301\n",
      "Encoder training step: 92/1000\n",
      "[DEBUG] Loss this iteration: 0.019655\n",
      "Encoder training step: 93/1000\n",
      "[DEBUG] Loss this iteration: 0.019380\n",
      "Encoder training step: 94/1000\n",
      "[DEBUG] Loss this iteration: 0.016678\n",
      "Encoder training step: 95/1000\n",
      "[DEBUG] Loss this iteration: 0.018268\n",
      "Encoder training step: 96/1000\n",
      "[DEBUG] Loss this iteration: 0.015662\n",
      "Encoder training step: 97/1000\n",
      "[DEBUG] Loss this iteration: 0.014579\n",
      "Encoder training step: 98/1000\n",
      "[DEBUG] Loss this iteration: 0.013281\n",
      "Encoder training step: 99/1000\n",
      "[DEBUG] Loss this iteration: 0.021881\n",
      "Encoder training step: 100/1000\n",
      "[DEBUG] Loss this iteration: 0.015376\n",
      "Encoder training step: 101/1000\n",
      "[DEBUG] Loss this iteration: 0.011339\n",
      "Encoder training step: 102/1000\n",
      "[DEBUG] Loss this iteration: 0.018095\n",
      "Encoder training step: 103/1000\n",
      "[DEBUG] Loss this iteration: 0.015590\n",
      "Encoder training step: 104/1000\n",
      "[DEBUG] Loss this iteration: 0.025914\n",
      "Encoder training step: 105/1000\n",
      "[DEBUG] Loss this iteration: 0.018358\n",
      "Encoder training step: 106/1000\n",
      "[DEBUG] Loss this iteration: 0.018491\n",
      "Encoder training step: 107/1000\n",
      "[DEBUG] Loss this iteration: 0.018555\n",
      "Encoder training step: 108/1000\n",
      "[DEBUG] Loss this iteration: 0.017781\n",
      "Encoder training step: 109/1000\n",
      "[DEBUG] Loss this iteration: 0.018495\n",
      "Encoder training step: 110/1000\n",
      "[DEBUG] Loss this iteration: 0.020244\n",
      "Encoder training step: 111/1000\n",
      "[DEBUG] Loss this iteration: 0.019321\n",
      "Encoder training step: 112/1000\n",
      "[DEBUG] Loss this iteration: 0.020339\n",
      "Encoder training step: 113/1000\n",
      "[DEBUG] Loss this iteration: 0.014796\n",
      "Encoder training step: 114/1000\n",
      "[DEBUG] Loss this iteration: 0.015964\n",
      "Encoder training step: 115/1000\n",
      "[DEBUG] Loss this iteration: 0.014627\n",
      "Encoder training step: 116/1000\n",
      "[DEBUG] Loss this iteration: 0.019317\n",
      "Encoder training step: 117/1000\n",
      "[DEBUG] Loss this iteration: 0.017264\n",
      "Encoder training step: 118/1000\n",
      "[DEBUG] Loss this iteration: 0.016115\n",
      "Encoder training step: 119/1000\n",
      "[DEBUG] Loss this iteration: 0.016923\n",
      "Encoder training step: 120/1000\n",
      "[DEBUG] Loss this iteration: 0.017894\n",
      "Encoder training step: 121/1000\n",
      "[DEBUG] Loss this iteration: 0.020759\n",
      "Encoder training step: 122/1000\n",
      "[DEBUG] Loss this iteration: 0.017595\n",
      "Encoder training step: 123/1000\n",
      "[DEBUG] Loss this iteration: 0.018202\n",
      "Encoder training step: 124/1000\n",
      "[DEBUG] Loss this iteration: 0.020304\n",
      "Encoder training step: 125/1000\n",
      "[DEBUG] Loss this iteration: 0.016715\n",
      "Encoder training step: 126/1000\n",
      "[DEBUG] Loss this iteration: 0.018832\n",
      "Encoder training step: 127/1000\n",
      "[DEBUG] Loss this iteration: 0.012963\n",
      "Encoder training step: 128/1000\n",
      "[DEBUG] Loss this iteration: 0.013534\n",
      "Encoder training step: 129/1000\n",
      "[DEBUG] Loss this iteration: 0.017399\n",
      "Encoder training step: 130/1000\n",
      "[DEBUG] Loss this iteration: 0.021607\n",
      "Encoder training step: 131/1000\n",
      "[DEBUG] Loss this iteration: 0.020148\n",
      "Encoder training step: 132/1000\n",
      "[DEBUG] Loss this iteration: 0.012463\n",
      "Encoder training step: 133/1000\n",
      "[DEBUG] Loss this iteration: 0.017064\n",
      "Encoder training step: 134/1000\n",
      "[DEBUG] Loss this iteration: 0.015844\n",
      "Encoder training step: 135/1000\n",
      "[DEBUG] Loss this iteration: 0.017144\n",
      "Encoder training step: 136/1000\n",
      "[DEBUG] Loss this iteration: 0.015495\n",
      "Encoder training step: 137/1000\n",
      "[DEBUG] Loss this iteration: 0.020390\n",
      "Encoder training step: 138/1000\n",
      "[DEBUG] Loss this iteration: 0.014877\n",
      "Encoder training step: 139/1000\n",
      "[DEBUG] Loss this iteration: 0.016252\n",
      "Encoder training step: 140/1000\n",
      "[DEBUG] Loss this iteration: 0.012724\n",
      "Encoder training step: 141/1000\n",
      "[DEBUG] Loss this iteration: 0.014625\n",
      "Encoder training step: 142/1000\n",
      "[DEBUG] Loss this iteration: 0.016597\n",
      "Encoder training step: 143/1000\n",
      "[DEBUG] Loss this iteration: 0.018264\n",
      "Encoder training step: 144/1000\n",
      "[DEBUG] Loss this iteration: 0.016702\n",
      "Encoder training step: 145/1000\n",
      "[DEBUG] Loss this iteration: 0.018040\n",
      "Encoder training step: 146/1000\n",
      "[DEBUG] Loss this iteration: 0.016077\n",
      "Encoder training step: 147/1000\n",
      "[DEBUG] Loss this iteration: 0.019845\n",
      "Encoder training step: 148/1000\n",
      "[DEBUG] Loss this iteration: 0.012266\n",
      "Encoder training step: 149/1000\n",
      "[DEBUG] Loss this iteration: 0.015081\n",
      "Encoder training step: 150/1000\n",
      "[DEBUG] Loss this iteration: 0.017552\n",
      "Encoder training step: 151/1000\n",
      "[DEBUG] Loss this iteration: 0.019748\n",
      "Encoder training step: 152/1000\n",
      "[DEBUG] Loss this iteration: 0.015883\n",
      "Encoder training step: 153/1000\n",
      "[DEBUG] Loss this iteration: 0.015324\n",
      "Encoder training step: 154/1000\n",
      "[DEBUG] Loss this iteration: 0.013362\n",
      "Encoder training step: 155/1000\n",
      "[DEBUG] Loss this iteration: 0.021509\n",
      "Encoder training step: 156/1000\n",
      "[DEBUG] Loss this iteration: 0.014853\n",
      "Encoder training step: 157/1000\n",
      "[DEBUG] Loss this iteration: 0.020474\n",
      "Encoder training step: 158/1000\n",
      "[DEBUG] Loss this iteration: 0.018052\n",
      "Encoder training step: 159/1000\n",
      "[DEBUG] Loss this iteration: 0.017743\n",
      "Encoder training step: 160/1000\n",
      "[DEBUG] Loss this iteration: 0.016978\n",
      "Encoder training step: 161/1000\n",
      "[DEBUG] Loss this iteration: 0.016392\n",
      "Encoder training step: 162/1000\n",
      "[DEBUG] Loss this iteration: 0.016521\n",
      "Encoder training step: 163/1000\n",
      "[DEBUG] Loss this iteration: 0.015352\n",
      "Encoder training step: 164/1000\n",
      "[DEBUG] Loss this iteration: 0.015709\n",
      "Encoder training step: 165/1000\n",
      "[DEBUG] Loss this iteration: 0.015290\n",
      "Encoder training step: 166/1000\n",
      "[DEBUG] Loss this iteration: 0.015487\n",
      "Encoder training step: 167/1000\n",
      "[DEBUG] Loss this iteration: 0.018638\n",
      "Encoder training step: 168/1000\n",
      "[DEBUG] Loss this iteration: 0.021183\n",
      "Encoder training step: 169/1000\n",
      "[DEBUG] Loss this iteration: 0.015453\n",
      "Encoder training step: 170/1000\n",
      "[DEBUG] Loss this iteration: 0.019929\n",
      "Encoder training step: 171/1000\n",
      "[DEBUG] Loss this iteration: 0.017560\n",
      "Encoder training step: 172/1000\n",
      "[DEBUG] Loss this iteration: 0.017372\n",
      "Encoder training step: 173/1000\n",
      "[DEBUG] Loss this iteration: 0.018799\n",
      "Encoder training step: 174/1000\n",
      "[DEBUG] Loss this iteration: 0.017221\n",
      "Encoder training step: 175/1000\n",
      "[DEBUG] Loss this iteration: 0.016979\n",
      "Encoder training step: 176/1000\n",
      "[DEBUG] Loss this iteration: 0.013358\n",
      "Encoder training step: 177/1000\n",
      "[DEBUG] Loss this iteration: 0.015542\n",
      "Encoder training step: 178/1000\n",
      "[DEBUG] Loss this iteration: 0.016809\n",
      "Encoder training step: 179/1000\n",
      "[DEBUG] Loss this iteration: 0.014665\n",
      "Encoder training step: 180/1000\n",
      "[DEBUG] Loss this iteration: 0.013755\n",
      "Encoder training step: 181/1000\n",
      "[DEBUG] Loss this iteration: 0.015838\n",
      "Encoder training step: 182/1000\n",
      "[DEBUG] Loss this iteration: 0.019023\n",
      "Encoder training step: 183/1000\n",
      "[DEBUG] Loss this iteration: 0.014912\n",
      "Encoder training step: 184/1000\n",
      "[DEBUG] Loss this iteration: 0.017930\n",
      "Encoder training step: 185/1000\n",
      "[DEBUG] Loss this iteration: 0.014770\n",
      "Encoder training step: 186/1000\n",
      "[DEBUG] Loss this iteration: 0.022382\n",
      "Encoder training step: 187/1000\n",
      "[DEBUG] Loss this iteration: 0.014505\n",
      "Encoder training step: 188/1000\n",
      "[DEBUG] Loss this iteration: 0.015776\n",
      "Encoder training step: 189/1000\n",
      "[DEBUG] Loss this iteration: 0.014678\n",
      "Encoder training step: 190/1000\n",
      "[DEBUG] Loss this iteration: 0.014751\n",
      "Encoder training step: 191/1000\n",
      "[DEBUG] Loss this iteration: 0.020746\n",
      "Encoder training step: 192/1000\n",
      "[DEBUG] Loss this iteration: 0.020140\n",
      "Encoder training step: 193/1000\n",
      "[DEBUG] Loss this iteration: 0.013529\n",
      "Encoder training step: 194/1000\n",
      "[DEBUG] Loss this iteration: 0.014654\n",
      "Encoder training step: 195/1000\n",
      "[DEBUG] Loss this iteration: 0.017603\n",
      "Encoder training step: 196/1000\n",
      "[DEBUG] Loss this iteration: 0.015293\n",
      "Encoder training step: 197/1000\n",
      "[DEBUG] Loss this iteration: 0.015230\n",
      "Encoder training step: 198/1000\n",
      "[DEBUG] Loss this iteration: 0.012878\n",
      "Encoder training step: 199/1000\n",
      "[DEBUG] Loss this iteration: 0.014867\n",
      "Encoder training step: 200/1000\n",
      "[DEBUG] Loss this iteration: 0.016329\n",
      "Encoder training step: 201/1000\n",
      "[DEBUG] Loss this iteration: 0.013582\n",
      "Encoder training step: 202/1000\n",
      "[DEBUG] Loss this iteration: 0.016230\n",
      "Encoder training step: 203/1000\n",
      "[DEBUG] Loss this iteration: 0.017436\n",
      "Encoder training step: 204/1000\n",
      "[DEBUG] Loss this iteration: 0.015205\n",
      "Encoder training step: 205/1000\n",
      "[DEBUG] Loss this iteration: 0.010783\n",
      "Encoder training step: 206/1000\n",
      "[DEBUG] Loss this iteration: 0.011936\n",
      "Encoder training step: 207/1000\n",
      "[DEBUG] Loss this iteration: 0.020441\n",
      "Encoder training step: 208/1000\n",
      "[DEBUG] Loss this iteration: 0.015727\n",
      "Encoder training step: 209/1000\n",
      "[DEBUG] Loss this iteration: 0.019843\n",
      "Encoder training step: 210/1000\n",
      "[DEBUG] Loss this iteration: 0.017601\n",
      "Encoder training step: 211/1000\n",
      "[DEBUG] Loss this iteration: 0.020654\n",
      "Encoder training step: 212/1000\n",
      "[DEBUG] Loss this iteration: 0.013892\n",
      "Encoder training step: 213/1000\n",
      "[DEBUG] Loss this iteration: 0.014410\n",
      "Encoder training step: 214/1000\n",
      "[DEBUG] Loss this iteration: 0.014255\n",
      "Encoder training step: 215/1000\n",
      "[DEBUG] Loss this iteration: 0.012647\n",
      "Encoder training step: 216/1000\n",
      "[DEBUG] Loss this iteration: 0.014848\n",
      "Encoder training step: 217/1000\n",
      "[DEBUG] Loss this iteration: 0.016729\n",
      "Encoder training step: 218/1000\n",
      "[DEBUG] Loss this iteration: 0.016688\n",
      "Encoder training step: 219/1000\n",
      "[DEBUG] Loss this iteration: 0.019745\n",
      "Encoder training step: 220/1000\n",
      "[DEBUG] Loss this iteration: 0.013625\n",
      "Encoder training step: 221/1000\n",
      "[DEBUG] Loss this iteration: 0.015384\n",
      "Encoder training step: 222/1000\n",
      "[DEBUG] Loss this iteration: 0.012528\n",
      "Encoder training step: 223/1000\n",
      "[DEBUG] Loss this iteration: 0.015672\n",
      "Encoder training step: 224/1000\n",
      "[DEBUG] Loss this iteration: 0.017113\n",
      "Encoder training step: 225/1000\n",
      "[DEBUG] Loss this iteration: 0.018888\n",
      "Encoder training step: 226/1000\n",
      "[DEBUG] Loss this iteration: 0.013571\n",
      "Encoder training step: 227/1000\n",
      "[DEBUG] Loss this iteration: 0.017053\n",
      "Encoder training step: 228/1000\n",
      "[DEBUG] Loss this iteration: 0.016326\n",
      "Encoder training step: 229/1000\n",
      "[DEBUG] Loss this iteration: 0.010808\n",
      "Encoder training step: 230/1000\n",
      "[DEBUG] Loss this iteration: 0.012915\n",
      "Encoder training step: 231/1000\n",
      "[DEBUG] Loss this iteration: 0.017850\n",
      "Encoder training step: 232/1000\n",
      "[DEBUG] Loss this iteration: 0.015414\n",
      "Encoder training step: 233/1000\n",
      "[DEBUG] Loss this iteration: 0.012381\n",
      "Encoder training step: 234/1000\n",
      "[DEBUG] Loss this iteration: 0.016320\n",
      "Encoder training step: 235/1000\n",
      "[DEBUG] Loss this iteration: 0.013593\n",
      "Encoder training step: 236/1000\n",
      "[DEBUG] Loss this iteration: 0.014786\n",
      "Encoder training step: 237/1000\n",
      "[DEBUG] Loss this iteration: 0.014434\n",
      "Encoder training step: 238/1000\n",
      "[DEBUG] Loss this iteration: 0.016884\n",
      "Encoder training step: 239/1000\n",
      "[DEBUG] Loss this iteration: 0.015964\n",
      "Encoder training step: 240/1000\n",
      "[DEBUG] Loss this iteration: 0.012377\n",
      "Encoder training step: 241/1000\n",
      "[DEBUG] Loss this iteration: 0.012646\n",
      "Encoder training step: 242/1000\n",
      "[DEBUG] Loss this iteration: 0.010850\n",
      "Encoder training step: 243/1000\n",
      "[DEBUG] Loss this iteration: 0.016735\n",
      "Encoder training step: 244/1000\n",
      "[DEBUG] Loss this iteration: 0.014846\n",
      "Encoder training step: 245/1000\n",
      "[DEBUG] Loss this iteration: 0.013899\n",
      "Encoder training step: 246/1000\n",
      "[DEBUG] Loss this iteration: 0.012540\n",
      "Encoder training step: 247/1000\n",
      "[DEBUG] Loss this iteration: 0.011800\n",
      "Encoder training step: 248/1000\n",
      "[DEBUG] Loss this iteration: 0.012464\n",
      "Encoder training step: 249/1000\n",
      "[DEBUG] Loss this iteration: 0.011633\n",
      "Encoder training step: 250/1000\n",
      "[DEBUG] Loss this iteration: 0.012158\n",
      "Encoder training step: 251/1000\n",
      "[DEBUG] Loss this iteration: 0.012984\n",
      "Encoder training step: 252/1000\n",
      "[DEBUG] Loss this iteration: 0.011345\n",
      "Encoder training step: 253/1000\n",
      "[DEBUG] Loss this iteration: 0.011316\n",
      "Encoder training step: 254/1000\n",
      "[DEBUG] Loss this iteration: 0.009726\n",
      "Encoder training step: 255/1000\n",
      "[DEBUG] Loss this iteration: 0.011470\n",
      "Encoder training step: 256/1000\n",
      "[DEBUG] Loss this iteration: 0.010532\n",
      "Encoder training step: 257/1000\n",
      "[DEBUG] Loss this iteration: 0.009288\n",
      "Encoder training step: 258/1000\n",
      "[DEBUG] Loss this iteration: 0.014515\n",
      "Encoder training step: 259/1000\n",
      "[DEBUG] Loss this iteration: 0.009266\n",
      "Encoder training step: 260/1000\n",
      "[DEBUG] Loss this iteration: 0.008909\n",
      "Encoder training step: 261/1000\n",
      "[DEBUG] Loss this iteration: 0.008929\n",
      "Encoder training step: 262/1000\n",
      "[DEBUG] Loss this iteration: 0.008730\n",
      "Encoder training step: 263/1000\n",
      "[DEBUG] Loss this iteration: 0.008981\n",
      "Encoder training step: 264/1000\n",
      "[DEBUG] Loss this iteration: 0.009027\n",
      "Encoder training step: 265/1000\n",
      "[DEBUG] Loss this iteration: 0.006877\n",
      "Encoder training step: 266/1000\n",
      "[DEBUG] Loss this iteration: 0.006484\n",
      "Encoder training step: 267/1000\n",
      "[DEBUG] Loss this iteration: 0.007002\n",
      "Encoder training step: 268/1000\n",
      "[DEBUG] Loss this iteration: 0.006103\n",
      "Encoder training step: 269/1000\n",
      "[DEBUG] Loss this iteration: 0.005562\n",
      "Encoder training step: 270/1000\n",
      "[DEBUG] Loss this iteration: 0.006675\n",
      "Encoder training step: 271/1000\n",
      "[DEBUG] Loss this iteration: 0.008356\n",
      "Encoder training step: 272/1000\n",
      "[DEBUG] Loss this iteration: 0.005891\n",
      "Encoder training step: 273/1000\n",
      "[DEBUG] Loss this iteration: 0.006376\n",
      "Encoder training step: 274/1000\n",
      "[DEBUG] Loss this iteration: 0.005714\n",
      "Encoder training step: 275/1000\n",
      "[DEBUG] Loss this iteration: 0.004892\n",
      "Encoder training step: 276/1000\n",
      "[DEBUG] Loss this iteration: 0.004275\n",
      "Encoder training step: 277/1000\n",
      "[DEBUG] Loss this iteration: 0.005157\n",
      "Encoder training step: 278/1000\n",
      "[DEBUG] Loss this iteration: 0.005271\n",
      "Encoder training step: 279/1000\n",
      "[DEBUG] Loss this iteration: 0.004761\n",
      "Encoder training step: 280/1000\n",
      "[DEBUG] Loss this iteration: 0.005147\n",
      "Encoder training step: 281/1000\n",
      "[DEBUG] Loss this iteration: 0.004953\n",
      "Encoder training step: 282/1000\n",
      "[DEBUG] Loss this iteration: 0.004874\n",
      "Encoder training step: 283/1000\n",
      "[DEBUG] Loss this iteration: 0.004050\n",
      "Encoder training step: 284/1000\n",
      "[DEBUG] Loss this iteration: 0.004850\n",
      "Encoder training step: 285/1000\n",
      "[DEBUG] Loss this iteration: 0.003942\n",
      "Encoder training step: 286/1000\n",
      "[DEBUG] Loss this iteration: 0.004047\n",
      "Encoder training step: 287/1000\n",
      "[DEBUG] Loss this iteration: 0.004862\n",
      "Encoder training step: 288/1000\n",
      "[DEBUG] Loss this iteration: 0.004378\n",
      "Encoder training step: 289/1000\n",
      "[DEBUG] Loss this iteration: 0.004479\n",
      "Encoder training step: 290/1000\n",
      "[DEBUG] Loss this iteration: 0.004485\n",
      "Encoder training step: 291/1000\n",
      "[DEBUG] Loss this iteration: 0.004421\n",
      "Encoder training step: 292/1000\n",
      "[DEBUG] Loss this iteration: 0.004201\n",
      "Encoder training step: 293/1000\n",
      "[DEBUG] Loss this iteration: 0.004488\n",
      "Encoder training step: 294/1000\n",
      "[DEBUG] Loss this iteration: 0.003856\n",
      "Encoder training step: 295/1000\n",
      "[DEBUG] Loss this iteration: 0.003439\n",
      "Encoder training step: 296/1000\n",
      "[DEBUG] Loss this iteration: 0.003588\n",
      "Encoder training step: 297/1000\n",
      "[DEBUG] Loss this iteration: 0.003827\n",
      "Encoder training step: 298/1000\n",
      "[DEBUG] Loss this iteration: 0.003600\n",
      "Encoder training step: 299/1000\n",
      "[DEBUG] Loss this iteration: 0.003995\n",
      "Encoder training step: 300/1000\n",
      "[DEBUG] Loss this iteration: 0.003813\n",
      "Encoder training step: 301/1000\n",
      "[DEBUG] Loss this iteration: 0.003587\n",
      "Encoder training step: 302/1000\n",
      "[DEBUG] Loss this iteration: 0.004688\n",
      "Encoder training step: 303/1000\n",
      "[DEBUG] Loss this iteration: 0.003868\n",
      "Encoder training step: 304/1000\n",
      "[DEBUG] Loss this iteration: 0.004423\n",
      "Encoder training step: 305/1000\n",
      "[DEBUG] Loss this iteration: 0.003205\n",
      "Encoder training step: 306/1000\n",
      "[DEBUG] Loss this iteration: 0.003699\n",
      "Encoder training step: 307/1000\n",
      "[DEBUG] Loss this iteration: 0.003595\n",
      "Encoder training step: 308/1000\n",
      "[DEBUG] Loss this iteration: 0.003987\n",
      "Encoder training step: 309/1000\n",
      "[DEBUG] Loss this iteration: 0.003469\n",
      "Encoder training step: 310/1000\n",
      "[DEBUG] Loss this iteration: 0.003108\n",
      "Encoder training step: 311/1000\n",
      "[DEBUG] Loss this iteration: 0.003311\n",
      "Encoder training step: 312/1000\n",
      "[DEBUG] Loss this iteration: 0.003625\n",
      "Encoder training step: 313/1000\n",
      "[DEBUG] Loss this iteration: 0.003677\n",
      "Encoder training step: 314/1000\n",
      "[DEBUG] Loss this iteration: 0.003714\n",
      "Encoder training step: 315/1000\n",
      "[DEBUG] Loss this iteration: 0.003603\n",
      "Encoder training step: 316/1000\n",
      "[DEBUG] Loss this iteration: 0.003309\n",
      "Encoder training step: 317/1000\n",
      "[DEBUG] Loss this iteration: 0.004535\n",
      "Encoder training step: 318/1000\n",
      "[DEBUG] Loss this iteration: 0.003565\n",
      "Encoder training step: 319/1000\n",
      "[DEBUG] Loss this iteration: 0.003306\n",
      "Encoder training step: 320/1000\n",
      "[DEBUG] Loss this iteration: 0.003575\n",
      "Encoder training step: 321/1000\n",
      "[DEBUG] Loss this iteration: 0.003453\n",
      "Encoder training step: 322/1000\n",
      "[DEBUG] Loss this iteration: 0.003445\n",
      "Encoder training step: 323/1000\n",
      "[DEBUG] Loss this iteration: 0.003910\n",
      "Encoder training step: 324/1000\n",
      "[DEBUG] Loss this iteration: 0.003447\n",
      "Encoder training step: 325/1000\n",
      "[DEBUG] Loss this iteration: 0.003645\n",
      "Encoder training step: 326/1000\n",
      "[DEBUG] Loss this iteration: 0.004010\n",
      "Encoder training step: 327/1000\n",
      "[DEBUG] Loss this iteration: 0.003678\n",
      "Encoder training step: 328/1000\n",
      "[DEBUG] Loss this iteration: 0.003821\n",
      "Encoder training step: 329/1000\n",
      "[DEBUG] Loss this iteration: 0.003586\n",
      "Encoder training step: 330/1000\n",
      "[DEBUG] Loss this iteration: 0.003917\n",
      "Encoder training step: 331/1000\n",
      "[DEBUG] Loss this iteration: 0.004112\n",
      "Encoder training step: 332/1000\n",
      "[DEBUG] Loss this iteration: 0.003985\n",
      "Encoder training step: 333/1000\n",
      "[DEBUG] Loss this iteration: 0.003870\n",
      "Encoder training step: 334/1000\n",
      "[DEBUG] Loss this iteration: 0.003097\n",
      "Encoder training step: 335/1000\n",
      "[DEBUG] Loss this iteration: 0.003741\n",
      "Encoder training step: 336/1000\n",
      "[DEBUG] Loss this iteration: 0.003615\n",
      "Encoder training step: 337/1000\n",
      "[DEBUG] Loss this iteration: 0.003375\n",
      "Encoder training step: 338/1000\n",
      "[DEBUG] Loss this iteration: 0.004006\n",
      "Encoder training step: 339/1000\n",
      "[DEBUG] Loss this iteration: 0.003456\n",
      "Encoder training step: 340/1000\n",
      "[DEBUG] Loss this iteration: 0.003775\n",
      "Encoder training step: 341/1000\n",
      "[DEBUG] Loss this iteration: 0.003338\n",
      "Encoder training step: 342/1000\n",
      "[DEBUG] Loss this iteration: 0.003327\n",
      "Encoder training step: 343/1000\n",
      "[DEBUG] Loss this iteration: 0.003689\n",
      "Encoder training step: 344/1000\n",
      "[DEBUG] Loss this iteration: 0.003863\n",
      "Encoder training step: 345/1000\n",
      "[DEBUG] Loss this iteration: 0.003622\n",
      "Encoder training step: 346/1000\n",
      "[DEBUG] Loss this iteration: 0.003850\n",
      "Encoder training step: 347/1000\n",
      "[DEBUG] Loss this iteration: 0.003715\n",
      "Encoder training step: 348/1000\n",
      "[DEBUG] Loss this iteration: 0.004124\n",
      "Encoder training step: 349/1000\n",
      "[DEBUG] Loss this iteration: 0.003072\n",
      "Encoder training step: 350/1000\n",
      "[DEBUG] Loss this iteration: 0.003117\n",
      "Encoder training step: 351/1000\n",
      "[DEBUG] Loss this iteration: 0.003018\n",
      "Encoder training step: 352/1000\n",
      "[DEBUG] Loss this iteration: 0.003546\n",
      "Encoder training step: 353/1000\n",
      "[DEBUG] Loss this iteration: 0.003204\n",
      "Encoder training step: 354/1000\n",
      "[DEBUG] Loss this iteration: 0.003239\n",
      "Encoder training step: 355/1000\n",
      "[DEBUG] Loss this iteration: 0.003184\n",
      "Encoder training step: 356/1000\n",
      "[DEBUG] Loss this iteration: 0.003700\n",
      "Encoder training step: 357/1000\n",
      "[DEBUG] Loss this iteration: 0.003197\n",
      "Encoder training step: 358/1000\n",
      "[DEBUG] Loss this iteration: 0.003314\n",
      "Encoder training step: 359/1000\n",
      "[DEBUG] Loss this iteration: 0.003365\n",
      "Encoder training step: 360/1000\n",
      "[DEBUG] Loss this iteration: 0.003969\n",
      "Encoder training step: 361/1000\n",
      "[DEBUG] Loss this iteration: 0.003694\n",
      "Encoder training step: 362/1000\n",
      "[DEBUG] Loss this iteration: 0.003111\n",
      "Encoder training step: 363/1000\n",
      "[DEBUG] Loss this iteration: 0.003201\n",
      "Encoder training step: 364/1000\n",
      "[DEBUG] Loss this iteration: 0.003661\n",
      "Encoder training step: 365/1000\n",
      "[DEBUG] Loss this iteration: 0.003512\n",
      "Encoder training step: 366/1000\n",
      "[DEBUG] Loss this iteration: 0.003113\n",
      "Encoder training step: 367/1000\n",
      "[DEBUG] Loss this iteration: 0.003115\n",
      "Encoder training step: 368/1000\n",
      "[DEBUG] Loss this iteration: 0.003737\n",
      "Encoder training step: 369/1000\n",
      "[DEBUG] Loss this iteration: 0.002987\n",
      "Encoder training step: 370/1000\n",
      "[DEBUG] Loss this iteration: 0.003205\n",
      "Encoder training step: 371/1000\n",
      "[DEBUG] Loss this iteration: 0.003513\n",
      "Encoder training step: 372/1000\n",
      "[DEBUG] Loss this iteration: 0.003540\n",
      "Encoder training step: 373/1000\n",
      "[DEBUG] Loss this iteration: 0.003747\n",
      "Encoder training step: 374/1000\n",
      "[DEBUG] Loss this iteration: 0.003433\n",
      "Encoder training step: 375/1000\n",
      "[DEBUG] Loss this iteration: 0.002997\n",
      "Encoder training step: 376/1000\n",
      "[DEBUG] Loss this iteration: 0.003122\n",
      "Encoder training step: 377/1000\n",
      "[DEBUG] Loss this iteration: 0.003428\n",
      "Encoder training step: 378/1000\n",
      "[DEBUG] Loss this iteration: 0.003444\n",
      "Encoder training step: 379/1000\n",
      "[DEBUG] Loss this iteration: 0.002934\n",
      "Encoder training step: 380/1000\n",
      "[DEBUG] Loss this iteration: 0.003279\n",
      "Encoder training step: 381/1000\n",
      "[DEBUG] Loss this iteration: 0.003038\n",
      "Encoder training step: 382/1000\n",
      "[DEBUG] Loss this iteration: 0.002923\n",
      "Encoder training step: 383/1000\n",
      "[DEBUG] Loss this iteration: 0.003496\n",
      "Encoder training step: 384/1000\n",
      "[DEBUG] Loss this iteration: 0.003550\n",
      "Encoder training step: 385/1000\n",
      "[DEBUG] Loss this iteration: 0.003342\n",
      "Encoder training step: 386/1000\n",
      "[DEBUG] Loss this iteration: 0.003697\n",
      "Encoder training step: 387/1000\n",
      "[DEBUG] Loss this iteration: 0.003806\n",
      "Encoder training step: 388/1000\n",
      "[DEBUG] Loss this iteration: 0.003380\n",
      "Encoder training step: 389/1000\n",
      "[DEBUG] Loss this iteration: 0.003101\n",
      "Encoder training step: 390/1000\n",
      "[DEBUG] Loss this iteration: 0.003307\n",
      "Encoder training step: 391/1000\n",
      "[DEBUG] Loss this iteration: 0.003424\n",
      "Encoder training step: 392/1000\n",
      "[DEBUG] Loss this iteration: 0.003716\n",
      "Encoder training step: 393/1000\n",
      "[DEBUG] Loss this iteration: 0.003410\n",
      "Encoder training step: 394/1000\n",
      "[DEBUG] Loss this iteration: 0.003128\n",
      "Encoder training step: 395/1000\n",
      "[DEBUG] Loss this iteration: 0.003522\n",
      "Encoder training step: 396/1000\n",
      "[DEBUG] Loss this iteration: 0.003159\n",
      "Encoder training step: 397/1000\n",
      "[DEBUG] Loss this iteration: 0.003130\n",
      "Encoder training step: 398/1000\n",
      "[DEBUG] Loss this iteration: 0.003950\n",
      "Encoder training step: 399/1000\n",
      "[DEBUG] Loss this iteration: 0.003126\n",
      "Encoder training step: 400/1000\n",
      "[DEBUG] Loss this iteration: 0.003501\n",
      "Encoder training step: 401/1000\n",
      "[DEBUG] Loss this iteration: 0.003172\n",
      "Encoder training step: 402/1000\n",
      "[DEBUG] Loss this iteration: 0.002941\n",
      "Encoder training step: 403/1000\n",
      "[DEBUG] Loss this iteration: 0.003222\n",
      "Encoder training step: 404/1000\n",
      "[DEBUG] Loss this iteration: 0.003054\n",
      "Encoder training step: 405/1000\n",
      "[DEBUG] Loss this iteration: 0.003013\n",
      "Encoder training step: 406/1000\n",
      "[DEBUG] Loss this iteration: 0.003341\n",
      "Encoder training step: 407/1000\n",
      "[DEBUG] Loss this iteration: 0.003667\n",
      "Encoder training step: 408/1000\n",
      "[DEBUG] Loss this iteration: 0.003383\n",
      "Encoder training step: 409/1000\n",
      "[DEBUG] Loss this iteration: 0.003155\n",
      "Encoder training step: 410/1000\n",
      "[DEBUG] Loss this iteration: 0.003325\n",
      "Encoder training step: 411/1000\n",
      "[DEBUG] Loss this iteration: 0.003217\n",
      "Encoder training step: 412/1000\n",
      "[DEBUG] Loss this iteration: 0.003452\n",
      "Encoder training step: 413/1000\n",
      "[DEBUG] Loss this iteration: 0.003709\n",
      "Encoder training step: 414/1000\n",
      "[DEBUG] Loss this iteration: 0.003369\n",
      "Encoder training step: 415/1000\n",
      "[DEBUG] Loss this iteration: 0.003751\n",
      "Encoder training step: 416/1000\n",
      "[DEBUG] Loss this iteration: 0.003189\n",
      "Encoder training step: 417/1000\n",
      "[DEBUG] Loss this iteration: 0.003686\n",
      "Encoder training step: 418/1000\n",
      "[DEBUG] Loss this iteration: 0.003196\n",
      "Encoder training step: 419/1000\n",
      "[DEBUG] Loss this iteration: 0.003123\n",
      "Encoder training step: 420/1000\n",
      "[DEBUG] Loss this iteration: 0.003559\n",
      "Encoder training step: 421/1000\n",
      "[DEBUG] Loss this iteration: 0.003326\n",
      "Encoder training step: 422/1000\n",
      "[DEBUG] Loss this iteration: 0.003038\n",
      "Encoder training step: 423/1000\n",
      "[DEBUG] Loss this iteration: 0.003436\n",
      "Encoder training step: 424/1000\n",
      "[DEBUG] Loss this iteration: 0.003103\n",
      "Encoder training step: 425/1000\n",
      "[DEBUG] Loss this iteration: 0.003128\n",
      "Encoder training step: 426/1000\n",
      "[DEBUG] Loss this iteration: 0.003023\n",
      "Encoder training step: 427/1000\n",
      "[DEBUG] Loss this iteration: 0.003224\n",
      "Encoder training step: 428/1000\n",
      "[DEBUG] Loss this iteration: 0.003312\n",
      "Encoder training step: 429/1000\n",
      "[DEBUG] Loss this iteration: 0.003243\n",
      "Encoder training step: 430/1000\n",
      "[DEBUG] Loss this iteration: 0.003234\n",
      "Encoder training step: 431/1000\n",
      "[DEBUG] Loss this iteration: 0.002820\n",
      "Encoder training step: 432/1000\n",
      "[DEBUG] Loss this iteration: 0.003401\n",
      "Encoder training step: 433/1000\n",
      "[DEBUG] Loss this iteration: 0.003150\n",
      "Encoder training step: 434/1000\n",
      "[DEBUG] Loss this iteration: 0.002987\n",
      "Encoder training step: 435/1000\n",
      "[DEBUG] Loss this iteration: 0.003161\n",
      "Encoder training step: 436/1000\n",
      "[DEBUG] Loss this iteration: 0.003167\n",
      "Encoder training step: 437/1000\n",
      "[DEBUG] Loss this iteration: 0.003579\n",
      "Encoder training step: 438/1000\n",
      "[DEBUG] Loss this iteration: 0.003046\n",
      "Encoder training step: 439/1000\n",
      "[DEBUG] Loss this iteration: 0.003701\n",
      "Encoder training step: 440/1000\n",
      "[DEBUG] Loss this iteration: 0.003787\n",
      "Encoder training step: 441/1000\n",
      "[DEBUG] Loss this iteration: 0.003464\n",
      "Encoder training step: 442/1000\n",
      "[DEBUG] Loss this iteration: 0.003289\n",
      "Encoder training step: 443/1000\n",
      "[DEBUG] Loss this iteration: 0.003225\n",
      "Encoder training step: 444/1000\n",
      "[DEBUG] Loss this iteration: 0.003911\n",
      "Encoder training step: 445/1000\n",
      "[DEBUG] Loss this iteration: 0.003782\n",
      "Encoder training step: 446/1000\n",
      "[DEBUG] Loss this iteration: 0.004011\n",
      "Encoder training step: 447/1000\n",
      "[DEBUG] Loss this iteration: 0.003012\n",
      "Encoder training step: 448/1000\n",
      "[DEBUG] Loss this iteration: 0.003715\n",
      "Encoder training step: 449/1000\n",
      "[DEBUG] Loss this iteration: 0.003810\n",
      "Encoder training step: 450/1000\n",
      "[DEBUG] Loss this iteration: 0.003797\n",
      "Encoder training step: 451/1000\n",
      "[DEBUG] Loss this iteration: 0.003010\n",
      "Encoder training step: 452/1000\n",
      "[DEBUG] Loss this iteration: 0.003394\n",
      "Encoder training step: 453/1000\n",
      "[DEBUG] Loss this iteration: 0.003140\n",
      "Encoder training step: 454/1000\n",
      "[DEBUG] Loss this iteration: 0.002980\n",
      "Encoder training step: 455/1000\n",
      "[DEBUG] Loss this iteration: 0.003147\n",
      "Encoder training step: 456/1000\n",
      "[DEBUG] Loss this iteration: 0.002893\n",
      "Encoder training step: 457/1000\n",
      "[DEBUG] Loss this iteration: 0.003453\n",
      "Encoder training step: 458/1000\n",
      "[DEBUG] Loss this iteration: 0.003698\n",
      "Encoder training step: 459/1000\n",
      "[DEBUG] Loss this iteration: 0.003307\n",
      "Encoder training step: 460/1000\n",
      "[DEBUG] Loss this iteration: 0.003272\n",
      "Encoder training step: 461/1000\n",
      "[DEBUG] Loss this iteration: 0.003382\n",
      "Encoder training step: 462/1000\n",
      "[DEBUG] Loss this iteration: 0.003228\n",
      "Encoder training step: 463/1000\n",
      "[DEBUG] Loss this iteration: 0.003156\n",
      "Encoder training step: 464/1000\n",
      "[DEBUG] Loss this iteration: 0.002864\n",
      "Encoder training step: 465/1000\n",
      "[DEBUG] Loss this iteration: 0.002862\n",
      "Encoder training step: 466/1000\n",
      "[DEBUG] Loss this iteration: 0.003305\n",
      "Encoder training step: 467/1000\n",
      "[DEBUG] Loss this iteration: 0.002770\n",
      "Encoder training step: 468/1000\n",
      "[DEBUG] Loss this iteration: 0.003393\n",
      "Encoder training step: 469/1000\n",
      "[DEBUG] Loss this iteration: 0.003245\n",
      "Encoder training step: 470/1000\n",
      "[DEBUG] Loss this iteration: 0.003291\n",
      "Encoder training step: 471/1000\n",
      "[DEBUG] Loss this iteration: 0.003042\n",
      "Encoder training step: 472/1000\n",
      "[DEBUG] Loss this iteration: 0.003148\n",
      "Encoder training step: 473/1000\n",
      "[DEBUG] Loss this iteration: 0.003450\n",
      "Encoder training step: 474/1000\n",
      "[DEBUG] Loss this iteration: 0.003559\n",
      "Encoder training step: 475/1000\n",
      "[DEBUG] Loss this iteration: 0.003525\n",
      "Encoder training step: 476/1000\n",
      "[DEBUG] Loss this iteration: 0.003402\n",
      "Encoder training step: 477/1000\n",
      "[DEBUG] Loss this iteration: 0.003317\n",
      "Encoder training step: 478/1000\n",
      "[DEBUG] Loss this iteration: 0.003410\n",
      "Encoder training step: 479/1000\n",
      "[DEBUG] Loss this iteration: 0.003510\n",
      "Encoder training step: 480/1000\n",
      "[DEBUG] Loss this iteration: 0.003468\n",
      "Encoder training step: 481/1000\n",
      "[DEBUG] Loss this iteration: 0.004006\n",
      "Encoder training step: 482/1000\n",
      "[DEBUG] Loss this iteration: 0.003774\n",
      "Encoder training step: 483/1000\n",
      "[DEBUG] Loss this iteration: 0.003762\n",
      "Encoder training step: 484/1000\n",
      "[DEBUG] Loss this iteration: 0.003236\n",
      "Encoder training step: 485/1000\n",
      "[DEBUG] Loss this iteration: 0.003090\n",
      "Encoder training step: 486/1000\n",
      "[DEBUG] Loss this iteration: 0.002979\n",
      "Encoder training step: 487/1000\n",
      "[DEBUG] Loss this iteration: 0.003334\n",
      "Encoder training step: 488/1000\n",
      "[DEBUG] Loss this iteration: 0.003200\n",
      "Encoder training step: 489/1000\n",
      "[DEBUG] Loss this iteration: 0.003142\n",
      "Encoder training step: 490/1000\n",
      "[DEBUG] Loss this iteration: 0.003105\n",
      "Encoder training step: 491/1000\n",
      "[DEBUG] Loss this iteration: 0.002939\n",
      "Encoder training step: 492/1000\n",
      "[DEBUG] Loss this iteration: 0.003528\n",
      "Encoder training step: 493/1000\n",
      "[DEBUG] Loss this iteration: 0.002869\n",
      "Encoder training step: 494/1000\n",
      "[DEBUG] Loss this iteration: 0.003072\n",
      "Encoder training step: 495/1000\n",
      "[DEBUG] Loss this iteration: 0.002976\n",
      "Encoder training step: 496/1000\n",
      "[DEBUG] Loss this iteration: 0.003382\n",
      "Encoder training step: 497/1000\n",
      "[DEBUG] Loss this iteration: 0.002857\n",
      "Encoder training step: 498/1000\n",
      "[DEBUG] Loss this iteration: 0.003206\n",
      "Encoder training step: 499/1000\n",
      "[DEBUG] Loss this iteration: 0.003275\n",
      "Encoder training step: 500/1000\n",
      "[DEBUG] Loss this iteration: 0.002907\n",
      "Encoder training step: 501/1000\n",
      "[DEBUG] Loss this iteration: 0.003034\n",
      "Encoder training step: 502/1000\n",
      "[DEBUG] Loss this iteration: 0.003294\n",
      "Encoder training step: 503/1000\n",
      "[DEBUG] Loss this iteration: 0.003389\n",
      "Encoder training step: 504/1000\n",
      "[DEBUG] Loss this iteration: 0.002994\n",
      "Encoder training step: 505/1000\n",
      "[DEBUG] Loss this iteration: 0.002720\n",
      "Encoder training step: 506/1000\n",
      "[DEBUG] Loss this iteration: 0.003252\n",
      "Encoder training step: 507/1000\n",
      "[DEBUG] Loss this iteration: 0.003183\n",
      "Encoder training step: 508/1000\n",
      "[DEBUG] Loss this iteration: 0.002933\n",
      "Encoder training step: 509/1000\n",
      "[DEBUG] Loss this iteration: 0.003048\n",
      "Encoder training step: 510/1000\n",
      "[DEBUG] Loss this iteration: 0.003174\n",
      "Encoder training step: 511/1000\n",
      "[DEBUG] Loss this iteration: 0.003294\n",
      "Encoder training step: 512/1000\n",
      "[DEBUG] Loss this iteration: 0.003011\n",
      "Encoder training step: 513/1000\n",
      "[DEBUG] Loss this iteration: 0.003104\n",
      "Encoder training step: 514/1000\n",
      "[DEBUG] Loss this iteration: 0.003176\n",
      "Encoder training step: 515/1000\n",
      "[DEBUG] Loss this iteration: 0.003493\n",
      "Encoder training step: 516/1000\n",
      "[DEBUG] Loss this iteration: 0.003196\n",
      "Encoder training step: 517/1000\n",
      "[DEBUG] Loss this iteration: 0.003240\n",
      "Encoder training step: 518/1000\n",
      "[DEBUG] Loss this iteration: 0.003143\n",
      "Encoder training step: 519/1000\n",
      "[DEBUG] Loss this iteration: 0.003122\n",
      "Encoder training step: 520/1000\n",
      "[DEBUG] Loss this iteration: 0.003144\n",
      "Encoder training step: 521/1000\n",
      "[DEBUG] Loss this iteration: 0.003412\n",
      "Encoder training step: 522/1000\n",
      "[DEBUG] Loss this iteration: 0.003082\n",
      "Encoder training step: 523/1000\n",
      "[DEBUG] Loss this iteration: 0.003274\n",
      "Encoder training step: 524/1000\n",
      "[DEBUG] Loss this iteration: 0.003242\n",
      "Encoder training step: 525/1000\n",
      "[DEBUG] Loss this iteration: 0.003232\n",
      "Encoder training step: 526/1000\n",
      "[DEBUG] Loss this iteration: 0.003404\n",
      "Encoder training step: 527/1000\n",
      "[DEBUG] Loss this iteration: 0.002822\n",
      "Encoder training step: 528/1000\n",
      "[DEBUG] Loss this iteration: 0.003224\n",
      "Encoder training step: 529/1000\n",
      "[DEBUG] Loss this iteration: 0.003004\n",
      "Encoder training step: 530/1000\n",
      "[DEBUG] Loss this iteration: 0.003041\n",
      "Encoder training step: 531/1000\n",
      "[DEBUG] Loss this iteration: 0.003064\n",
      "Encoder training step: 532/1000\n",
      "[DEBUG] Loss this iteration: 0.002956\n",
      "Encoder training step: 533/1000\n",
      "[DEBUG] Loss this iteration: 0.003074\n",
      "Encoder training step: 534/1000\n",
      "[DEBUG] Loss this iteration: 0.003212\n",
      "Encoder training step: 535/1000\n",
      "[DEBUG] Loss this iteration: 0.003704\n",
      "Encoder training step: 536/1000\n",
      "[DEBUG] Loss this iteration: 0.003502\n",
      "Encoder training step: 537/1000\n",
      "[DEBUG] Loss this iteration: 0.003271\n",
      "Encoder training step: 538/1000\n",
      "[DEBUG] Loss this iteration: 0.003281\n",
      "Encoder training step: 539/1000\n",
      "[DEBUG] Loss this iteration: 0.003171\n",
      "Encoder training step: 540/1000\n",
      "[DEBUG] Loss this iteration: 0.003206\n",
      "Encoder training step: 541/1000\n",
      "[DEBUG] Loss this iteration: 0.003010\n",
      "Encoder training step: 542/1000\n",
      "[DEBUG] Loss this iteration: 0.003003\n",
      "Encoder training step: 543/1000\n",
      "[DEBUG] Loss this iteration: 0.003248\n",
      "Encoder training step: 544/1000\n",
      "[DEBUG] Loss this iteration: 0.002989\n",
      "Encoder training step: 545/1000\n",
      "[DEBUG] Loss this iteration: 0.002957\n",
      "Encoder training step: 546/1000\n",
      "[DEBUG] Loss this iteration: 0.003090\n",
      "Encoder training step: 547/1000\n",
      "[DEBUG] Loss this iteration: 0.003116\n",
      "Encoder training step: 548/1000\n",
      "[DEBUG] Loss this iteration: 0.002739\n",
      "Encoder training step: 549/1000\n",
      "[DEBUG] Loss this iteration: 0.002861\n",
      "Encoder training step: 550/1000\n",
      "[DEBUG] Loss this iteration: 0.003011\n",
      "Encoder training step: 551/1000\n",
      "[DEBUG] Loss this iteration: 0.003086\n",
      "Encoder training step: 552/1000\n",
      "[DEBUG] Loss this iteration: 0.002857\n",
      "Encoder training step: 553/1000\n",
      "[DEBUG] Loss this iteration: 0.003033\n",
      "Encoder training step: 554/1000\n",
      "[DEBUG] Loss this iteration: 0.003065\n",
      "Encoder training step: 555/1000\n",
      "[DEBUG] Loss this iteration: 0.002971\n",
      "Encoder training step: 556/1000\n",
      "[DEBUG] Loss this iteration: 0.003574\n",
      "Encoder training step: 557/1000\n",
      "[DEBUG] Loss this iteration: 0.002955\n",
      "Encoder training step: 558/1000\n",
      "[DEBUG] Loss this iteration: 0.003055\n",
      "Encoder training step: 559/1000\n",
      "[DEBUG] Loss this iteration: 0.002747\n",
      "Encoder training step: 560/1000\n",
      "[DEBUG] Loss this iteration: 0.003038\n",
      "Encoder training step: 561/1000\n",
      "[DEBUG] Loss this iteration: 0.003187\n",
      "Encoder training step: 562/1000\n",
      "[DEBUG] Loss this iteration: 0.003010\n",
      "Encoder training step: 563/1000\n",
      "[DEBUG] Loss this iteration: 0.002811\n",
      "Encoder training step: 564/1000\n",
      "[DEBUG] Loss this iteration: 0.002998\n",
      "Encoder training step: 565/1000\n",
      "[DEBUG] Loss this iteration: 0.002839\n",
      "Encoder training step: 566/1000\n",
      "[DEBUG] Loss this iteration: 0.003001\n",
      "Encoder training step: 567/1000\n",
      "[DEBUG] Loss this iteration: 0.003271\n",
      "Encoder training step: 568/1000\n",
      "[DEBUG] Loss this iteration: 0.003158\n",
      "Encoder training step: 569/1000\n",
      "[DEBUG] Loss this iteration: 0.003150\n",
      "Encoder training step: 570/1000\n",
      "[DEBUG] Loss this iteration: 0.003046\n",
      "Encoder training step: 571/1000\n",
      "[DEBUG] Loss this iteration: 0.002988\n",
      "Encoder training step: 572/1000\n",
      "[DEBUG] Loss this iteration: 0.003043\n",
      "Encoder training step: 573/1000\n",
      "[DEBUG] Loss this iteration: 0.003236\n",
      "Encoder training step: 574/1000\n",
      "[DEBUG] Loss this iteration: 0.003245\n",
      "Encoder training step: 575/1000\n",
      "[DEBUG] Loss this iteration: 0.003129\n",
      "Encoder training step: 576/1000\n",
      "[DEBUG] Loss this iteration: 0.003145\n",
      "Encoder training step: 577/1000\n",
      "[DEBUG] Loss this iteration: 0.003090\n",
      "Encoder training step: 578/1000\n",
      "[DEBUG] Loss this iteration: 0.002815\n",
      "Encoder training step: 579/1000\n",
      "[DEBUG] Loss this iteration: 0.002913\n",
      "Encoder training step: 580/1000\n",
      "[DEBUG] Loss this iteration: 0.003167\n",
      "Encoder training step: 581/1000\n",
      "[DEBUG] Loss this iteration: 0.002959\n",
      "Encoder training step: 582/1000\n",
      "[DEBUG] Loss this iteration: 0.002821\n",
      "Encoder training step: 583/1000\n",
      "[DEBUG] Loss this iteration: 0.002822\n",
      "Encoder training step: 584/1000\n",
      "[DEBUG] Loss this iteration: 0.002804\n",
      "Encoder training step: 585/1000\n",
      "[DEBUG] Loss this iteration: 0.002863\n",
      "Encoder training step: 586/1000\n",
      "[DEBUG] Loss this iteration: 0.003079\n",
      "Encoder training step: 587/1000\n",
      "[DEBUG] Loss this iteration: 0.003015\n",
      "Encoder training step: 588/1000\n",
      "[DEBUG] Loss this iteration: 0.002760\n",
      "Encoder training step: 589/1000\n",
      "[DEBUG] Loss this iteration: 0.003120\n",
      "Encoder training step: 590/1000\n",
      "[DEBUG] Loss this iteration: 0.003025\n",
      "Encoder training step: 591/1000\n",
      "[DEBUG] Loss this iteration: 0.002896\n",
      "Encoder training step: 592/1000\n",
      "[DEBUG] Loss this iteration: 0.002829\n",
      "Encoder training step: 593/1000\n",
      "[DEBUG] Loss this iteration: 0.003064\n",
      "Encoder training step: 594/1000\n",
      "[DEBUG] Loss this iteration: 0.003057\n",
      "Encoder training step: 595/1000\n",
      "[DEBUG] Loss this iteration: 0.003046\n",
      "Encoder training step: 596/1000\n",
      "[DEBUG] Loss this iteration: 0.002819\n",
      "Encoder training step: 597/1000\n",
      "[DEBUG] Loss this iteration: 0.002822\n",
      "Encoder training step: 598/1000\n",
      "[DEBUG] Loss this iteration: 0.002847\n",
      "Encoder training step: 599/1000\n",
      "[DEBUG] Loss this iteration: 0.002984\n",
      "Encoder training step: 600/1000\n",
      "[DEBUG] Loss this iteration: 0.002949\n",
      "Encoder training step: 601/1000\n",
      "[DEBUG] Loss this iteration: 0.003167\n",
      "Encoder training step: 602/1000\n",
      "[DEBUG] Loss this iteration: 0.003648\n",
      "Encoder training step: 603/1000\n",
      "[DEBUG] Loss this iteration: 0.003289\n",
      "Encoder training step: 604/1000\n",
      "[DEBUG] Loss this iteration: 0.003234\n",
      "Encoder training step: 605/1000\n",
      "[DEBUG] Loss this iteration: 0.003298\n",
      "Encoder training step: 606/1000\n",
      "[DEBUG] Loss this iteration: 0.003325\n",
      "Encoder training step: 607/1000\n",
      "[DEBUG] Loss this iteration: 0.003053\n",
      "Encoder training step: 608/1000\n",
      "[DEBUG] Loss this iteration: 0.003263\n",
      "Encoder training step: 609/1000\n",
      "[DEBUG] Loss this iteration: 0.003035\n",
      "Encoder training step: 610/1000\n",
      "[DEBUG] Loss this iteration: 0.002820\n",
      "Encoder training step: 611/1000\n",
      "[DEBUG] Loss this iteration: 0.002709\n",
      "Encoder training step: 612/1000\n",
      "[DEBUG] Loss this iteration: 0.003006\n",
      "Encoder training step: 613/1000\n",
      "[DEBUG] Loss this iteration: 0.002828\n",
      "Encoder training step: 614/1000\n",
      "[DEBUG] Loss this iteration: 0.002785\n",
      "Encoder training step: 615/1000\n",
      "[DEBUG] Loss this iteration: 0.002793\n",
      "Encoder training step: 616/1000\n",
      "[DEBUG] Loss this iteration: 0.003164\n",
      "Encoder training step: 617/1000\n",
      "[DEBUG] Loss this iteration: 0.003342\n",
      "Encoder training step: 618/1000\n",
      "[DEBUG] Loss this iteration: 0.002817\n",
      "Encoder training step: 619/1000\n",
      "[DEBUG] Loss this iteration: 0.003187\n",
      "Encoder training step: 620/1000\n",
      "[DEBUG] Loss this iteration: 0.003189\n",
      "Encoder training step: 621/1000\n",
      "[DEBUG] Loss this iteration: 0.003102\n",
      "Encoder training step: 622/1000\n",
      "[DEBUG] Loss this iteration: 0.002836\n",
      "Encoder training step: 623/1000\n",
      "[DEBUG] Loss this iteration: 0.003041\n",
      "Encoder training step: 624/1000\n",
      "[DEBUG] Loss this iteration: 0.002749\n",
      "Encoder training step: 625/1000\n",
      "[DEBUG] Loss this iteration: 0.002732\n",
      "Encoder training step: 626/1000\n",
      "[DEBUG] Loss this iteration: 0.002995\n",
      "Encoder training step: 627/1000\n",
      "[DEBUG] Loss this iteration: 0.003112\n",
      "Encoder training step: 628/1000\n",
      "[DEBUG] Loss this iteration: 0.002934\n",
      "Encoder training step: 629/1000\n",
      "[DEBUG] Loss this iteration: 0.002701\n",
      "Encoder training step: 630/1000\n",
      "[DEBUG] Loss this iteration: 0.002874\n",
      "Encoder training step: 631/1000\n",
      "[DEBUG] Loss this iteration: 0.002657\n",
      "Encoder training step: 632/1000\n",
      "[DEBUG] Loss this iteration: 0.003050\n",
      "Encoder training step: 633/1000\n",
      "[DEBUG] Loss this iteration: 0.002658\n",
      "Encoder training step: 634/1000\n",
      "[DEBUG] Loss this iteration: 0.002795\n",
      "Encoder training step: 635/1000\n",
      "[DEBUG] Loss this iteration: 0.002880\n",
      "Encoder training step: 636/1000\n",
      "[DEBUG] Loss this iteration: 0.002981\n",
      "Encoder training step: 637/1000\n",
      "[DEBUG] Loss this iteration: 0.002649\n",
      "Encoder training step: 638/1000\n",
      "[DEBUG] Loss this iteration: 0.002923\n",
      "Encoder training step: 639/1000\n",
      "[DEBUG] Loss this iteration: 0.003400\n",
      "Encoder training step: 640/1000\n",
      "[DEBUG] Loss this iteration: 0.003463\n",
      "Encoder training step: 641/1000\n",
      "[DEBUG] Loss this iteration: 0.003111\n",
      "Encoder training step: 642/1000\n",
      "[DEBUG] Loss this iteration: 0.003225\n",
      "Encoder training step: 643/1000\n",
      "[DEBUG] Loss this iteration: 0.003149\n",
      "Encoder training step: 644/1000\n",
      "[DEBUG] Loss this iteration: 0.003368\n",
      "Encoder training step: 645/1000\n",
      "[DEBUG] Loss this iteration: 0.003409\n",
      "Encoder training step: 646/1000\n",
      "[DEBUG] Loss this iteration: 0.003386\n",
      "Encoder training step: 647/1000\n",
      "[DEBUG] Loss this iteration: 0.003385\n",
      "Encoder training step: 648/1000\n",
      "[DEBUG] Loss this iteration: 0.003168\n",
      "Encoder training step: 649/1000\n",
      "[DEBUG] Loss this iteration: 0.002886\n",
      "Encoder training step: 650/1000\n",
      "[DEBUG] Loss this iteration: 0.003870\n",
      "Encoder training step: 651/1000\n",
      "[DEBUG] Loss this iteration: 0.004033\n",
      "Encoder training step: 652/1000\n",
      "[DEBUG] Loss this iteration: 0.003601\n",
      "Encoder training step: 653/1000\n",
      "[DEBUG] Loss this iteration: 0.003253\n",
      "Encoder training step: 654/1000\n",
      "[DEBUG] Loss this iteration: 0.002858\n",
      "Encoder training step: 655/1000\n",
      "[DEBUG] Loss this iteration: 0.002774\n",
      "Encoder training step: 656/1000\n",
      "[DEBUG] Loss this iteration: 0.003082\n",
      "Encoder training step: 657/1000\n",
      "[DEBUG] Loss this iteration: 0.002809\n",
      "Encoder training step: 658/1000\n",
      "[DEBUG] Loss this iteration: 0.002945\n",
      "Encoder training step: 659/1000\n",
      "[DEBUG] Loss this iteration: 0.002818\n",
      "Encoder training step: 660/1000\n",
      "[DEBUG] Loss this iteration: 0.003046\n",
      "Encoder training step: 661/1000\n",
      "[DEBUG] Loss this iteration: 0.003083\n",
      "Encoder training step: 662/1000\n",
      "[DEBUG] Loss this iteration: 0.002916\n",
      "Encoder training step: 663/1000\n",
      "[DEBUG] Loss this iteration: 0.002839\n",
      "Encoder training step: 664/1000\n",
      "[DEBUG] Loss this iteration: 0.002911\n",
      "Encoder training step: 665/1000\n",
      "[DEBUG] Loss this iteration: 0.002836\n",
      "Encoder training step: 666/1000\n",
      "[DEBUG] Loss this iteration: 0.002957\n",
      "Encoder training step: 667/1000\n",
      "[DEBUG] Loss this iteration: 0.002684\n",
      "Encoder training step: 668/1000\n",
      "[DEBUG] Loss this iteration: 0.003285\n",
      "Encoder training step: 669/1000\n",
      "[DEBUG] Loss this iteration: 0.003213\n",
      "Encoder training step: 670/1000\n",
      "[DEBUG] Loss this iteration: 0.003140\n",
      "Encoder training step: 671/1000\n",
      "[DEBUG] Loss this iteration: 0.002889\n",
      "Encoder training step: 672/1000\n",
      "[DEBUG] Loss this iteration: 0.002813\n",
      "Encoder training step: 673/1000\n",
      "[DEBUG] Loss this iteration: 0.002998\n",
      "Encoder training step: 674/1000\n",
      "[DEBUG] Loss this iteration: 0.002838\n",
      "Encoder training step: 675/1000\n",
      "[DEBUG] Loss this iteration: 0.002920\n",
      "Encoder training step: 676/1000\n",
      "[DEBUG] Loss this iteration: 0.002698\n",
      "Encoder training step: 677/1000\n",
      "[DEBUG] Loss this iteration: 0.002863\n",
      "Encoder training step: 678/1000\n",
      "[DEBUG] Loss this iteration: 0.002880\n",
      "Encoder training step: 679/1000\n",
      "[DEBUG] Loss this iteration: 0.002715\n",
      "Encoder training step: 680/1000\n",
      "[DEBUG] Loss this iteration: 0.002789\n",
      "Encoder training step: 681/1000\n",
      "[DEBUG] Loss this iteration: 0.002936\n",
      "Encoder training step: 682/1000\n",
      "[DEBUG] Loss this iteration: 0.002835\n",
      "Encoder training step: 683/1000\n",
      "[DEBUG] Loss this iteration: 0.002862\n",
      "Encoder training step: 684/1000\n",
      "[DEBUG] Loss this iteration: 0.002630\n",
      "Encoder training step: 685/1000\n",
      "[DEBUG] Loss this iteration: 0.003108\n",
      "Encoder training step: 686/1000\n",
      "[DEBUG] Loss this iteration: 0.002958\n",
      "Encoder training step: 687/1000\n",
      "[DEBUG] Loss this iteration: 0.002744\n",
      "Encoder training step: 688/1000\n",
      "[DEBUG] Loss this iteration: 0.003079\n",
      "Encoder training step: 689/1000\n",
      "[DEBUG] Loss this iteration: 0.003581\n",
      "Encoder training step: 690/1000\n",
      "[DEBUG] Loss this iteration: 0.003100\n",
      "Encoder training step: 691/1000\n",
      "[DEBUG] Loss this iteration: 0.002824\n",
      "Encoder training step: 692/1000\n",
      "[DEBUG] Loss this iteration: 0.002732\n",
      "Encoder training step: 693/1000\n",
      "[DEBUG] Loss this iteration: 0.002744\n",
      "Encoder training step: 694/1000\n",
      "[DEBUG] Loss this iteration: 0.002679\n",
      "Encoder training step: 695/1000\n",
      "[DEBUG] Loss this iteration: 0.002812\n",
      "Encoder training step: 696/1000\n",
      "[DEBUG] Loss this iteration: 0.003011\n",
      "Encoder training step: 697/1000\n",
      "[DEBUG] Loss this iteration: 0.003011\n",
      "Encoder training step: 698/1000\n",
      "[DEBUG] Loss this iteration: 0.002992\n",
      "Encoder training step: 699/1000\n",
      "[DEBUG] Loss this iteration: 0.002776\n",
      "Encoder training step: 700/1000\n",
      "[DEBUG] Loss this iteration: 0.003099\n",
      "Encoder training step: 701/1000\n",
      "[DEBUG] Loss this iteration: 0.002800\n",
      "Encoder training step: 702/1000\n",
      "[DEBUG] Loss this iteration: 0.003124\n",
      "Encoder training step: 703/1000\n",
      "[DEBUG] Loss this iteration: 0.002737\n",
      "Encoder training step: 704/1000\n",
      "[DEBUG] Loss this iteration: 0.002850\n",
      "Encoder training step: 705/1000\n",
      "[DEBUG] Loss this iteration: 0.002731\n",
      "Encoder training step: 706/1000\n",
      "[DEBUG] Loss this iteration: 0.002902\n",
      "Encoder training step: 707/1000\n",
      "[DEBUG] Loss this iteration: 0.003184\n",
      "Encoder training step: 708/1000\n",
      "[DEBUG] Loss this iteration: 0.002715\n",
      "Encoder training step: 709/1000\n",
      "[DEBUG] Loss this iteration: 0.002799\n",
      "Encoder training step: 710/1000\n",
      "[DEBUG] Loss this iteration: 0.002708\n",
      "Encoder training step: 711/1000\n",
      "[DEBUG] Loss this iteration: 0.002959\n",
      "Encoder training step: 712/1000\n",
      "[DEBUG] Loss this iteration: 0.002806\n",
      "Encoder training step: 713/1000\n",
      "[DEBUG] Loss this iteration: 0.002794\n",
      "Encoder training step: 714/1000\n",
      "[DEBUG] Loss this iteration: 0.002887\n",
      "Encoder training step: 715/1000\n",
      "[DEBUG] Loss this iteration: 0.002848\n",
      "Encoder training step: 716/1000\n",
      "[DEBUG] Loss this iteration: 0.003171\n",
      "Encoder training step: 717/1000\n",
      "[DEBUG] Loss this iteration: 0.002917\n",
      "Encoder training step: 718/1000\n",
      "[DEBUG] Loss this iteration: 0.002799\n",
      "Encoder training step: 719/1000\n",
      "[DEBUG] Loss this iteration: 0.002844\n",
      "Encoder training step: 720/1000\n",
      "[DEBUG] Loss this iteration: 0.002925\n",
      "Encoder training step: 721/1000\n",
      "[DEBUG] Loss this iteration: 0.003000\n",
      "Encoder training step: 722/1000\n",
      "[DEBUG] Loss this iteration: 0.002969\n",
      "Encoder training step: 723/1000\n",
      "[DEBUG] Loss this iteration: 0.002581\n",
      "Encoder training step: 724/1000\n",
      "[DEBUG] Loss this iteration: 0.003061\n",
      "Encoder training step: 725/1000\n",
      "[DEBUG] Loss this iteration: 0.003040\n",
      "Encoder training step: 726/1000\n",
      "[DEBUG] Loss this iteration: 0.002997\n",
      "Encoder training step: 727/1000\n",
      "[DEBUG] Loss this iteration: 0.002583\n",
      "Encoder training step: 728/1000\n",
      "[DEBUG] Loss this iteration: 0.003173\n",
      "Encoder training step: 729/1000\n",
      "[DEBUG] Loss this iteration: 0.003230\n",
      "Encoder training step: 730/1000\n",
      "[DEBUG] Loss this iteration: 0.002883\n",
      "Encoder training step: 731/1000\n",
      "[DEBUG] Loss this iteration: 0.003209\n",
      "Encoder training step: 732/1000\n",
      "[DEBUG] Loss this iteration: 0.002960\n",
      "Encoder training step: 733/1000\n",
      "[DEBUG] Loss this iteration: 0.003096\n",
      "Encoder training step: 734/1000\n",
      "[DEBUG] Loss this iteration: 0.002832\n",
      "Encoder training step: 735/1000\n",
      "[DEBUG] Loss this iteration: 0.002760\n",
      "Encoder training step: 736/1000\n",
      "[DEBUG] Loss this iteration: 0.002941\n",
      "Encoder training step: 737/1000\n",
      "[DEBUG] Loss this iteration: 0.002791\n",
      "Encoder training step: 738/1000\n",
      "[DEBUG] Loss this iteration: 0.002867\n",
      "Encoder training step: 739/1000\n",
      "[DEBUG] Loss this iteration: 0.002689\n",
      "Encoder training step: 740/1000\n",
      "[DEBUG] Loss this iteration: 0.002710\n",
      "Encoder training step: 741/1000\n",
      "[DEBUG] Loss this iteration: 0.002760\n",
      "Encoder training step: 742/1000\n",
      "[DEBUG] Loss this iteration: 0.002799\n",
      "Encoder training step: 743/1000\n",
      "[DEBUG] Loss this iteration: 0.003000\n",
      "Encoder training step: 744/1000\n",
      "[DEBUG] Loss this iteration: 0.003015\n",
      "Encoder training step: 745/1000\n",
      "[DEBUG] Loss this iteration: 0.003035\n",
      "Encoder training step: 746/1000\n",
      "[DEBUG] Loss this iteration: 0.002945\n",
      "Encoder training step: 747/1000\n",
      "[DEBUG] Loss this iteration: 0.002861\n",
      "Encoder training step: 748/1000\n",
      "[DEBUG] Loss this iteration: 0.002858\n",
      "Encoder training step: 749/1000\n",
      "[DEBUG] Loss this iteration: 0.002660\n",
      "Encoder training step: 750/1000\n",
      "[DEBUG] Loss this iteration: 0.002626\n",
      "Encoder training step: 751/1000\n",
      "[DEBUG] Loss this iteration: 0.003054\n",
      "Encoder training step: 752/1000\n",
      "[DEBUG] Loss this iteration: 0.002882\n",
      "Encoder training step: 753/1000\n",
      "[DEBUG] Loss this iteration: 0.002941\n",
      "Encoder training step: 754/1000\n",
      "[DEBUG] Loss this iteration: 0.002743\n",
      "Encoder training step: 755/1000\n",
      "[DEBUG] Loss this iteration: 0.002845\n",
      "Encoder training step: 756/1000\n",
      "[DEBUG] Loss this iteration: 0.002855\n",
      "Encoder training step: 757/1000\n",
      "[DEBUG] Loss this iteration: 0.002752\n",
      "Encoder training step: 758/1000\n",
      "[DEBUG] Loss this iteration: 0.002869\n",
      "Encoder training step: 759/1000\n",
      "[DEBUG] Loss this iteration: 0.003048\n",
      "Encoder training step: 760/1000\n",
      "[DEBUG] Loss this iteration: 0.002995\n",
      "Encoder training step: 761/1000\n",
      "[DEBUG] Loss this iteration: 0.003506\n",
      "Encoder training step: 762/1000\n",
      "[DEBUG] Loss this iteration: 0.002909\n",
      "Encoder training step: 763/1000\n",
      "[DEBUG] Loss this iteration: 0.002692\n",
      "Encoder training step: 764/1000\n",
      "[DEBUG] Loss this iteration: 0.002823\n",
      "Encoder training step: 765/1000\n",
      "[DEBUG] Loss this iteration: 0.002990\n",
      "Encoder training step: 766/1000\n",
      "[DEBUG] Loss this iteration: 0.002881\n",
      "Encoder training step: 767/1000\n",
      "[DEBUG] Loss this iteration: 0.002587\n",
      "Encoder training step: 768/1000\n",
      "[DEBUG] Loss this iteration: 0.002823\n",
      "Encoder training step: 769/1000\n",
      "[DEBUG] Loss this iteration: 0.002779\n",
      "Encoder training step: 770/1000\n",
      "[DEBUG] Loss this iteration: 0.002897\n",
      "Encoder training step: 771/1000\n",
      "[DEBUG] Loss this iteration: 0.002473\n",
      "Encoder training step: 772/1000\n",
      "[DEBUG] Loss this iteration: 0.002787\n",
      "Encoder training step: 773/1000\n",
      "[DEBUG] Loss this iteration: 0.002863\n",
      "Encoder training step: 774/1000\n",
      "[DEBUG] Loss this iteration: 0.002872\n",
      "Encoder training step: 775/1000\n",
      "[DEBUG] Loss this iteration: 0.002867\n",
      "Encoder training step: 776/1000\n",
      "[DEBUG] Loss this iteration: 0.002754\n",
      "Encoder training step: 777/1000\n",
      "[DEBUG] Loss this iteration: 0.002706\n",
      "Encoder training step: 778/1000\n",
      "[DEBUG] Loss this iteration: 0.003028\n",
      "Encoder training step: 779/1000\n",
      "[DEBUG] Loss this iteration: 0.002812\n",
      "Encoder training step: 780/1000\n",
      "[DEBUG] Loss this iteration: 0.002981\n",
      "Encoder training step: 781/1000\n",
      "[DEBUG] Loss this iteration: 0.003080\n",
      "Encoder training step: 782/1000\n",
      "[DEBUG] Loss this iteration: 0.003032\n",
      "Encoder training step: 783/1000\n",
      "[DEBUG] Loss this iteration: 0.002671\n",
      "Encoder training step: 784/1000\n",
      "[DEBUG] Loss this iteration: 0.003001\n",
      "Encoder training step: 785/1000\n",
      "[DEBUG] Loss this iteration: 0.002767\n",
      "Encoder training step: 786/1000\n",
      "[DEBUG] Loss this iteration: 0.002947\n",
      "Encoder training step: 787/1000\n",
      "[DEBUG] Loss this iteration: 0.002876\n",
      "Encoder training step: 788/1000\n",
      "[DEBUG] Loss this iteration: 0.002697\n",
      "Encoder training step: 789/1000\n",
      "[DEBUG] Loss this iteration: 0.003017\n",
      "Encoder training step: 790/1000\n",
      "[DEBUG] Loss this iteration: 0.002683\n",
      "Encoder training step: 791/1000\n",
      "[DEBUG] Loss this iteration: 0.002658\n",
      "Encoder training step: 792/1000\n",
      "[DEBUG] Loss this iteration: 0.002751\n",
      "Encoder training step: 793/1000\n",
      "[DEBUG] Loss this iteration: 0.003207\n",
      "Encoder training step: 794/1000\n",
      "[DEBUG] Loss this iteration: 0.002822\n",
      "Encoder training step: 795/1000\n",
      "[DEBUG] Loss this iteration: 0.002910\n",
      "Encoder training step: 796/1000\n",
      "[DEBUG] Loss this iteration: 0.002553\n",
      "Encoder training step: 797/1000\n",
      "[DEBUG] Loss this iteration: 0.002756\n",
      "Encoder training step: 798/1000\n",
      "[DEBUG] Loss this iteration: 0.002934\n",
      "Encoder training step: 799/1000\n",
      "[DEBUG] Loss this iteration: 0.002949\n",
      "Encoder training step: 800/1000\n",
      "[DEBUG] Loss this iteration: 0.002867\n",
      "Encoder training step: 801/1000\n",
      "[DEBUG] Loss this iteration: 0.002781\n",
      "Encoder training step: 802/1000\n",
      "[DEBUG] Loss this iteration: 0.002920\n",
      "Encoder training step: 803/1000\n",
      "[DEBUG] Loss this iteration: 0.002943\n",
      "Encoder training step: 804/1000\n",
      "[DEBUG] Loss this iteration: 0.003301\n",
      "Encoder training step: 805/1000\n",
      "[DEBUG] Loss this iteration: 0.003066\n",
      "Encoder training step: 806/1000\n",
      "[DEBUG] Loss this iteration: 0.003169\n",
      "Encoder training step: 807/1000\n",
      "[DEBUG] Loss this iteration: 0.003197\n",
      "Encoder training step: 808/1000\n",
      "[DEBUG] Loss this iteration: 0.003090\n",
      "Encoder training step: 809/1000\n",
      "[DEBUG] Loss this iteration: 0.002880\n",
      "Encoder training step: 810/1000\n",
      "[DEBUG] Loss this iteration: 0.002687\n",
      "Encoder training step: 811/1000\n",
      "[DEBUG] Loss this iteration: 0.002889\n",
      "Encoder training step: 812/1000\n",
      "[DEBUG] Loss this iteration: 0.003030\n",
      "Encoder training step: 813/1000\n",
      "[DEBUG] Loss this iteration: 0.002770\n",
      "Encoder training step: 814/1000\n",
      "[DEBUG] Loss this iteration: 0.002638\n",
      "Encoder training step: 815/1000\n",
      "[DEBUG] Loss this iteration: 0.002589\n",
      "Encoder training step: 816/1000\n",
      "[DEBUG] Loss this iteration: 0.002645\n",
      "Encoder training step: 817/1000\n",
      "[DEBUG] Loss this iteration: 0.002875\n",
      "Encoder training step: 818/1000\n",
      "[DEBUG] Loss this iteration: 0.002836\n",
      "Encoder training step: 819/1000\n",
      "[DEBUG] Loss this iteration: 0.002677\n",
      "Encoder training step: 820/1000\n",
      "[DEBUG] Loss this iteration: 0.002454\n",
      "Encoder training step: 821/1000\n",
      "[DEBUG] Loss this iteration: 0.002838\n",
      "Encoder training step: 822/1000\n",
      "[DEBUG] Loss this iteration: 0.002891\n",
      "Encoder training step: 823/1000\n",
      "[DEBUG] Loss this iteration: 0.002595\n",
      "Encoder training step: 824/1000\n",
      "[DEBUG] Loss this iteration: 0.002781\n",
      "Encoder training step: 825/1000\n",
      "[DEBUG] Loss this iteration: 0.002642\n",
      "Encoder training step: 826/1000\n",
      "[DEBUG] Loss this iteration: 0.002700\n",
      "Encoder training step: 827/1000\n",
      "[DEBUG] Loss this iteration: 0.002936\n",
      "Encoder training step: 828/1000\n",
      "[DEBUG] Loss this iteration: 0.002701\n",
      "Encoder training step: 829/1000\n",
      "[DEBUG] Loss this iteration: 0.002718\n",
      "Encoder training step: 830/1000\n",
      "[DEBUG] Loss this iteration: 0.002591\n",
      "Encoder training step: 831/1000\n",
      "[DEBUG] Loss this iteration: 0.002651\n",
      "Encoder training step: 832/1000\n",
      "[DEBUG] Loss this iteration: 0.002985\n",
      "Encoder training step: 833/1000\n",
      "[DEBUG] Loss this iteration: 0.002791\n",
      "Encoder training step: 834/1000\n",
      "[DEBUG] Loss this iteration: 0.002747\n",
      "Encoder training step: 835/1000\n",
      "[DEBUG] Loss this iteration: 0.002835\n",
      "Encoder training step: 836/1000\n",
      "[DEBUG] Loss this iteration: 0.002500\n",
      "Encoder training step: 837/1000\n",
      "[DEBUG] Loss this iteration: 0.002908\n",
      "Encoder training step: 838/1000\n",
      "[DEBUG] Loss this iteration: 0.002603\n",
      "Encoder training step: 839/1000\n",
      "[DEBUG] Loss this iteration: 0.002537\n",
      "Encoder training step: 840/1000\n",
      "[DEBUG] Loss this iteration: 0.002800\n",
      "Encoder training step: 841/1000\n",
      "[DEBUG] Loss this iteration: 0.003174\n",
      "Encoder training step: 842/1000\n",
      "[DEBUG] Loss this iteration: 0.003056\n",
      "Encoder training step: 843/1000\n",
      "[DEBUG] Loss this iteration: 0.002867\n",
      "Encoder training step: 844/1000\n",
      "[DEBUG] Loss this iteration: 0.003007\n",
      "Encoder training step: 845/1000\n",
      "[DEBUG] Loss this iteration: 0.003044\n",
      "Encoder training step: 846/1000\n",
      "[DEBUG] Loss this iteration: 0.003071\n",
      "Encoder training step: 847/1000\n",
      "[DEBUG] Loss this iteration: 0.003230\n",
      "Encoder training step: 848/1000\n",
      "[DEBUG] Loss this iteration: 0.002833\n",
      "Encoder training step: 849/1000\n",
      "[DEBUG] Loss this iteration: 0.002586\n",
      "Encoder training step: 850/1000\n",
      "[DEBUG] Loss this iteration: 0.002664\n",
      "Encoder training step: 851/1000\n",
      "[DEBUG] Loss this iteration: 0.003079\n",
      "Encoder training step: 852/1000\n",
      "[DEBUG] Loss this iteration: 0.002723\n",
      "Encoder training step: 853/1000\n",
      "[DEBUG] Loss this iteration: 0.002914\n",
      "Encoder training step: 854/1000\n",
      "[DEBUG] Loss this iteration: 0.002671\n",
      "Encoder training step: 855/1000\n",
      "[DEBUG] Loss this iteration: 0.002940\n",
      "Encoder training step: 856/1000\n",
      "[DEBUG] Loss this iteration: 0.002627\n",
      "Encoder training step: 857/1000\n",
      "[DEBUG] Loss this iteration: 0.002404\n",
      "Encoder training step: 858/1000\n",
      "[DEBUG] Loss this iteration: 0.002776\n",
      "Encoder training step: 859/1000\n",
      "[DEBUG] Loss this iteration: 0.002696\n",
      "Encoder training step: 860/1000\n",
      "[DEBUG] Loss this iteration: 0.002989\n",
      "Encoder training step: 861/1000\n",
      "[DEBUG] Loss this iteration: 0.003451\n",
      "Encoder training step: 862/1000\n",
      "[DEBUG] Loss this iteration: 0.003077\n",
      "Encoder training step: 863/1000\n",
      "[DEBUG] Loss this iteration: 0.002800\n",
      "Encoder training step: 864/1000\n",
      "[DEBUG] Loss this iteration: 0.002950\n",
      "Encoder training step: 865/1000\n",
      "[DEBUG] Loss this iteration: 0.002919\n",
      "Encoder training step: 866/1000\n",
      "[DEBUG] Loss this iteration: 0.002769\n",
      "Encoder training step: 867/1000\n",
      "[DEBUG] Loss this iteration: 0.003175\n",
      "Encoder training step: 868/1000\n",
      "[DEBUG] Loss this iteration: 0.003045\n",
      "Encoder training step: 869/1000\n",
      "[DEBUG] Loss this iteration: 0.003244\n",
      "Encoder training step: 870/1000\n",
      "[DEBUG] Loss this iteration: 0.002897\n",
      "Encoder training step: 871/1000\n",
      "[DEBUG] Loss this iteration: 0.002727\n",
      "Encoder training step: 872/1000\n",
      "[DEBUG] Loss this iteration: 0.002750\n",
      "Encoder training step: 873/1000\n",
      "[DEBUG] Loss this iteration: 0.002589\n",
      "Encoder training step: 874/1000\n",
      "[DEBUG] Loss this iteration: 0.002544\n",
      "Encoder training step: 875/1000\n",
      "[DEBUG] Loss this iteration: 0.002714\n",
      "Encoder training step: 876/1000\n",
      "[DEBUG] Loss this iteration: 0.002360\n",
      "Encoder training step: 877/1000\n",
      "[DEBUG] Loss this iteration: 0.002415\n",
      "Encoder training step: 878/1000\n",
      "[DEBUG] Loss this iteration: 0.002658\n",
      "Encoder training step: 879/1000\n",
      "[DEBUG] Loss this iteration: 0.002716\n",
      "Encoder training step: 880/1000\n",
      "[DEBUG] Loss this iteration: 0.002790\n",
      "Encoder training step: 881/1000\n",
      "[DEBUG] Loss this iteration: 0.002900\n",
      "Encoder training step: 882/1000\n",
      "[DEBUG] Loss this iteration: 0.002705\n",
      "Encoder training step: 883/1000\n",
      "[DEBUG] Loss this iteration: 0.002582\n",
      "Encoder training step: 884/1000\n",
      "[DEBUG] Loss this iteration: 0.002616\n",
      "Encoder training step: 885/1000\n",
      "[DEBUG] Loss this iteration: 0.002873\n",
      "Encoder training step: 886/1000\n",
      "[DEBUG] Loss this iteration: 0.002547\n",
      "Encoder training step: 887/1000\n",
      "[DEBUG] Loss this iteration: 0.002812\n",
      "Encoder training step: 888/1000\n",
      "[DEBUG] Loss this iteration: 0.002568\n",
      "Encoder training step: 889/1000\n",
      "[DEBUG] Loss this iteration: 0.002845\n",
      "Encoder training step: 890/1000\n",
      "[DEBUG] Loss this iteration: 0.002450\n",
      "Encoder training step: 891/1000\n",
      "[DEBUG] Loss this iteration: 0.002663\n",
      "Encoder training step: 892/1000\n",
      "[DEBUG] Loss this iteration: 0.002529\n",
      "Encoder training step: 893/1000\n",
      "[DEBUG] Loss this iteration: 0.002749\n",
      "Encoder training step: 894/1000\n",
      "[DEBUG] Loss this iteration: 0.002761\n",
      "Encoder training step: 895/1000\n",
      "[DEBUG] Loss this iteration: 0.002515\n",
      "Encoder training step: 896/1000\n",
      "[DEBUG] Loss this iteration: 0.002352\n",
      "Encoder training step: 897/1000\n",
      "[DEBUG] Loss this iteration: 0.002461\n",
      "Encoder training step: 898/1000\n",
      "[DEBUG] Loss this iteration: 0.002672\n",
      "Encoder training step: 899/1000\n",
      "[DEBUG] Loss this iteration: 0.002653\n",
      "Encoder training step: 900/1000\n",
      "[DEBUG] Loss this iteration: 0.002585\n",
      "Encoder training step: 901/1000\n",
      "[DEBUG] Loss this iteration: 0.002385\n",
      "Encoder training step: 902/1000\n",
      "[DEBUG] Loss this iteration: 0.002488\n",
      "Encoder training step: 903/1000\n",
      "[DEBUG] Loss this iteration: 0.002744\n",
      "Encoder training step: 904/1000\n",
      "[DEBUG] Loss this iteration: 0.002588\n",
      "Encoder training step: 905/1000\n",
      "[DEBUG] Loss this iteration: 0.002541\n",
      "Encoder training step: 906/1000\n",
      "[DEBUG] Loss this iteration: 0.002628\n",
      "Encoder training step: 907/1000\n",
      "[DEBUG] Loss this iteration: 0.002705\n",
      "Encoder training step: 908/1000\n",
      "[DEBUG] Loss this iteration: 0.002481\n",
      "Encoder training step: 909/1000\n",
      "[DEBUG] Loss this iteration: 0.002711\n",
      "Encoder training step: 910/1000\n",
      "[DEBUG] Loss this iteration: 0.002724\n",
      "Encoder training step: 911/1000\n",
      "[DEBUG] Loss this iteration: 0.002551\n",
      "Encoder training step: 912/1000\n",
      "[DEBUG] Loss this iteration: 0.002672\n",
      "Encoder training step: 913/1000\n",
      "[DEBUG] Loss this iteration: 0.002573\n",
      "Encoder training step: 914/1000\n",
      "[DEBUG] Loss this iteration: 0.002429\n",
      "Encoder training step: 915/1000\n",
      "[DEBUG] Loss this iteration: 0.002277\n",
      "Encoder training step: 916/1000\n",
      "[DEBUG] Loss this iteration: 0.002334\n",
      "Encoder training step: 917/1000\n",
      "[DEBUG] Loss this iteration: 0.002585\n",
      "Encoder training step: 918/1000\n",
      "[DEBUG] Loss this iteration: 0.002698\n",
      "Encoder training step: 919/1000\n",
      "[DEBUG] Loss this iteration: 0.002592\n",
      "Encoder training step: 920/1000\n",
      "[DEBUG] Loss this iteration: 0.002518\n",
      "Encoder training step: 921/1000\n",
      "[DEBUG] Loss this iteration: 0.002336\n",
      "Encoder training step: 922/1000\n",
      "[DEBUG] Loss this iteration: 0.002599\n",
      "Encoder training step: 923/1000\n",
      "[DEBUG] Loss this iteration: 0.002538\n",
      "Encoder training step: 924/1000\n",
      "[DEBUG] Loss this iteration: 0.002539\n",
      "Encoder training step: 925/1000\n",
      "[DEBUG] Loss this iteration: 0.002592\n",
      "Encoder training step: 926/1000\n",
      "[DEBUG] Loss this iteration: 0.002326\n",
      "Encoder training step: 927/1000\n",
      "[DEBUG] Loss this iteration: 0.002171\n",
      "Encoder training step: 928/1000\n",
      "[DEBUG] Loss this iteration: 0.002348\n",
      "Encoder training step: 929/1000\n",
      "[DEBUG] Loss this iteration: 0.002593\n",
      "Encoder training step: 930/1000\n",
      "[DEBUG] Loss this iteration: 0.002458\n",
      "Encoder training step: 931/1000\n",
      "[DEBUG] Loss this iteration: 0.002788\n",
      "Encoder training step: 932/1000\n",
      "[DEBUG] Loss this iteration: 0.003016\n",
      "Encoder training step: 933/1000\n",
      "[DEBUG] Loss this iteration: 0.002933\n",
      "Encoder training step: 934/1000\n",
      "[DEBUG] Loss this iteration: 0.002584\n",
      "Encoder training step: 935/1000\n",
      "[DEBUG] Loss this iteration: 0.002491\n",
      "Encoder training step: 936/1000\n",
      "[DEBUG] Loss this iteration: 0.002379\n",
      "Encoder training step: 937/1000\n",
      "[DEBUG] Loss this iteration: 0.002246\n",
      "Encoder training step: 938/1000\n",
      "[DEBUG] Loss this iteration: 0.002642\n",
      "Encoder training step: 939/1000\n",
      "[DEBUG] Loss this iteration: 0.002399\n",
      "Encoder training step: 940/1000\n",
      "[DEBUG] Loss this iteration: 0.002398\n",
      "Encoder training step: 941/1000\n",
      "[DEBUG] Loss this iteration: 0.002450\n",
      "Encoder training step: 942/1000\n",
      "[DEBUG] Loss this iteration: 0.002503\n",
      "Encoder training step: 943/1000\n",
      "[DEBUG] Loss this iteration: 0.002536\n",
      "Encoder training step: 944/1000\n",
      "[DEBUG] Loss this iteration: 0.002641\n",
      "Encoder training step: 945/1000\n",
      "[DEBUG] Loss this iteration: 0.002373\n",
      "Encoder training step: 946/1000\n",
      "[DEBUG] Loss this iteration: 0.002560\n",
      "Encoder training step: 947/1000\n",
      "[DEBUG] Loss this iteration: 0.002470\n",
      "Encoder training step: 948/1000\n",
      "[DEBUG] Loss this iteration: 0.002528\n",
      "Encoder training step: 949/1000\n",
      "[DEBUG] Loss this iteration: 0.002525\n",
      "Encoder training step: 950/1000\n",
      "[DEBUG] Loss this iteration: 0.002800\n",
      "Encoder training step: 951/1000\n",
      "[DEBUG] Loss this iteration: 0.002618\n",
      "Encoder training step: 952/1000\n",
      "[DEBUG] Loss this iteration: 0.002550\n",
      "Encoder training step: 953/1000\n",
      "[DEBUG] Loss this iteration: 0.002528\n",
      "Encoder training step: 954/1000\n",
      "[DEBUG] Loss this iteration: 0.002453\n",
      "Encoder training step: 955/1000\n",
      "[DEBUG] Loss this iteration: 0.002731\n",
      "Encoder training step: 956/1000\n",
      "[DEBUG] Loss this iteration: 0.002356\n",
      "Encoder training step: 957/1000\n",
      "[DEBUG] Loss this iteration: 0.002280\n",
      "Encoder training step: 958/1000\n",
      "[DEBUG] Loss this iteration: 0.002179\n",
      "Encoder training step: 959/1000\n",
      "[DEBUG] Loss this iteration: 0.002632\n",
      "Encoder training step: 960/1000\n",
      "[DEBUG] Loss this iteration: 0.002646\n",
      "Encoder training step: 961/1000\n",
      "[DEBUG] Loss this iteration: 0.002618\n",
      "Encoder training step: 962/1000\n",
      "[DEBUG] Loss this iteration: 0.002623\n",
      "Encoder training step: 963/1000\n",
      "[DEBUG] Loss this iteration: 0.002427\n",
      "Encoder training step: 964/1000\n",
      "[DEBUG] Loss this iteration: 0.002407\n",
      "Encoder training step: 965/1000\n",
      "[DEBUG] Loss this iteration: 0.002440\n",
      "Encoder training step: 966/1000\n",
      "[DEBUG] Loss this iteration: 0.002343\n",
      "Encoder training step: 967/1000\n",
      "[DEBUG] Loss this iteration: 0.002321\n",
      "Encoder training step: 968/1000\n",
      "[DEBUG] Loss this iteration: 0.002239\n",
      "Encoder training step: 969/1000\n",
      "[DEBUG] Loss this iteration: 0.002259\n",
      "Encoder training step: 970/1000\n",
      "[DEBUG] Loss this iteration: 0.002360\n",
      "Encoder training step: 971/1000\n",
      "[DEBUG] Loss this iteration: 0.002379\n",
      "Encoder training step: 972/1000\n",
      "[DEBUG] Loss this iteration: 0.002238\n",
      "Encoder training step: 973/1000\n",
      "[DEBUG] Loss this iteration: 0.002444\n",
      "Encoder training step: 974/1000\n",
      "[DEBUG] Loss this iteration: 0.002340\n",
      "Encoder training step: 975/1000\n",
      "[DEBUG] Loss this iteration: 0.002375\n",
      "Encoder training step: 976/1000\n",
      "[DEBUG] Loss this iteration: 0.002565\n",
      "Encoder training step: 977/1000\n",
      "[DEBUG] Loss this iteration: 0.002565\n",
      "Encoder training step: 978/1000\n",
      "[DEBUG] Loss this iteration: 0.002819\n",
      "Encoder training step: 979/1000\n",
      "[DEBUG] Loss this iteration: 0.002976\n",
      "Encoder training step: 980/1000\n",
      "[DEBUG] Loss this iteration: 0.003229\n",
      "Encoder training step: 981/1000\n",
      "[DEBUG] Loss this iteration: 0.002954\n",
      "Encoder training step: 982/1000\n",
      "[DEBUG] Loss this iteration: 0.002564\n",
      "Encoder training step: 983/1000\n",
      "[DEBUG] Loss this iteration: 0.002515\n",
      "Encoder training step: 984/1000\n",
      "[DEBUG] Loss this iteration: 0.002533\n",
      "Encoder training step: 985/1000\n",
      "[DEBUG] Loss this iteration: 0.002442\n",
      "Encoder training step: 986/1000\n",
      "[DEBUG] Loss this iteration: 0.002306\n",
      "Encoder training step: 987/1000\n",
      "[DEBUG] Loss this iteration: 0.002844\n",
      "Encoder training step: 988/1000\n",
      "[DEBUG] Loss this iteration: 0.002885\n",
      "Encoder training step: 989/1000\n",
      "[DEBUG] Loss this iteration: 0.002677\n",
      "Encoder training step: 990/1000\n",
      "[DEBUG] Loss this iteration: 0.002494\n",
      "Encoder training step: 991/1000\n",
      "[DEBUG] Loss this iteration: 0.002324\n",
      "Encoder training step: 992/1000\n",
      "[DEBUG] Loss this iteration: 0.002028\n",
      "Encoder training step: 993/1000\n",
      "[DEBUG] Loss this iteration: 0.002419\n",
      "Encoder training step: 994/1000\n",
      "[DEBUG] Loss this iteration: 0.002757\n",
      "Encoder training step: 995/1000\n",
      "[DEBUG] Loss this iteration: 0.002400\n",
      "Encoder training step: 996/1000\n",
      "[DEBUG] Loss this iteration: 0.002413\n",
      "Encoder training step: 997/1000\n",
      "[DEBUG] Loss this iteration: 0.002202\n",
      "Encoder training step: 998/1000\n",
      "[DEBUG] Loss this iteration: 0.002200\n",
      "Encoder training step: 999/1000\n",
      "Loss S:  tensor(0.0932, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 0/1000\n",
      "Loss S:  tensor(0.0849, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 1/1000\n",
      "Loss S:  tensor(0.0777, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 2/1000\n",
      "Loss S:  tensor(0.0704, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 3/1000\n",
      "Loss S:  tensor(0.0643, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 4/1000\n",
      "Loss S:  tensor(0.0566, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 5/1000\n",
      "Loss S:  tensor(0.0538, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 6/1000\n",
      "Loss S:  tensor(0.0489, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 7/1000\n",
      "Loss S:  tensor(0.0441, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 8/1000\n",
      "Loss S:  tensor(0.0415, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 9/1000\n",
      "Loss S:  tensor(0.0383, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 10/1000\n",
      "Loss S:  tensor(0.0352, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 11/1000\n",
      "Loss S:  tensor(0.0360, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 12/1000\n",
      "Loss S:  tensor(0.0314, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 13/1000\n",
      "Loss S:  tensor(0.0299, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 14/1000\n",
      "Loss S:  tensor(0.0302, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 15/1000\n",
      "Loss S:  tensor(0.0269, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 16/1000\n",
      "Loss S:  tensor(0.0270, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 17/1000\n",
      "Loss S:  tensor(0.0286, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 18/1000\n",
      "Loss S:  tensor(0.0241, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 19/1000\n",
      "Loss S:  tensor(0.0252, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 20/1000\n",
      "Loss S:  tensor(0.0223, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 21/1000\n",
      "Loss S:  tensor(0.0221, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 22/1000\n",
      "Loss S:  tensor(0.0230, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 23/1000\n",
      "Loss S:  tensor(0.0232, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 24/1000\n",
      "Loss S:  tensor(0.0233, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 25/1000\n",
      "Loss S:  tensor(0.0224, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 26/1000\n",
      "Loss S:  tensor(0.0205, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 27/1000\n",
      "Loss S:  tensor(0.0197, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 28/1000\n",
      "Loss S:  tensor(0.0195, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 29/1000\n",
      "Loss S:  tensor(0.0195, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 30/1000\n",
      "Loss S:  tensor(0.0184, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 31/1000\n",
      "Loss S:  tensor(0.0168, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 32/1000\n",
      "Loss S:  tensor(0.0178, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 33/1000\n",
      "Loss S:  tensor(0.0161, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 34/1000\n",
      "Loss S:  tensor(0.0202, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 35/1000\n",
      "Loss S:  tensor(0.0167, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 36/1000\n",
      "Loss S:  tensor(0.0148, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 37/1000\n",
      "Loss S:  tensor(0.0180, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 38/1000\n",
      "Loss S:  tensor(0.0208, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 39/1000\n",
      "Loss S:  tensor(0.0167, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 40/1000\n",
      "Loss S:  tensor(0.0178, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 41/1000\n",
      "Loss S:  tensor(0.0146, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 42/1000\n",
      "Loss S:  tensor(0.0138, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 43/1000\n",
      "Loss S:  tensor(0.0166, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 44/1000\n",
      "Loss S:  tensor(0.0154, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 45/1000\n",
      "Loss S:  tensor(0.0140, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 46/1000\n",
      "Loss S:  tensor(0.0155, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 47/1000\n",
      "Loss S:  tensor(0.0161, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 48/1000\n",
      "Loss S:  tensor(0.0157, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 49/1000\n",
      "Loss S:  tensor(0.0143, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 50/1000\n",
      "Loss S:  tensor(0.0135, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 51/1000\n",
      "Loss S:  tensor(0.0172, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 52/1000\n",
      "Loss S:  tensor(0.0120, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 53/1000\n",
      "Loss S:  tensor(0.0156, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 54/1000\n",
      "Loss S:  tensor(0.0154, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 55/1000\n",
      "Loss S:  tensor(0.0150, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 56/1000\n",
      "Loss S:  tensor(0.0145, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 57/1000\n",
      "Loss S:  tensor(0.0167, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 58/1000\n",
      "Loss S:  tensor(0.0131, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 59/1000\n",
      "Loss S:  tensor(0.0141, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 60/1000\n",
      "Loss S:  tensor(0.0147, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 61/1000\n",
      "Loss S:  tensor(0.0136, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 62/1000\n",
      "Loss S:  tensor(0.0155, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 63/1000\n",
      "Loss S:  tensor(0.0121, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 64/1000\n",
      "Loss S:  tensor(0.0141, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 65/1000\n",
      "Loss S:  tensor(0.0148, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 66/1000\n",
      "Loss S:  tensor(0.0121, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 67/1000\n",
      "Loss S:  tensor(0.0143, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 68/1000\n",
      "Loss S:  tensor(0.0118, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 69/1000\n",
      "Loss S:  tensor(0.0123, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 70/1000\n",
      "Loss S:  tensor(0.0120, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 71/1000\n",
      "Loss S:  tensor(0.0140, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 72/1000\n",
      "Loss S:  tensor(0.0123, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 73/1000\n",
      "Loss S:  tensor(0.0140, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 74/1000\n",
      "Loss S:  tensor(0.0116, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 75/1000\n",
      "Loss S:  tensor(0.0147, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 76/1000\n",
      "Loss S:  tensor(0.0112, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 77/1000\n",
      "Loss S:  tensor(0.0126, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 78/1000\n",
      "Loss S:  tensor(0.0137, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 79/1000\n",
      "Loss S:  tensor(0.0167, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 80/1000\n",
      "Loss S:  tensor(0.0162, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 81/1000\n",
      "Loss S:  tensor(0.0142, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 82/1000\n",
      "Loss S:  tensor(0.0110, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 83/1000\n",
      "Loss S:  tensor(0.0135, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 84/1000\n",
      "Loss S:  tensor(0.0115, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 85/1000\n",
      "Loss S:  tensor(0.0116, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 86/1000\n",
      "Loss S:  tensor(0.0131, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 87/1000\n",
      "Loss S:  tensor(0.0117, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 88/1000\n",
      "Loss S:  tensor(0.0159, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 89/1000\n",
      "Loss S:  tensor(0.0141, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 90/1000\n",
      "Loss S:  tensor(0.0129, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 91/1000\n",
      "Loss S:  tensor(0.0098, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 92/1000\n",
      "Loss S:  tensor(0.0112, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 93/1000\n",
      "Loss S:  tensor(0.0103, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 94/1000\n",
      "Loss S:  tensor(0.0149, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 95/1000\n",
      "Loss S:  tensor(0.0170, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 96/1000\n",
      "Loss S:  tensor(0.0141, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 97/1000\n",
      "Loss S:  tensor(0.0151, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 98/1000\n",
      "Loss S:  tensor(0.0136, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 99/1000\n",
      "Loss S:  tensor(0.0131, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 100/1000\n",
      "Loss S:  tensor(0.0126, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 101/1000\n",
      "Loss S:  tensor(0.0117, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 102/1000\n",
      "Loss S:  tensor(0.0110, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 103/1000\n",
      "Loss S:  tensor(0.0105, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 104/1000\n",
      "Loss S:  tensor(0.0121, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 105/1000\n",
      "Loss S:  tensor(0.0133, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 106/1000\n",
      "Loss S:  tensor(0.0129, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 107/1000\n",
      "Loss S:  tensor(0.0131, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 108/1000\n",
      "Loss S:  tensor(0.0101, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 109/1000\n",
      "Loss S:  tensor(0.0098, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 110/1000\n",
      "Loss S:  tensor(0.0113, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 111/1000\n",
      "Loss S:  tensor(0.0129, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 112/1000\n",
      "Loss S:  tensor(0.0123, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 113/1000\n",
      "Loss S:  tensor(0.0112, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 114/1000\n",
      "Loss S:  tensor(0.0126, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 115/1000\n",
      "Loss S:  tensor(0.0140, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 116/1000\n",
      "Loss S:  tensor(0.0144, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 117/1000\n",
      "Loss S:  tensor(0.0091, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 118/1000\n",
      "Loss S:  tensor(0.0128, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 119/1000\n",
      "Loss S:  tensor(0.0101, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 120/1000\n",
      "Loss S:  tensor(0.0110, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 121/1000\n",
      "Loss S:  tensor(0.0096, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 122/1000\n",
      "Loss S:  tensor(0.0125, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 123/1000\n",
      "Loss S:  tensor(0.0110, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 124/1000\n",
      "Loss S:  tensor(0.0114, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 125/1000\n",
      "Loss S:  tensor(0.0137, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 126/1000\n",
      "Loss S:  tensor(0.0113, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 127/1000\n",
      "Loss S:  tensor(0.0104, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 128/1000\n",
      "Loss S:  tensor(0.0127, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 129/1000\n",
      "Loss S:  tensor(0.0125, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 130/1000\n",
      "Loss S:  tensor(0.0127, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 131/1000\n",
      "Loss S:  tensor(0.0125, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 132/1000\n",
      "Loss S:  tensor(0.0116, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 133/1000\n",
      "Loss S:  tensor(0.0087, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 134/1000\n",
      "Loss S:  tensor(0.0112, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 135/1000\n",
      "Loss S:  tensor(0.0088, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 136/1000\n",
      "Loss S:  tensor(0.0107, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 137/1000\n",
      "Loss S:  tensor(0.0106, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 138/1000\n",
      "Loss S:  tensor(0.0105, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 139/1000\n",
      "Loss S:  tensor(0.0127, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 140/1000\n",
      "Loss S:  tensor(0.0104, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 141/1000\n",
      "Loss S:  tensor(0.0099, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 142/1000\n",
      "Loss S:  tensor(0.0112, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 143/1000\n",
      "Loss S:  tensor(0.0100, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 144/1000\n",
      "Loss S:  tensor(0.0119, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 145/1000\n",
      "Loss S:  tensor(0.0114, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 146/1000\n",
      "Loss S:  tensor(0.0131, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 147/1000\n",
      "Loss S:  tensor(0.0108, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 148/1000\n",
      "Loss S:  tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 149/1000\n",
      "Loss S:  tensor(0.0085, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 150/1000\n",
      "Loss S:  tensor(0.0087, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 151/1000\n",
      "Loss S:  tensor(0.0106, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 152/1000\n",
      "Loss S:  tensor(0.0086, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 153/1000\n",
      "Loss S:  tensor(0.0095, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 154/1000\n",
      "Loss S:  tensor(0.0119, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 155/1000\n",
      "Loss S:  tensor(0.0095, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 156/1000\n",
      "Loss S:  tensor(0.0083, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 157/1000\n",
      "Loss S:  tensor(0.0091, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 158/1000\n",
      "Loss S:  tensor(0.0105, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 159/1000\n",
      "Loss S:  tensor(0.0086, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 160/1000\n",
      "Loss S:  tensor(0.0082, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 161/1000\n",
      "Loss S:  tensor(0.0100, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 162/1000\n",
      "Loss S:  tensor(0.0085, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 163/1000\n",
      "Loss S:  tensor(0.0084, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 164/1000\n",
      "Loss S:  tensor(0.0069, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 165/1000\n",
      "Loss S:  tensor(0.0093, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 166/1000\n",
      "Loss S:  tensor(0.0087, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 167/1000\n",
      "Loss S:  tensor(0.0086, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 168/1000\n",
      "Loss S:  tensor(0.0082, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 169/1000\n",
      "Loss S:  tensor(0.0092, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 170/1000\n",
      "Loss S:  tensor(0.0081, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 171/1000\n",
      "Loss S:  tensor(0.0078, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 172/1000\n",
      "Loss S:  tensor(0.0099, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 173/1000\n",
      "Loss S:  tensor(0.0087, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 174/1000\n",
      "Loss S:  tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 175/1000\n",
      "Loss S:  tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 176/1000\n",
      "Loss S:  tensor(0.0085, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 177/1000\n",
      "Loss S:  tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 178/1000\n",
      "Loss S:  tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 179/1000\n",
      "Loss S:  tensor(0.0070, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 180/1000\n",
      "Loss S:  tensor(0.0078, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 181/1000\n",
      "Loss S:  tensor(0.0073, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 182/1000\n",
      "Loss S:  tensor(0.0077, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 183/1000\n",
      "Loss S:  tensor(0.0065, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 184/1000\n",
      "Loss S:  tensor(0.0070, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 185/1000\n",
      "Loss S:  tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 186/1000\n",
      "Loss S:  tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 187/1000\n",
      "Loss S:  tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 188/1000\n",
      "Loss S:  tensor(0.0073, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 189/1000\n",
      "Loss S:  tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 190/1000\n",
      "Loss S:  tensor(0.0069, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 191/1000\n",
      "Loss S:  tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 192/1000\n",
      "Loss S:  tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 193/1000\n",
      "Loss S:  tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 194/1000\n",
      "Loss S:  tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 195/1000\n",
      "Loss S:  tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 196/1000\n",
      "Loss S:  tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 197/1000\n",
      "Loss S:  tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 198/1000\n",
      "Loss S:  tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 199/1000\n",
      "Loss S:  tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 200/1000\n",
      "Loss S:  tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 201/1000\n",
      "Loss S:  tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 202/1000\n",
      "Loss S:  tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 203/1000\n",
      "Loss S:  tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 204/1000\n",
      "Loss S:  tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 205/1000\n",
      "Loss S:  tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 206/1000\n",
      "Loss S:  tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 207/1000\n",
      "Loss S:  tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 208/1000\n",
      "Loss S:  tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 209/1000\n",
      "Loss S:  tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 210/1000\n",
      "Loss S:  tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 211/1000\n",
      "Loss S:  tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 212/1000\n",
      "Loss S:  tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 213/1000\n",
      "Loss S:  tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 214/1000\n",
      "Loss S:  tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 215/1000\n",
      "Loss S:  tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 216/1000\n",
      "Loss S:  tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 217/1000\n",
      "Loss S:  tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 218/1000\n",
      "Loss S:  tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 219/1000\n",
      "Loss S:  tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 220/1000\n",
      "Loss S:  tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 221/1000\n",
      "Loss S:  tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 222/1000\n",
      "Loss S:  tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 223/1000\n",
      "Loss S:  tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 224/1000\n",
      "Loss S:  tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 225/1000\n",
      "Loss S:  tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 226/1000\n",
      "Loss S:  tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 227/1000\n",
      "Loss S:  tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 228/1000\n",
      "Loss S:  tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 229/1000\n",
      "Loss S:  tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 230/1000\n",
      "Loss S:  tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 231/1000\n",
      "Loss S:  tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 232/1000\n",
      "Loss S:  tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 233/1000\n",
      "Loss S:  tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 234/1000\n",
      "Loss S:  tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 235/1000\n",
      "Loss S:  tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 236/1000\n",
      "Loss S:  tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 237/1000\n",
      "Loss S:  tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 238/1000\n",
      "Loss S:  tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 239/1000\n",
      "Loss S:  tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 240/1000\n",
      "Loss S:  tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 241/1000\n",
      "Loss S:  tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 242/1000\n",
      "Loss S:  tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 243/1000\n",
      "Loss S:  tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 244/1000\n",
      "Loss S:  tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 245/1000\n",
      "Loss S:  tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 246/1000\n",
      "Loss S:  tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 247/1000\n",
      "Loss S:  tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 248/1000\n",
      "Loss S:  tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 249/1000\n",
      "Loss S:  tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 250/1000\n",
      "Loss S:  tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 251/1000\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 252/1000\n",
      "Loss S:  tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 253/1000\n",
      "Loss S:  tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 254/1000\n",
      "Loss S:  tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 255/1000\n",
      "Loss S:  tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 256/1000\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 257/1000\n",
      "Loss S:  tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 258/1000\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 259/1000\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 260/1000\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 261/1000\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 262/1000\n",
      "Loss S:  tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 263/1000\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 264/1000\n",
      "Loss S:  tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 265/1000\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 266/1000\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 267/1000\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 268/1000\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 269/1000\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 270/1000\n",
      "Loss S:  tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 271/1000\n",
      "Loss S:  tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 272/1000\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 273/1000\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 274/1000\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 275/1000\n",
      "Loss S:  tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 276/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 277/1000\n",
      "Loss S:  tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 278/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 279/1000\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 280/1000\n",
      "Loss S:  tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 281/1000\n",
      "Loss S:  tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 282/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 283/1000\n",
      "Loss S:  tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 284/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 285/1000\n",
      "Loss S:  tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 286/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 287/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 288/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 289/1000\n",
      "Loss S:  tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 290/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 291/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 292/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 293/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 294/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 295/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 296/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 297/1000\n",
      "Loss S:  tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 298/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 299/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 300/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 301/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 302/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 303/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 304/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 305/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 306/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 307/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 308/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 309/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 310/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 311/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 312/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 313/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 314/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 315/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 316/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 317/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 318/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 319/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 320/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 321/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 322/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 323/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 324/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 325/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 326/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 327/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 328/1000\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 329/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 330/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 331/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 332/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 333/1000\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 334/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 335/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 336/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 337/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 338/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 339/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 340/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 341/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 342/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 343/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 344/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 345/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 346/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 347/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 348/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 349/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 350/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 351/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 352/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 353/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 354/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 355/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 356/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 357/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 358/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 359/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 360/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 361/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 362/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 363/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 364/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 365/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 366/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 367/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 368/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 369/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 370/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 371/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 372/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 373/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 374/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 375/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 376/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 377/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 378/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 379/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 380/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 381/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 382/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 383/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 384/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 385/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 386/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 387/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 388/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 389/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 390/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 391/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 392/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 393/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 394/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 395/1000\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 396/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 397/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 398/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 399/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 400/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 401/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 402/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 403/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 404/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 405/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 406/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 407/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 408/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 409/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 410/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 411/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 412/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 413/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 414/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 415/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 416/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 417/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 418/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 419/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 420/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 421/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 422/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 423/1000\n",
      "Loss S:  tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 424/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 425/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 426/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 427/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 428/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 429/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 430/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 431/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 432/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 433/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 434/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 435/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 436/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 437/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 438/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 439/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 440/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 441/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 442/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 443/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 444/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 445/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 446/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 447/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 448/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 449/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 450/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 451/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 452/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 453/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 454/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 455/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 456/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 457/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 458/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 459/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 460/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 461/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 462/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 463/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 464/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 465/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 466/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 467/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 468/1000\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 469/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 470/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 471/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 472/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 473/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 474/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 475/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 476/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 477/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 478/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 479/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 480/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 481/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 482/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 483/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 484/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 485/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 486/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 487/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 488/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 489/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 490/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 491/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 492/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 493/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 494/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 495/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 496/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 497/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 498/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 499/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 500/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 501/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 502/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 503/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 504/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 505/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 506/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 507/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 508/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 509/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 510/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 511/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 512/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 513/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 514/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 515/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 516/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 517/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 518/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 519/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 520/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 521/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 522/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 523/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 524/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 525/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 526/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 527/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 528/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 529/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 530/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 531/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 532/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 533/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 534/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 535/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 536/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 537/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 538/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 539/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 540/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 541/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 542/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 543/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 544/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 545/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 546/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 547/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 548/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 549/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 550/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 551/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 552/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 553/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 554/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 555/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 556/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 557/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 558/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 559/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 560/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 561/1000\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 562/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 563/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 564/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 565/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 566/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 567/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 568/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 569/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 570/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 571/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 572/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 573/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 574/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 575/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 576/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 577/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 578/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 579/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 580/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 581/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 582/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 583/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 584/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 585/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 586/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 587/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 588/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 589/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 590/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 591/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 592/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 593/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 594/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 595/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 596/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 597/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 598/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 599/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 600/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 601/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 602/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 603/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 604/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 605/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 606/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 607/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 608/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 609/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 610/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 611/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 612/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 613/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 614/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 615/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 616/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 617/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 618/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 619/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 620/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 621/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 622/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 623/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 624/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 625/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 626/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 627/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 628/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 629/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 630/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 631/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 632/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 633/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 634/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 635/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 636/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 637/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 638/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 639/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 640/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 641/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 642/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 643/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 644/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 645/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 646/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 647/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 648/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 649/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 650/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 651/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 652/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 653/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 654/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 655/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 656/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 657/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 658/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 659/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 660/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 661/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 662/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 663/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 664/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 665/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 666/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 667/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 668/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 669/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 670/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 671/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 672/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 673/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 674/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 675/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 676/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 677/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 678/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 679/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 680/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 681/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 682/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 683/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 684/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 685/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 686/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 687/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 688/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 689/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 690/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 691/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 692/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 693/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 694/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 695/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 696/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 697/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 698/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 699/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 700/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 701/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 702/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 703/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 704/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 705/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 706/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 707/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 708/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 709/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 710/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 711/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 712/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 713/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 714/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 715/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 716/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 717/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 718/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 719/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 720/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 721/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 722/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 723/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 724/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 725/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 726/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 727/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 728/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 729/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 730/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 731/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 732/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 733/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 734/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 735/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 736/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 737/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 738/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 739/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 740/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 741/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 742/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 743/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 744/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 745/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 746/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 747/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 748/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 749/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 750/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 751/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 752/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 753/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 754/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 755/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 756/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 757/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 758/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 759/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 760/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 761/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 762/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 763/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 764/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 765/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 766/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 767/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 768/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 769/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 770/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 771/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 772/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 773/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 774/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 775/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 776/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 777/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 778/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 779/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 780/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 781/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 782/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 783/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 784/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 785/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 786/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 787/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 788/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 789/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 790/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 791/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 792/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 793/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 794/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 795/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 796/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 797/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 798/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 799/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 800/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 801/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 802/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 803/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 804/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 805/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 806/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 807/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 808/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 809/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 810/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 811/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 812/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 813/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 814/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 815/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 816/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 817/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 818/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 819/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 820/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 821/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 822/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 823/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 824/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 825/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 826/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 827/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 828/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 829/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 830/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 831/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 832/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 833/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 834/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 835/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 836/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 837/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 838/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 839/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 840/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 841/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 842/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 843/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 844/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 845/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 846/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 847/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 848/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 849/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 850/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 851/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 852/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 853/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 854/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 855/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 856/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 857/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 858/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 859/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 860/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 861/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 862/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 863/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 864/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 865/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 866/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 867/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 868/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 869/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 870/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 871/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 872/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 873/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 874/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 875/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 876/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 877/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 878/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 879/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 880/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 881/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 882/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 883/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 884/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 885/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 886/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 887/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 888/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 889/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 890/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 891/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 892/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 893/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 894/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 895/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 896/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 897/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 898/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 899/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 900/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 901/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 902/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 903/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 904/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 905/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 906/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 907/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 908/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 909/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 910/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 911/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 912/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 913/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 914/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 915/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 916/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 917/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 918/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 919/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 920/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 921/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 922/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 923/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 924/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 925/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 926/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 927/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 928/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 929/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 930/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 931/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 932/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 933/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 934/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 935/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 936/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 937/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 938/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 939/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 940/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 941/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 942/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 943/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 944/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 945/1000\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 946/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 947/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 948/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 949/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 950/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 951/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 952/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 953/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 954/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 955/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 956/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 957/1000\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 958/1000\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 959/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 960/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 961/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 962/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 963/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 964/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 965/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 966/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 967/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 968/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 969/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 970/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 971/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 972/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 973/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 974/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 975/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 976/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 977/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 978/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 979/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 980/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 981/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 982/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 983/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 984/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 985/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 986/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 987/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 988/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 989/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 990/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 991/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 992/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 993/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 994/1000\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 995/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 996/1000\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 997/1000\n",
      "Loss S:  tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 998/1000\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 999/1000\n",
      "Loss G (total):  tensor(11.0918, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 0/1000\n",
      "Loss G (total):  tensor(19.3010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 1/1000\n",
      "Loss G (total):  tensor(14.2374, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 2/1000\n",
      "Loss G (total):  tensor(13.6584, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 3/1000\n",
      "Loss G (total):  tensor(13.0992, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 4/1000\n",
      "Loss G (total):  tensor(11.8047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 5/1000\n",
      "Loss G (total):  tensor(14.0933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 6/1000\n",
      "Loss G (total):  tensor(15.2149, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 7/1000\n",
      "Loss G (total):  tensor(14.9517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 8/1000\n",
      "Loss G (total):  tensor(14.9822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 9/1000\n",
      "Loss G (total):  tensor(15.7884, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 10/1000\n",
      "Loss G (total):  tensor(17.5909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 11/1000\n",
      "Loss G (total):  tensor(18.0896, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 12/1000\n",
      "Loss G (total):  tensor(19.1980, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 13/1000\n",
      "Loss G (total):  tensor(21.4702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 14/1000\n",
      "Loss G (total):  tensor(20.1019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 15/1000\n",
      "Loss G (total):  tensor(21.0759, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 16/1000\n",
      "Loss G (total):  tensor(22.0163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 17/1000\n",
      "Loss G (total):  tensor(21.3839, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 18/1000\n",
      "Loss G (total):  tensor(21.9360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 19/1000\n",
      "Loss G (total):  tensor(22.9428, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 20/1000\n",
      "Loss G (total):  tensor(25.5511, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 21/1000\n",
      "Loss G (total):  tensor(23.8968, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 22/1000\n",
      "Loss G (total):  tensor(23.8479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 23/1000\n",
      "Loss G (total):  tensor(22.8173, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 24/1000\n",
      "Loss G (total):  tensor(24.4038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 25/1000\n",
      "Loss G (total):  tensor(26.4587, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 26/1000\n",
      "Loss G (total):  tensor(25.5229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 27/1000\n",
      "Loss G (total):  tensor(25.6955, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 28/1000\n",
      "Loss G (total):  tensor(26.1699, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 29/1000\n",
      "Loss G (total):  tensor(25.2621, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 30/1000\n",
      "Loss G (total):  tensor(26.5434, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 31/1000\n",
      "Loss G (total):  tensor(26.1169, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 32/1000\n",
      "Loss G (total):  tensor(24.9415, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 33/1000\n",
      "Loss G (total):  tensor(25.0569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 34/1000\n",
      "Loss G (total):  tensor(27.0151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 35/1000\n",
      "Loss G (total):  tensor(26.5577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 36/1000\n",
      "Loss G (total):  tensor(25.2638, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 37/1000\n",
      "Loss G (total):  tensor(26.5605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 38/1000\n",
      "Loss G (total):  tensor(26.8513, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 39/1000\n",
      "Loss G (total):  tensor(29.3545, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 40/1000\n",
      "Loss G (total):  tensor(26.4378, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 41/1000\n",
      "Loss G (total):  tensor(24.9212, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 42/1000\n",
      "Loss G (total):  tensor(26.1152, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 43/1000\n",
      "Loss G (total):  tensor(26.3767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 44/1000\n",
      "Loss G (total):  tensor(26.4155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 45/1000\n",
      "Loss G (total):  tensor(26.5769, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 46/1000\n",
      "Loss G (total):  tensor(27.5328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 47/1000\n",
      "Loss G (total):  tensor(26.4695, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 48/1000\n",
      "Loss G (total):  tensor(27.0402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 49/1000\n",
      "Loss G (total):  tensor(28.2357, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 50/1000\n",
      "Loss G (total):  tensor(27.2938, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 51/1000\n",
      "Loss G (total):  tensor(27.9419, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 52/1000\n",
      "Loss G (total):  tensor(28.9502, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 53/1000\n",
      "Loss G (total):  tensor(27.4972, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 54/1000\n",
      "Loss G (total):  tensor(26.8642, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 55/1000\n",
      "Loss G (total):  tensor(30.4782, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 56/1000\n",
      "Loss G (total):  tensor(27.7302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 57/1000\n",
      "Loss G (total):  tensor(28.5786, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 58/1000\n",
      "Loss G (total):  tensor(29.5373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 59/1000\n",
      "Loss G (total):  tensor(30.5124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 60/1000\n",
      "Loss G (total):  tensor(29.5372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 61/1000\n",
      "Loss G (total):  tensor(28.4703, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 62/1000\n",
      "Loss G (total):  tensor(28.9165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 63/1000\n",
      "Loss G (total):  tensor(31.8186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 64/1000\n",
      "Loss G (total):  tensor(29.2876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 65/1000\n",
      "Loss G (total):  tensor(29.4889, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 66/1000\n",
      "Loss G (total):  tensor(29.4260, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 67/1000\n",
      "Loss G (total):  tensor(29.1088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 68/1000\n",
      "Loss G (total):  tensor(30.0070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 69/1000\n",
      "Loss G (total):  tensor(29.9565, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 70/1000\n",
      "Loss G (total):  tensor(33.1590, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 71/1000\n",
      "Loss G (total):  tensor(29.9797, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 72/1000\n",
      "Loss G (total):  tensor(29.4255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 73/1000\n",
      "Loss G (total):  tensor(29.3908, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 74/1000\n",
      "Loss G (total):  tensor(31.0851, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 75/1000\n",
      "Loss G (total):  tensor(30.6077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 76/1000\n",
      "Loss G (total):  tensor(30.6599, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 77/1000\n",
      "Loss G (total):  tensor(30.0459, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 78/1000\n",
      "Loss G (total):  tensor(30.6591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 79/1000\n",
      "Loss G (total):  tensor(31.4559, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 80/1000\n",
      "Loss G (total):  tensor(30.8713, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 81/1000\n",
      "Loss G (total):  tensor(30.5064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 82/1000\n",
      "Loss G (total):  tensor(31.5620, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 83/1000\n",
      "Loss G (total):  tensor(32.4066, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 84/1000\n",
      "Loss G (total):  tensor(32.3230, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 85/1000\n",
      "Loss G (total):  tensor(31.4150, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 86/1000\n",
      "Loss G (total):  tensor(30.1437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 87/1000\n",
      "Loss G (total):  tensor(33.7915, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 88/1000\n",
      "Loss G (total):  tensor(34.1170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 89/1000\n",
      "Loss G (total):  tensor(31.4901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 90/1000\n",
      "Loss G (total):  tensor(32.6003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 91/1000\n",
      "Loss G (total):  tensor(34.1840, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 92/1000\n",
      "Loss G (total):  tensor(31.7697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 93/1000\n",
      "Loss G (total):  tensor(33.0404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 94/1000\n",
      "Loss G (total):  tensor(33.5522, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 95/1000\n",
      "Loss G (total):  tensor(32.4478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 96/1000\n",
      "Loss G (total):  tensor(35.6678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 97/1000\n",
      "Loss G (total):  tensor(33.3667, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 98/1000\n",
      "Loss G (total):  tensor(34.3402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 99/1000\n",
      "Loss G (total):  tensor(35.1036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 100/1000\n",
      "Loss G (total):  tensor(35.7277, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 101/1000\n",
      "Loss G (total):  tensor(33.7806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 102/1000\n",
      "Loss G (total):  tensor(36.4899, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 103/1000\n",
      "Loss G (total):  tensor(35.8710, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 104/1000\n",
      "Loss G (total):  tensor(36.2778, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 105/1000\n",
      "Loss G (total):  tensor(34.1412, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 106/1000\n",
      "Loss G (total):  tensor(36.8137, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 107/1000\n",
      "Loss G (total):  tensor(35.3664, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 108/1000\n",
      "Loss G (total):  tensor(35.7605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 109/1000\n",
      "Loss G (total):  tensor(34.9186, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 110/1000\n",
      "Loss G (total):  tensor(35.9764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 111/1000\n",
      "Loss G (total):  tensor(38.2618, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 112/1000\n",
      "Loss G (total):  tensor(38.2314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 113/1000\n",
      "Loss G (total):  tensor(37.2246, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 114/1000\n",
      "Loss G (total):  tensor(35.8645, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 115/1000\n",
      "Loss G (total):  tensor(35.9636, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 116/1000\n",
      "Loss G (total):  tensor(37.3093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 117/1000\n",
      "Loss G (total):  tensor(37.7116, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 118/1000\n",
      "Loss G (total):  tensor(37.3559, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 119/1000\n",
      "Loss G (total):  tensor(40.2775, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 120/1000\n",
      "Loss G (total):  tensor(38.2597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 121/1000\n",
      "Loss G (total):  tensor(36.9405, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 122/1000\n",
      "Loss G (total):  tensor(38.2539, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 123/1000\n",
      "Loss G (total):  tensor(38.6850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 124/1000\n",
      "Loss G (total):  tensor(39.2931, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 125/1000\n",
      "Loss G (total):  tensor(38.1431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 126/1000\n",
      "Loss G (total):  tensor(39.4755, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 127/1000\n",
      "Loss G (total):  tensor(38.3951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 128/1000\n",
      "Loss G (total):  tensor(40.9457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 129/1000\n",
      "Loss G (total):  tensor(39.9207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 130/1000\n",
      "Loss G (total):  tensor(39.3005, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 131/1000\n",
      "Loss G (total):  tensor(39.1325, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 132/1000\n",
      "Loss G (total):  tensor(40.4902, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 133/1000\n",
      "Loss G (total):  tensor(39.7707, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 134/1000\n",
      "Loss G (total):  tensor(41.1810, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 135/1000\n",
      "Loss G (total):  tensor(40.2100, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 136/1000\n",
      "Loss G (total):  tensor(40.0423, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 137/1000\n",
      "Loss G (total):  tensor(40.9757, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 138/1000\n",
      "Loss G (total):  tensor(42.8215, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 139/1000\n",
      "Loss G (total):  tensor(43.4268, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 140/1000\n",
      "Loss G (total):  tensor(41.5122, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 141/1000\n",
      "Loss G (total):  tensor(39.8934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 142/1000\n",
      "Loss G (total):  tensor(41.6850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 143/1000\n",
      "Loss G (total):  tensor(43.8500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 144/1000\n",
      "Loss G (total):  tensor(42.3585, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 145/1000\n",
      "Loss G (total):  tensor(43.1227, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 146/1000\n",
      "Loss G (total):  tensor(43.6322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 147/1000\n",
      "Loss G (total):  tensor(44.1070, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 148/1000\n",
      "Loss G (total):  tensor(43.6252, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 149/1000\n",
      "Loss G (total):  tensor(45.5582, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 150/1000\n",
      "Loss G (total):  tensor(45.1674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 151/1000\n",
      "Loss G (total):  tensor(46.1202, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 152/1000\n",
      "Loss G (total):  tensor(44.6813, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 153/1000\n",
      "Loss G (total):  tensor(45.0565, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 154/1000\n",
      "Loss G (total):  tensor(47.0555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 155/1000\n",
      "Loss G (total):  tensor(43.5448, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 156/1000\n",
      "Loss G (total):  tensor(44.7998, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 157/1000\n",
      "Loss G (total):  tensor(46.2645, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 158/1000\n",
      "Loss G (total):  tensor(45.0877, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 159/1000\n",
      "Loss G (total):  tensor(46.6804, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 160/1000\n",
      "Loss G (total):  tensor(47.1909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 161/1000\n",
      "Loss G (total):  tensor(47.2257, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 162/1000\n",
      "Loss G (total):  tensor(47.5160, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 163/1000\n",
      "Loss G (total):  tensor(47.6738, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 164/1000\n",
      "Loss G (total):  tensor(47.1957, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 165/1000\n",
      "Loss G (total):  tensor(46.9180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 166/1000\n",
      "Loss G (total):  tensor(47.2056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 167/1000\n",
      "Loss G (total):  tensor(47.9152, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 168/1000\n",
      "Loss G (total):  tensor(47.2255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 169/1000\n",
      "Loss G (total):  tensor(48.6232, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 170/1000\n",
      "Loss G (total):  tensor(49.2157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 171/1000\n",
      "Loss G (total):  tensor(50.4699, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 172/1000\n",
      "Loss G (total):  tensor(50.1756, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 173/1000\n",
      "Loss G (total):  tensor(49.9388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 174/1000\n",
      "Loss G (total):  tensor(49.7602, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 175/1000\n",
      "Loss G (total):  tensor(49.5955, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 176/1000\n",
      "Loss G (total):  tensor(50.8499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 177/1000\n",
      "Loss G (total):  tensor(51.0014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 178/1000\n",
      "Loss G (total):  tensor(51.2096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 179/1000\n",
      "Loss G (total):  tensor(50.6855, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 180/1000\n",
      "Loss G (total):  tensor(52.0766, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 181/1000\n",
      "Loss G (total):  tensor(51.1713, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 182/1000\n",
      "Loss G (total):  tensor(52.2924, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 183/1000\n",
      "Loss G (total):  tensor(52.1803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 184/1000\n",
      "Loss G (total):  tensor(51.6245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 185/1000\n",
      "Loss G (total):  tensor(52.8624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 186/1000\n",
      "Loss G (total):  tensor(51.7307, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 187/1000\n",
      "Loss G (total):  tensor(53.1076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 188/1000\n",
      "Loss G (total):  tensor(54.2774, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 189/1000\n",
      "Loss G (total):  tensor(54.3105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 190/1000\n",
      "Loss G (total):  tensor(54.8650, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 191/1000\n",
      "Loss G (total):  tensor(53.6552, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 192/1000\n",
      "Loss G (total):  tensor(53.9533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 193/1000\n",
      "Loss G (total):  tensor(54.4736, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 194/1000\n",
      "Loss G (total):  tensor(54.3717, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 195/1000\n",
      "Loss G (total):  tensor(54.1685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 196/1000\n",
      "Loss G (total):  tensor(54.6042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 197/1000\n",
      "Loss G (total):  tensor(57.9244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 198/1000\n",
      "Loss G (total):  tensor(57.1614, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 199/1000\n",
      "Loss G (total):  tensor(57.1012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 200/1000\n",
      "Loss G (total):  tensor(56.3067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 201/1000\n",
      "Loss G (total):  tensor(56.1833, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 202/1000\n",
      "Loss G (total):  tensor(54.6790, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 203/1000\n",
      "Loss G (total):  tensor(54.8075, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 204/1000\n",
      "Loss G (total):  tensor(57.4306, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 205/1000\n",
      "Loss G (total):  tensor(58.4073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 206/1000\n",
      "Loss G (total):  tensor(57.9528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 207/1000\n",
      "Loss G (total):  tensor(59.1876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 208/1000\n",
      "Loss G (total):  tensor(60.0413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 209/1000\n",
      "Loss G (total):  tensor(58.4656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 210/1000\n",
      "Loss G (total):  tensor(58.5226, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 211/1000\n",
      "Loss G (total):  tensor(60.8609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 212/1000\n",
      "Loss G (total):  tensor(60.8900, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 213/1000\n",
      "Loss G (total):  tensor(60.8542, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 214/1000\n",
      "Loss G (total):  tensor(60.5923, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 215/1000\n",
      "Loss G (total):  tensor(60.4521, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 216/1000\n",
      "Loss G (total):  tensor(62.3068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 217/1000\n",
      "Loss G (total):  tensor(60.8821, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 218/1000\n",
      "Loss G (total):  tensor(61.9630, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 219/1000\n",
      "Loss G (total):  tensor(62.0430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 220/1000\n",
      "Loss G (total):  tensor(61.9267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 221/1000\n",
      "Loss G (total):  tensor(63.4665, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 222/1000\n",
      "Loss G (total):  tensor(60.8250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 223/1000\n",
      "Loss G (total):  tensor(63.1185, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 224/1000\n",
      "Loss G (total):  tensor(63.5136, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 225/1000\n",
      "Loss G (total):  tensor(63.4345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 226/1000\n",
      "Loss G (total):  tensor(63.3504, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 227/1000\n",
      "Loss G (total):  tensor(63.6736, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 228/1000\n",
      "Loss G (total):  tensor(65.6748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 229/1000\n",
      "Loss G (total):  tensor(65.1133, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 230/1000\n",
      "Loss G (total):  tensor(67.5050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 231/1000\n",
      "Loss G (total):  tensor(65.8495, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 232/1000\n",
      "Loss G (total):  tensor(67.4827, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 233/1000\n",
      "Loss G (total):  tensor(67.4862, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 234/1000\n",
      "Loss G (total):  tensor(66.8456, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 235/1000\n",
      "Loss G (total):  tensor(66.5986, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 236/1000\n",
      "Loss G (total):  tensor(68.3063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 237/1000\n",
      "Loss G (total):  tensor(68.5436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 238/1000\n",
      "Loss G (total):  tensor(69.4228, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 239/1000\n",
      "Loss G (total):  tensor(68.2973, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 240/1000\n",
      "Loss G (total):  tensor(68.4038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 241/1000\n",
      "Loss G (total):  tensor(69.5673, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 242/1000\n",
      "Loss G (total):  tensor(69.2353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 243/1000\n",
      "Loss G (total):  tensor(69.4047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 244/1000\n",
      "Loss G (total):  tensor(70.5298, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 245/1000\n",
      "Loss G (total):  tensor(70.6890, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 246/1000\n",
      "Loss G (total):  tensor(69.7525, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 247/1000\n",
      "Loss G (total):  tensor(72.5118, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 248/1000\n",
      "Loss G (total):  tensor(70.2255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 249/1000\n",
      "Loss G (total):  tensor(73.4927, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 250/1000\n",
      "Loss G (total):  tensor(70.4882, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 251/1000\n",
      "Loss G (total):  tensor(70.1897, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 252/1000\n",
      "Loss G (total):  tensor(72.9391, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 253/1000\n",
      "Loss G (total):  tensor(73.0481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 254/1000\n",
      "Loss G (total):  tensor(73.8051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 255/1000\n",
      "Loss G (total):  tensor(73.0628, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 256/1000\n",
      "Loss G (total):  tensor(73.7259, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 257/1000\n",
      "Loss G (total):  tensor(74.0181, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 258/1000\n",
      "Loss G (total):  tensor(75.8753, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 259/1000\n",
      "Loss G (total):  tensor(75.3142, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 260/1000\n",
      "Loss G (total):  tensor(75.4251, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 261/1000\n",
      "Loss G (total):  tensor(75.3339, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 262/1000\n",
      "Loss G (total):  tensor(76.8332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 263/1000\n",
      "Loss G (total):  tensor(76.9145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 264/1000\n",
      "Loss G (total):  tensor(76.1587, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 265/1000\n",
      "Loss G (total):  tensor(75.5682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 266/1000\n",
      "Loss G (total):  tensor(78.0412, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 267/1000\n",
      "Loss G (total):  tensor(76.6844, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 268/1000\n",
      "Loss G (total):  tensor(79.3617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 269/1000\n",
      "Loss G (total):  tensor(78.2188, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 270/1000\n",
      "Loss G (total):  tensor(79.2324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 271/1000\n",
      "Loss G (total):  tensor(79.6835, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 272/1000\n",
      "Loss G (total):  tensor(79.5771, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 273/1000\n",
      "Loss G (total):  tensor(79.3932, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 274/1000\n",
      "Loss G (total):  tensor(80.3724, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 275/1000\n",
      "Loss G (total):  tensor(81.1110, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 276/1000\n",
      "Loss G (total):  tensor(81.2000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 277/1000\n",
      "Loss G (total):  tensor(80.8630, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 278/1000\n",
      "Loss G (total):  tensor(82.3121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 279/1000\n",
      "Loss G (total):  tensor(81.9829, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 280/1000\n",
      "Loss G (total):  tensor(82.3520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 281/1000\n",
      "Loss G (total):  tensor(84.1580, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 282/1000\n",
      "Loss G (total):  tensor(84.1737, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 283/1000\n",
      "Loss G (total):  tensor(84.7517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 284/1000\n",
      "Loss G (total):  tensor(83.3637, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 285/1000\n",
      "Loss G (total):  tensor(82.4047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 286/1000\n",
      "Loss G (total):  tensor(84.7101, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 287/1000\n",
      "Loss G (total):  tensor(85.2107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 288/1000\n",
      "Loss G (total):  tensor(85.9688, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 289/1000\n",
      "Loss G (total):  tensor(86.2540, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 290/1000\n",
      "Loss G (total):  tensor(85.0463, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 291/1000\n",
      "Loss G (total):  tensor(88.7934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 292/1000\n",
      "Loss G (total):  tensor(87.8569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 293/1000\n",
      "Loss G (total):  tensor(86.8952, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 294/1000\n",
      "Loss G (total):  tensor(85.1131, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 295/1000\n",
      "Loss G (total):  tensor(88.4626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 296/1000\n",
      "Loss G (total):  tensor(89.6537, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 297/1000\n",
      "Loss G (total):  tensor(88.4696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 298/1000\n",
      "Loss G (total):  tensor(88.3920, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 299/1000\n",
      "Loss G (total):  tensor(90.4062, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 300/1000\n",
      "Loss G (total):  tensor(89.7240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 301/1000\n",
      "Loss G (total):  tensor(90.3515, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 302/1000\n",
      "Loss G (total):  tensor(92.2591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 303/1000\n",
      "Loss G (total):  tensor(90.2817, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 304/1000\n",
      "Loss G (total):  tensor(90.3265, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 305/1000\n",
      "Loss G (total):  tensor(91.4352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 306/1000\n",
      "Loss G (total):  tensor(92.5843, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 307/1000\n",
      "Loss G (total):  tensor(93.6780, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 308/1000\n",
      "Loss G (total):  tensor(93.0535, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 309/1000\n",
      "Loss G (total):  tensor(93.9389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 310/1000\n",
      "Loss G (total):  tensor(96.3939, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 311/1000\n",
      "Loss G (total):  tensor(94.0655, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 312/1000\n",
      "Loss G (total):  tensor(94.3971, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 313/1000\n",
      "Loss G (total):  tensor(94.9257, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 314/1000\n",
      "Loss G (total):  tensor(96.3938, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 315/1000\n",
      "Loss G (total):  tensor(95.3163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 316/1000\n",
      "Loss G (total):  tensor(96.2431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 317/1000\n",
      "Loss G (total):  tensor(99.2482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 318/1000\n",
      "Loss G (total):  tensor(98.8128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 319/1000\n",
      "Loss G (total):  tensor(97.6820, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 320/1000\n",
      "Loss G (total):  tensor(98.1393, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 321/1000\n",
      "Loss G (total):  tensor(98.7727, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 322/1000\n",
      "Loss G (total):  tensor(98.0648, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 323/1000\n",
      "Loss G (total):  tensor(99.7811, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 324/1000\n",
      "Loss G (total):  tensor(100.4249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 325/1000\n",
      "Loss G (total):  tensor(100.1426, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 326/1000\n",
      "Loss G (total):  tensor(100.9356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 327/1000\n",
      "Loss G (total):  tensor(101.0718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 328/1000\n",
      "Loss G (total):  tensor(101.0398, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 329/1000\n",
      "Loss G (total):  tensor(101.8473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 330/1000\n",
      "Loss G (total):  tensor(101.2318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 331/1000\n",
      "Loss G (total):  tensor(103.3394, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 332/1000\n",
      "Loss G (total):  tensor(101.9544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 333/1000\n",
      "Loss G (total):  tensor(104.6475, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 334/1000\n",
      "Loss G (total):  tensor(104.2013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 335/1000\n",
      "Loss G (total):  tensor(105.2979, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 336/1000\n",
      "Loss G (total):  tensor(103.0250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 337/1000\n",
      "Loss G (total):  tensor(105.0434, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 338/1000\n",
      "Loss G (total):  tensor(105.5554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 339/1000\n",
      "Loss G (total):  tensor(106.1326, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 340/1000\n",
      "Loss G (total):  tensor(108.9006, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 341/1000\n",
      "Loss G (total):  tensor(106.1739, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 342/1000\n",
      "Loss G (total):  tensor(106.9873, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 343/1000\n",
      "Loss G (total):  tensor(107.8447, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 344/1000\n",
      "Loss G (total):  tensor(107.3950, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 345/1000\n",
      "Loss G (total):  tensor(107.1425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 346/1000\n",
      "Loss G (total):  tensor(108.9941, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 347/1000\n",
      "Loss G (total):  tensor(108.5649, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 348/1000\n",
      "Loss G (total):  tensor(109.2505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 349/1000\n",
      "Loss G (total):  tensor(109.8193, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 350/1000\n",
      "Loss G (total):  tensor(109.9463, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 351/1000\n",
      "Loss G (total):  tensor(111.4519, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 352/1000\n",
      "Loss G (total):  tensor(111.6245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 353/1000\n",
      "Loss G (total):  tensor(112.4959, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 354/1000\n",
      "Loss G (total):  tensor(112.9086, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 355/1000\n",
      "Loss G (total):  tensor(113.5339, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 356/1000\n",
      "Loss G (total):  tensor(112.7440, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 357/1000\n",
      "Loss G (total):  tensor(115.3608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 358/1000\n",
      "Loss G (total):  tensor(114.9582, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 359/1000\n",
      "Loss G (total):  tensor(114.3698, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 360/1000\n",
      "Loss G (total):  tensor(114.6607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 361/1000\n",
      "Loss G (total):  tensor(114.9063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 362/1000\n",
      "Loss G (total):  tensor(115.6248, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 363/1000\n",
      "Loss G (total):  tensor(115.8028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 364/1000\n",
      "Loss G (total):  tensor(116.4366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 365/1000\n",
      "Loss G (total):  tensor(116.9933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 366/1000\n",
      "Loss G (total):  tensor(117.0579, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 367/1000\n",
      "Loss G (total):  tensor(118.8141, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 368/1000\n",
      "Loss G (total):  tensor(118.4457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 369/1000\n",
      "Loss G (total):  tensor(118.1201, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 370/1000\n",
      "Loss G (total):  tensor(120.3854, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 371/1000\n",
      "Loss G (total):  tensor(119.9374, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 372/1000\n",
      "Loss G (total):  tensor(119.3060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 373/1000\n",
      "Loss G (total):  tensor(121.7287, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 374/1000\n",
      "Loss G (total):  tensor(121.0968, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 375/1000\n",
      "Loss G (total):  tensor(122.6441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 376/1000\n",
      "Loss G (total):  tensor(123.2045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 377/1000\n",
      "Loss G (total):  tensor(123.8849, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 378/1000\n",
      "Loss G (total):  tensor(123.2988, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 379/1000\n",
      "Loss G (total):  tensor(122.7513, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 380/1000\n",
      "Loss G (total):  tensor(124.3108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 381/1000\n",
      "Loss G (total):  tensor(124.0503, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 382/1000\n",
      "Loss G (total):  tensor(125.2041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 383/1000\n",
      "Loss G (total):  tensor(126.1479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 384/1000\n",
      "Loss G (total):  tensor(126.5333, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 385/1000\n",
      "Loss G (total):  tensor(127.0746, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 386/1000\n",
      "Loss G (total):  tensor(127.3271, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 387/1000\n",
      "Loss G (total):  tensor(126.4324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 388/1000\n",
      "Loss G (total):  tensor(128.0714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 389/1000\n",
      "Loss G (total):  tensor(129.7677, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 390/1000\n",
      "Loss G (total):  tensor(128.5108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 391/1000\n",
      "Loss G (total):  tensor(129.2656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 392/1000\n",
      "Loss G (total):  tensor(130.6062, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 393/1000\n",
      "Loss G (total):  tensor(129.7398, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 394/1000\n",
      "Loss G (total):  tensor(131.6105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 395/1000\n",
      "Loss G (total):  tensor(131.3587, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 396/1000\n",
      "Loss G (total):  tensor(132.7981, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 397/1000\n",
      "Loss G (total):  tensor(133.2564, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 398/1000\n",
      "Loss G (total):  tensor(133.1678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 399/1000\n",
      "Loss G (total):  tensor(133.1481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 400/1000\n",
      "Loss G (total):  tensor(134.1796, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 401/1000\n",
      "Loss G (total):  tensor(131.9646, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 402/1000\n",
      "Loss G (total):  tensor(133.8617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 403/1000\n",
      "Loss G (total):  tensor(134.4588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 404/1000\n",
      "Loss G (total):  tensor(136.2985, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 405/1000\n",
      "Loss G (total):  tensor(136.1636, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 406/1000\n",
      "Loss G (total):  tensor(136.4559, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 407/1000\n",
      "Loss G (total):  tensor(136.7878, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 408/1000\n",
      "Loss G (total):  tensor(137.0733, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 409/1000\n",
      "Loss G (total):  tensor(137.8727, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 410/1000\n",
      "Loss G (total):  tensor(138.8122, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 411/1000\n",
      "Loss G (total):  tensor(137.7429, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 412/1000\n",
      "Loss G (total):  tensor(138.8765, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 413/1000\n",
      "Loss G (total):  tensor(140.8634, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 414/1000\n",
      "Loss G (total):  tensor(141.0275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 415/1000\n",
      "Loss G (total):  tensor(140.4581, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 416/1000\n",
      "Loss G (total):  tensor(141.1731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 417/1000\n",
      "Loss G (total):  tensor(142.1043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 418/1000\n",
      "Loss G (total):  tensor(142.9416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 419/1000\n",
      "Loss G (total):  tensor(145.6431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 420/1000\n",
      "Loss G (total):  tensor(145.8656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 421/1000\n",
      "Loss G (total):  tensor(144.6918, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 422/1000\n",
      "Loss G (total):  tensor(145.1792, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 423/1000\n",
      "Loss G (total):  tensor(145.2076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 424/1000\n",
      "Loss G (total):  tensor(145.5913, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 425/1000\n",
      "Loss G (total):  tensor(146.1808, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 426/1000\n",
      "Loss G (total):  tensor(146.9902, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 427/1000\n",
      "Loss G (total):  tensor(147.7515, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 428/1000\n",
      "Loss G (total):  tensor(147.9750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 429/1000\n",
      "Loss G (total):  tensor(148.9399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 430/1000\n",
      "Loss G (total):  tensor(149.3135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 431/1000\n",
      "Loss G (total):  tensor(149.3412, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 432/1000\n",
      "Loss G (total):  tensor(151.6544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 433/1000\n",
      "Loss G (total):  tensor(150.2899, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 434/1000\n",
      "Loss G (total):  tensor(150.3518, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 435/1000\n",
      "Loss G (total):  tensor(151.2161, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 436/1000\n",
      "Loss G (total):  tensor(151.8693, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 437/1000\n",
      "Loss G (total):  tensor(152.9073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 438/1000\n",
      "Loss G (total):  tensor(153.6080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 439/1000\n",
      "Loss G (total):  tensor(154.0269, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 440/1000\n",
      "Loss G (total):  tensor(154.8988, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 441/1000\n",
      "Loss G (total):  tensor(154.4934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 442/1000\n",
      "Loss G (total):  tensor(154.5004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 443/1000\n",
      "Loss G (total):  tensor(155.7948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 444/1000\n",
      "Loss G (total):  tensor(155.9472, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 445/1000\n",
      "Loss G (total):  tensor(156.8132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 446/1000\n",
      "Loss G (total):  tensor(158.1396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 447/1000\n",
      "Loss G (total):  tensor(157.1718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 448/1000\n",
      "Loss G (total):  tensor(158.4989, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 449/1000\n",
      "Loss G (total):  tensor(159.6878, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 450/1000\n",
      "Loss G (total):  tensor(158.9034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 451/1000\n",
      "Loss G (total):  tensor(162.1056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 452/1000\n",
      "Loss G (total):  tensor(160.9858, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 453/1000\n",
      "Loss G (total):  tensor(162.0040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 454/1000\n",
      "Loss G (total):  tensor(161.9421, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 455/1000\n",
      "Loss G (total):  tensor(159.8481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 456/1000\n",
      "Loss G (total):  tensor(162.7570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 457/1000\n",
      "Loss G (total):  tensor(162.8032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 458/1000\n",
      "Loss G (total):  tensor(163.8245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 459/1000\n",
      "Loss G (total):  tensor(163.6045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 460/1000\n",
      "Loss G (total):  tensor(164.2860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 461/1000\n",
      "Loss G (total):  tensor(169.5651, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 462/1000\n",
      "Loss G (total):  tensor(167.2581, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 463/1000\n",
      "Loss G (total):  tensor(168.0902, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 464/1000\n",
      "Loss G (total):  tensor(167.2553, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 465/1000\n",
      "Loss G (total):  tensor(167.0606, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 466/1000\n",
      "Loss G (total):  tensor(168.7235, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 467/1000\n",
      "Loss G (total):  tensor(169.5815, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 468/1000\n",
      "Loss G (total):  tensor(171.8336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 469/1000\n",
      "Loss G (total):  tensor(171.3879, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 470/1000\n",
      "Loss G (total):  tensor(171.5045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 471/1000\n",
      "Loss G (total):  tensor(169.5295, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 472/1000\n",
      "Loss G (total):  tensor(172.3716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 473/1000\n",
      "Loss G (total):  tensor(173.2122, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 474/1000\n",
      "Loss G (total):  tensor(171.1631, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 475/1000\n",
      "Loss G (total):  tensor(171.7146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 476/1000\n",
      "Loss G (total):  tensor(173.5120, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 477/1000\n",
      "Loss G (total):  tensor(174.2956, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 478/1000\n",
      "Loss G (total):  tensor(175.1307, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 479/1000\n",
      "Loss G (total):  tensor(174.4096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 480/1000\n",
      "Loss G (total):  tensor(176.1862, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 481/1000\n",
      "Loss G (total):  tensor(176.7045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 482/1000\n",
      "Loss G (total):  tensor(179.6994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 483/1000\n",
      "Loss G (total):  tensor(176.6106, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 484/1000\n",
      "Loss G (total):  tensor(178.4425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 485/1000\n",
      "Loss G (total):  tensor(179.4633, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 486/1000\n",
      "Loss G (total):  tensor(179.0855, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 487/1000\n",
      "Loss G (total):  tensor(180.2919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 488/1000\n",
      "Loss G (total):  tensor(180.7910, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 489/1000\n",
      "Loss G (total):  tensor(180.2595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 490/1000\n",
      "Loss G (total):  tensor(181.9351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 491/1000\n",
      "Loss G (total):  tensor(182.5099, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 492/1000\n",
      "Loss G (total):  tensor(183.0531, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 493/1000\n",
      "Loss G (total):  tensor(186.1569, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 494/1000\n",
      "Loss G (total):  tensor(184.0270, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 495/1000\n",
      "Loss G (total):  tensor(184.0266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 496/1000\n",
      "Loss G (total):  tensor(186.3628, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 497/1000\n",
      "Loss G (total):  tensor(186.1660, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 498/1000\n",
      "Loss G (total):  tensor(187.1854, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 499/1000\n",
      "Loss G (total):  tensor(186.4205, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 500/1000\n",
      "Loss G (total):  tensor(189.0838, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 501/1000\n",
      "Loss G (total):  tensor(189.3126, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 502/1000\n",
      "Loss G (total):  tensor(189.2346, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 503/1000\n",
      "Loss G (total):  tensor(190.8320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 504/1000\n",
      "Loss G (total):  tensor(189.7768, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 505/1000\n",
      "Loss G (total):  tensor(189.9365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 506/1000\n",
      "Loss G (total):  tensor(192.1846, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 507/1000\n",
      "Loss G (total):  tensor(192.6199, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 508/1000\n",
      "Loss G (total):  tensor(193.9080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 509/1000\n",
      "Loss G (total):  tensor(192.5166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 510/1000\n",
      "Loss G (total):  tensor(193.7208, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 511/1000\n",
      "Loss G (total):  tensor(195.4962, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 512/1000\n",
      "Loss G (total):  tensor(194.5854, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 513/1000\n",
      "Loss G (total):  tensor(195.4720, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 514/1000\n",
      "Loss G (total):  tensor(195.3632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 515/1000\n",
      "Loss G (total):  tensor(196.9955, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 516/1000\n",
      "Loss G (total):  tensor(197.3887, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 517/1000\n",
      "Loss G (total):  tensor(196.5248, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 518/1000\n",
      "Loss G (total):  tensor(198.5437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 519/1000\n",
      "Loss G (total):  tensor(199.0410, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 520/1000\n",
      "Loss G (total):  tensor(200.1545, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 521/1000\n",
      "Loss G (total):  tensor(201.4149, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 522/1000\n",
      "Loss G (total):  tensor(202.2593, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 523/1000\n",
      "Loss G (total):  tensor(201.9772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 524/1000\n",
      "Loss G (total):  tensor(201.9437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 525/1000\n",
      "Loss G (total):  tensor(203.1080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 526/1000\n",
      "Loss G (total):  tensor(202.8335, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 527/1000\n",
      "Loss G (total):  tensor(203.4319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 528/1000\n",
      "Loss G (total):  tensor(204.2156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 529/1000\n",
      "Loss G (total):  tensor(202.6278, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 530/1000\n",
      "Loss G (total):  tensor(206.0592, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 531/1000\n",
      "Loss G (total):  tensor(207.2843, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 532/1000\n",
      "Loss G (total):  tensor(207.1330, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 533/1000\n",
      "Loss G (total):  tensor(207.4857, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 534/1000\n",
      "Loss G (total):  tensor(208.0710, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 535/1000\n",
      "Loss G (total):  tensor(209.1026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 536/1000\n",
      "Loss G (total):  tensor(209.4493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 537/1000\n",
      "Loss G (total):  tensor(211.1864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 538/1000\n",
      "Loss G (total):  tensor(211.1178, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 539/1000\n",
      "Loss G (total):  tensor(210.1299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 540/1000\n",
      "Loss G (total):  tensor(211.2790, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 541/1000\n",
      "Loss G (total):  tensor(212.5433, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 542/1000\n",
      "Loss G (total):  tensor(213.7199, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 543/1000\n",
      "Loss G (total):  tensor(214.2445, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 544/1000\n",
      "Loss G (total):  tensor(214.4717, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 545/1000\n",
      "Loss G (total):  tensor(215.1681, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 546/1000\n",
      "Loss G (total):  tensor(216.3205, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 547/1000\n",
      "Loss G (total):  tensor(215.6396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 548/1000\n",
      "Loss G (total):  tensor(216.4349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 549/1000\n",
      "Loss G (total):  tensor(216.8683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 550/1000\n",
      "Loss G (total):  tensor(218.7839, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 551/1000\n",
      "Loss G (total):  tensor(219.2222, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 552/1000\n",
      "Loss G (total):  tensor(219.8142, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 553/1000\n",
      "Loss G (total):  tensor(219.5816, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 554/1000\n",
      "Loss G (total):  tensor(219.9709, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 555/1000\n",
      "Loss G (total):  tensor(221.3328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 556/1000\n",
      "Loss G (total):  tensor(221.2314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 557/1000\n",
      "Loss G (total):  tensor(223.4091, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 558/1000\n",
      "Loss G (total):  tensor(224.0432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 559/1000\n",
      "Loss G (total):  tensor(222.3649, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 560/1000\n",
      "Loss G (total):  tensor(225.1911, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 561/1000\n",
      "Loss G (total):  tensor(226.9990, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 562/1000\n",
      "Loss G (total):  tensor(225.2396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 563/1000\n",
      "Loss G (total):  tensor(228.1869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 564/1000\n",
      "Loss G (total):  tensor(227.8685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 565/1000\n",
      "Loss G (total):  tensor(226.3524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 566/1000\n",
      "Loss G (total):  tensor(228.0469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 567/1000\n",
      "Loss G (total):  tensor(229.7623, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 568/1000\n",
      "Loss G (total):  tensor(229.1478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 569/1000\n",
      "Loss G (total):  tensor(231.7385, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 570/1000\n",
      "Loss G (total):  tensor(230.9477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 571/1000\n",
      "Loss G (total):  tensor(231.0859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 572/1000\n",
      "Loss G (total):  tensor(233.1843, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 573/1000\n",
      "Loss G (total):  tensor(234.6193, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 574/1000\n",
      "Loss G (total):  tensor(232.9830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 575/1000\n",
      "Loss G (total):  tensor(233.8200, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 576/1000\n",
      "Loss G (total):  tensor(236.2019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 577/1000\n",
      "Loss G (total):  tensor(235.7481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 578/1000\n",
      "Loss G (total):  tensor(235.5747, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 579/1000\n",
      "Loss G (total):  tensor(238.4708, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 580/1000\n",
      "Loss G (total):  tensor(237.4942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 581/1000\n",
      "Loss G (total):  tensor(239.0970, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 582/1000\n",
      "Loss G (total):  tensor(238.5042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 583/1000\n",
      "Loss G (total):  tensor(239.7043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 584/1000\n",
      "Loss G (total):  tensor(239.6948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 585/1000\n",
      "Loss G (total):  tensor(241.4065, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 586/1000\n",
      "Loss G (total):  tensor(241.9511, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 587/1000\n",
      "Loss G (total):  tensor(241.6236, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 588/1000\n",
      "Loss G (total):  tensor(242.1991, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 589/1000\n",
      "Loss G (total):  tensor(241.5851, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 590/1000\n",
      "Loss G (total):  tensor(243.6601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 591/1000\n",
      "Loss G (total):  tensor(243.9731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 592/1000\n",
      "Loss G (total):  tensor(246.4093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 593/1000\n",
      "Loss G (total):  tensor(246.1683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 594/1000\n",
      "Loss G (total):  tensor(247.8588, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 595/1000\n",
      "Loss G (total):  tensor(248.4388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 596/1000\n",
      "Loss G (total):  tensor(247.2804, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 597/1000\n",
      "Loss G (total):  tensor(248.1770, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 598/1000\n",
      "Loss G (total):  tensor(249.9969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 599/1000\n",
      "Loss G (total):  tensor(250.1941, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 600/1000\n",
      "Loss G (total):  tensor(252.3510, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 601/1000\n",
      "Loss G (total):  tensor(250.2137, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 602/1000\n",
      "Loss G (total):  tensor(253.6071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 603/1000\n",
      "Loss G (total):  tensor(252.7302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 604/1000\n",
      "Loss G (total):  tensor(253.5484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 605/1000\n",
      "Loss G (total):  tensor(253.9933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 606/1000\n",
      "Loss G (total):  tensor(254.7360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 607/1000\n",
      "Loss G (total):  tensor(256.5136, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 608/1000\n",
      "Loss G (total):  tensor(256.6750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 609/1000\n",
      "Loss G (total):  tensor(256.2443, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 610/1000\n",
      "Loss G (total):  tensor(256.8922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 611/1000\n",
      "Loss G (total):  tensor(257.1631, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 612/1000\n",
      "Loss G (total):  tensor(259.7025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 613/1000\n",
      "Loss G (total):  tensor(259.6773, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 614/1000\n",
      "Loss G (total):  tensor(259.6945, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 615/1000\n",
      "Loss G (total):  tensor(260.1044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 616/1000\n",
      "Loss G (total):  tensor(260.2946, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 617/1000\n",
      "Loss G (total):  tensor(263.8064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 618/1000\n",
      "Loss G (total):  tensor(263.7975, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 619/1000\n",
      "Loss G (total):  tensor(262.9424, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 620/1000\n",
      "Loss G (total):  tensor(264.5878, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 621/1000\n",
      "Loss G (total):  tensor(264.3320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 622/1000\n",
      "Loss G (total):  tensor(264.3819, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 623/1000\n",
      "Loss G (total):  tensor(267.4105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 624/1000\n",
      "Loss G (total):  tensor(266.0407, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 625/1000\n",
      "Loss G (total):  tensor(268.5110, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 626/1000\n",
      "Loss G (total):  tensor(268.4930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 627/1000\n",
      "Loss G (total):  tensor(269.0650, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 628/1000\n",
      "Loss G (total):  tensor(268.8380, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 629/1000\n",
      "Loss G (total):  tensor(271.0121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 630/1000\n",
      "Loss G (total):  tensor(271.8444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 631/1000\n",
      "Loss G (total):  tensor(271.5377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 632/1000\n",
      "Loss G (total):  tensor(272.8113, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 633/1000\n",
      "Loss G (total):  tensor(272.8102, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 634/1000\n",
      "Loss G (total):  tensor(275.2167, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 635/1000\n",
      "Loss G (total):  tensor(273.4511, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 636/1000\n",
      "Loss G (total):  tensor(277.4455, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 637/1000\n",
      "Loss G (total):  tensor(276.8375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 638/1000\n",
      "Loss G (total):  tensor(276.4132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 639/1000\n",
      "Loss G (total):  tensor(278.8143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 640/1000\n",
      "Loss G (total):  tensor(277.0714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 641/1000\n",
      "Loss G (total):  tensor(279.0762, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 642/1000\n",
      "Loss G (total):  tensor(279.9376, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 643/1000\n",
      "Loss G (total):  tensor(280.8087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 644/1000\n",
      "Loss G (total):  tensor(280.8453, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 645/1000\n",
      "Loss G (total):  tensor(282.1123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 646/1000\n",
      "Loss G (total):  tensor(281.2530, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 647/1000\n",
      "Loss G (total):  tensor(283.8386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 648/1000\n",
      "Loss G (total):  tensor(284.8489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 649/1000\n",
      "Loss G (total):  tensor(284.4794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 650/1000\n",
      "Loss G (total):  tensor(285.9789, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 651/1000\n",
      "Loss G (total):  tensor(285.7034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 652/1000\n",
      "Loss G (total):  tensor(287.7493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 653/1000\n",
      "Loss G (total):  tensor(287.5847, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 654/1000\n",
      "Loss G (total):  tensor(288.2238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 655/1000\n",
      "Loss G (total):  tensor(289.0462, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 656/1000\n",
      "Loss G (total):  tensor(289.9869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 657/1000\n",
      "Loss G (total):  tensor(289.8058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 658/1000\n",
      "Loss G (total):  tensor(292.1389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 659/1000\n",
      "Loss G (total):  tensor(292.5558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 660/1000\n",
      "Loss G (total):  tensor(292.6296, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 661/1000\n",
      "Loss G (total):  tensor(292.2276, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 662/1000\n",
      "Loss G (total):  tensor(295.4712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 663/1000\n",
      "Loss G (total):  tensor(294.5442, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 664/1000\n",
      "Loss G (total):  tensor(295.2130, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 665/1000\n",
      "Loss G (total):  tensor(295.2089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 666/1000\n",
      "Loss G (total):  tensor(296.1371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 667/1000\n",
      "Loss G (total):  tensor(297.4607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 668/1000\n",
      "Loss G (total):  tensor(297.4215, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 669/1000\n",
      "Loss G (total):  tensor(299.3166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 670/1000\n",
      "Loss G (total):  tensor(301.3233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 671/1000\n",
      "Loss G (total):  tensor(300.0838, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 672/1000\n",
      "Loss G (total):  tensor(300.7768, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 673/1000\n",
      "Loss G (total):  tensor(302.7210, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 674/1000\n",
      "Loss G (total):  tensor(302.4788, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 675/1000\n",
      "Loss G (total):  tensor(303.6349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 676/1000\n",
      "Loss G (total):  tensor(304.6667, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 677/1000\n",
      "Loss G (total):  tensor(306.1347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 678/1000\n",
      "Loss G (total):  tensor(305.1999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 679/1000\n",
      "Loss G (total):  tensor(306.5763, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 680/1000\n",
      "Loss G (total):  tensor(307.3132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 681/1000\n",
      "Loss G (total):  tensor(308.5171, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 682/1000\n",
      "Loss G (total):  tensor(308.0371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 683/1000\n",
      "Loss G (total):  tensor(309.3698, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 684/1000\n",
      "Loss G (total):  tensor(310.4570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 685/1000\n",
      "Loss G (total):  tensor(309.9951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 686/1000\n",
      "Loss G (total):  tensor(312.1736, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 687/1000\n",
      "Loss G (total):  tensor(312.8666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 688/1000\n",
      "Loss G (total):  tensor(313.6903, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 689/1000\n",
      "Loss G (total):  tensor(313.3152, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 690/1000\n",
      "Loss G (total):  tensor(313.2635, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 691/1000\n",
      "Loss G (total):  tensor(315.4343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 692/1000\n",
      "Loss G (total):  tensor(314.9888, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 693/1000\n",
      "Loss G (total):  tensor(316.8693, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 694/1000\n",
      "Loss G (total):  tensor(317.1436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 695/1000\n",
      "Loss G (total):  tensor(318.7885, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 696/1000\n",
      "Loss G (total):  tensor(319.4888, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 697/1000\n",
      "Loss G (total):  tensor(319.6599, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 698/1000\n",
      "Loss G (total):  tensor(320.4659, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 699/1000\n",
      "Loss G (total):  tensor(321.6599, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 700/1000\n",
      "Loss G (total):  tensor(322.5316, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 701/1000\n",
      "Loss G (total):  tensor(323.4619, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 702/1000\n",
      "Loss G (total):  tensor(323.5474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 703/1000\n",
      "Loss G (total):  tensor(324.2696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 704/1000\n",
      "Loss G (total):  tensor(326.1163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 705/1000\n",
      "Loss G (total):  tensor(325.4408, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 706/1000\n",
      "Loss G (total):  tensor(325.7139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 707/1000\n",
      "Loss G (total):  tensor(328.2209, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 708/1000\n",
      "Loss G (total):  tensor(328.0619, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 709/1000\n",
      "Loss G (total):  tensor(327.8672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 710/1000\n",
      "Loss G (total):  tensor(330.9609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 711/1000\n",
      "Loss G (total):  tensor(330.0505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 712/1000\n",
      "Loss G (total):  tensor(331.7855, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 713/1000\n",
      "Loss G (total):  tensor(333.1509, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 714/1000\n",
      "Loss G (total):  tensor(332.5690, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 715/1000\n",
      "Loss G (total):  tensor(334.1269, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 716/1000\n",
      "Loss G (total):  tensor(335.0864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 717/1000\n",
      "Loss G (total):  tensor(334.9259, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 718/1000\n",
      "Loss G (total):  tensor(334.9777, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 719/1000\n",
      "Loss G (total):  tensor(335.9145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 720/1000\n",
      "Loss G (total):  tensor(338.7632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 721/1000\n",
      "Loss G (total):  tensor(338.0473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 722/1000\n",
      "Loss G (total):  tensor(337.6129, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 723/1000\n",
      "Loss G (total):  tensor(339.3362, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 724/1000\n",
      "Loss G (total):  tensor(340.5253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 725/1000\n",
      "Loss G (total):  tensor(340.8303, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 726/1000\n",
      "Loss G (total):  tensor(341.7323, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 727/1000\n",
      "Loss G (total):  tensor(343.3224, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 728/1000\n",
      "Loss G (total):  tensor(343.1704, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 729/1000\n",
      "Loss G (total):  tensor(345.7030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 730/1000\n",
      "Loss G (total):  tensor(346.2837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 731/1000\n",
      "Loss G (total):  tensor(345.5756, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 732/1000\n",
      "Loss G (total):  tensor(345.0706, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 733/1000\n",
      "Loss G (total):  tensor(344.9370, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 734/1000\n",
      "Loss G (total):  tensor(347.7876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 735/1000\n",
      "Loss G (total):  tensor(349.6256, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 736/1000\n",
      "Loss G (total):  tensor(349.8502, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 737/1000\n",
      "Loss G (total):  tensor(350.4880, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 738/1000\n",
      "Loss G (total):  tensor(351.6462, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 739/1000\n",
      "Loss G (total):  tensor(352.5153, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 740/1000\n",
      "Loss G (total):  tensor(352.9852, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 741/1000\n",
      "Loss G (total):  tensor(353.6549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 742/1000\n",
      "Loss G (total):  tensor(354.2744, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 743/1000\n",
      "Loss G (total):  tensor(354.9379, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 744/1000\n",
      "Loss G (total):  tensor(355.5580, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 745/1000\n",
      "Loss G (total):  tensor(355.8079, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 746/1000\n",
      "Loss G (total):  tensor(357.1674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 747/1000\n",
      "Loss G (total):  tensor(357.9626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 748/1000\n",
      "Loss G (total):  tensor(360.8782, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 749/1000\n",
      "Loss G (total):  tensor(359.9164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 750/1000\n",
      "Loss G (total):  tensor(360.6549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 751/1000\n",
      "Loss G (total):  tensor(361.0081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 752/1000\n",
      "Loss G (total):  tensor(362.3803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 753/1000\n",
      "Loss G (total):  tensor(361.6895, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 754/1000\n",
      "Loss G (total):  tensor(363.7231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 755/1000\n",
      "Loss G (total):  tensor(365.5640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 756/1000\n",
      "Loss G (total):  tensor(365.9915, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 757/1000\n",
      "Loss G (total):  tensor(366.9544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 758/1000\n",
      "Loss G (total):  tensor(367.0408, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 759/1000\n",
      "Loss G (total):  tensor(367.8267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 760/1000\n",
      "Loss G (total):  tensor(369.1905, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 761/1000\n",
      "Loss G (total):  tensor(370.5409, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 762/1000\n",
      "Loss G (total):  tensor(370.7369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 763/1000\n",
      "Loss G (total):  tensor(372.0039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 764/1000\n",
      "Loss G (total):  tensor(371.5570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 765/1000\n",
      "Loss G (total):  tensor(372.0718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 766/1000\n",
      "Loss G (total):  tensor(372.8797, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 767/1000\n",
      "Loss G (total):  tensor(373.6227, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 768/1000\n",
      "Loss G (total):  tensor(376.6686, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 769/1000\n",
      "Loss G (total):  tensor(376.4233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 770/1000\n",
      "Loss G (total):  tensor(376.5328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 771/1000\n",
      "Loss G (total):  tensor(376.5638, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 772/1000\n",
      "Loss G (total):  tensor(377.8441, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 773/1000\n",
      "Loss G (total):  tensor(378.4058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 774/1000\n",
      "Loss G (total):  tensor(379.3649, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 775/1000\n",
      "Loss G (total):  tensor(382.1280, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 776/1000\n",
      "Loss G (total):  tensor(383.4043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 777/1000\n",
      "Loss G (total):  tensor(382.6753, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 778/1000\n",
      "Loss G (total):  tensor(382.0429, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 779/1000\n",
      "Loss G (total):  tensor(383.2575, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 780/1000\n",
      "Loss G (total):  tensor(386.8947, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 781/1000\n",
      "Loss G (total):  tensor(385.1353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 782/1000\n",
      "Loss G (total):  tensor(387.1624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 783/1000\n",
      "Loss G (total):  tensor(387.4859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 784/1000\n",
      "Loss G (total):  tensor(388.6497, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 785/1000\n",
      "Loss G (total):  tensor(389.4051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 786/1000\n",
      "Loss G (total):  tensor(390.8760, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 787/1000\n",
      "Loss G (total):  tensor(389.9125, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 788/1000\n",
      "Loss G (total):  tensor(391.7177, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 789/1000\n",
      "Loss G (total):  tensor(391.0268, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 790/1000\n",
      "Loss G (total):  tensor(393.7393, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 791/1000\n",
      "Loss G (total):  tensor(393.2147, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 792/1000\n",
      "Loss G (total):  tensor(393.9313, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 793/1000\n",
      "Loss G (total):  tensor(396.3424, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 794/1000\n",
      "Loss G (total):  tensor(396.8632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 795/1000\n",
      "Loss G (total):  tensor(396.2474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 796/1000\n",
      "Loss G (total):  tensor(398.7292, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 797/1000\n",
      "Loss G (total):  tensor(397.8470, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 798/1000\n",
      "Loss G (total):  tensor(401.4430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 799/1000\n",
      "Loss G (total):  tensor(400.5222, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 800/1000\n",
      "Loss G (total):  tensor(399.9307, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 801/1000\n",
      "Loss G (total):  tensor(402.3284, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 802/1000\n",
      "Loss G (total):  tensor(402.5552, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 803/1000\n",
      "Loss G (total):  tensor(405.1541, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 804/1000\n",
      "Loss G (total):  tensor(404.5654, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 805/1000\n",
      "Loss G (total):  tensor(405.2970, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 806/1000\n",
      "Loss G (total):  tensor(406.0136, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 807/1000\n",
      "Loss G (total):  tensor(407.3043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 808/1000\n",
      "Loss G (total):  tensor(406.8806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 809/1000\n",
      "Loss G (total):  tensor(409.7921, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 810/1000\n",
      "Loss G (total):  tensor(409.7179, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 811/1000\n",
      "Loss G (total):  tensor(410.1643, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 812/1000\n",
      "Loss G (total):  tensor(411.6938, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 813/1000\n",
      "Loss G (total):  tensor(411.4177, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 814/1000\n",
      "Loss G (total):  tensor(412.2728, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 815/1000\n",
      "Loss G (total):  tensor(412.9308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 816/1000\n",
      "Loss G (total):  tensor(414.5047, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 817/1000\n",
      "Loss G (total):  tensor(416.4706, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 818/1000\n",
      "Loss G (total):  tensor(416.2915, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 819/1000\n",
      "Loss G (total):  tensor(417.0408, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 820/1000\n",
      "Loss G (total):  tensor(418.2213, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 821/1000\n",
      "Loss G (total):  tensor(419.1790, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 822/1000\n",
      "Loss G (total):  tensor(419.2529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 823/1000\n",
      "Loss G (total):  tensor(420.1774, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 824/1000\n",
      "Loss G (total):  tensor(420.6068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 825/1000\n",
      "Loss G (total):  tensor(422.4853, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 826/1000\n",
      "Loss G (total):  tensor(424.3632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 827/1000\n",
      "Loss G (total):  tensor(425.1351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 828/1000\n",
      "Loss G (total):  tensor(424.7356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 829/1000\n",
      "Loss G (total):  tensor(425.8453, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 830/1000\n",
      "Loss G (total):  tensor(426.4377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 831/1000\n",
      "Loss G (total):  tensor(426.6664, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 832/1000\n",
      "Loss G (total):  tensor(427.5244, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 833/1000\n",
      "Loss G (total):  tensor(428.4574, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 834/1000\n",
      "Loss G (total):  tensor(430.3538, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 835/1000\n",
      "Loss G (total):  tensor(431.2451, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 836/1000\n",
      "Loss G (total):  tensor(430.5020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 837/1000\n",
      "Loss G (total):  tensor(432.2313, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 838/1000\n",
      "Loss G (total):  tensor(433.4741, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 839/1000\n",
      "Loss G (total):  tensor(434.6655, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 840/1000\n",
      "Loss G (total):  tensor(434.8053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 841/1000\n",
      "Loss G (total):  tensor(435.8872, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 842/1000\n",
      "Loss G (total):  tensor(436.3077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 843/1000\n",
      "Loss G (total):  tensor(437.1817, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 844/1000\n",
      "Loss G (total):  tensor(437.2033, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 845/1000\n",
      "Loss G (total):  tensor(438.8976, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 846/1000\n",
      "Loss G (total):  tensor(440.1399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 847/1000\n",
      "Loss G (total):  tensor(441.1115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 848/1000\n",
      "Loss G (total):  tensor(443.0178, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 849/1000\n",
      "Loss G (total):  tensor(443.3744, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 850/1000\n",
      "Loss G (total):  tensor(445.4772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 851/1000\n",
      "Loss G (total):  tensor(444.8841, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 852/1000\n",
      "Loss G (total):  tensor(445.6999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 853/1000\n",
      "Loss G (total):  tensor(446.2180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 854/1000\n",
      "Loss G (total):  tensor(448.6528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 855/1000\n",
      "Loss G (total):  tensor(447.8258, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 856/1000\n",
      "Loss G (total):  tensor(447.7518, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 857/1000\n",
      "Loss G (total):  tensor(452.0055, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 858/1000\n",
      "Loss G (total):  tensor(451.3195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 859/1000\n",
      "Loss G (total):  tensor(452.0705, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 860/1000\n",
      "Loss G (total):  tensor(451.4959, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 861/1000\n",
      "Loss G (total):  tensor(454.3947, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 862/1000\n",
      "Loss G (total):  tensor(454.4776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 863/1000\n",
      "Loss G (total):  tensor(454.9764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 864/1000\n",
      "Loss G (total):  tensor(455.0550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 865/1000\n",
      "Loss G (total):  tensor(456.3859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 866/1000\n",
      "Loss G (total):  tensor(457.6352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 867/1000\n",
      "Loss G (total):  tensor(457.7165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 868/1000\n",
      "Loss G (total):  tensor(459.1870, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 869/1000\n",
      "Loss G (total):  tensor(461.7495, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 870/1000\n",
      "Loss G (total):  tensor(460.3736, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 871/1000\n",
      "Loss G (total):  tensor(461.9480, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 872/1000\n",
      "Loss G (total):  tensor(463.3903, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 873/1000\n",
      "Loss G (total):  tensor(465.5350, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 874/1000\n",
      "Loss G (total):  tensor(465.0253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 875/1000\n",
      "Loss G (total):  tensor(466.2436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 876/1000\n",
      "Loss G (total):  tensor(465.5912, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 877/1000\n",
      "Loss G (total):  tensor(466.8793, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 878/1000\n",
      "Loss G (total):  tensor(468.0013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 879/1000\n",
      "Loss G (total):  tensor(468.5623, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 880/1000\n",
      "Loss G (total):  tensor(470.8443, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 881/1000\n",
      "Loss G (total):  tensor(470.6994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 882/1000\n",
      "Loss G (total):  tensor(471.9153, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 883/1000\n",
      "Loss G (total):  tensor(472.0066, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 884/1000\n",
      "Loss G (total):  tensor(472.3901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 885/1000\n",
      "Loss G (total):  tensor(473.5065, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 886/1000\n",
      "Loss G (total):  tensor(475.4135, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 887/1000\n",
      "Loss G (total):  tensor(476.4482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 888/1000\n",
      "Loss G (total):  tensor(476.4836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 889/1000\n",
      "Loss G (total):  tensor(478.4101, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 890/1000\n",
      "Loss G (total):  tensor(477.5846, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 891/1000\n",
      "Loss G (total):  tensor(478.9829, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 892/1000\n",
      "Loss G (total):  tensor(480.7138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 893/1000\n",
      "Loss G (total):  tensor(480.9828, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 894/1000\n",
      "Loss G (total):  tensor(483.6764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 895/1000\n",
      "Loss G (total):  tensor(484.3196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 896/1000\n",
      "Loss G (total):  tensor(483.1024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 897/1000\n",
      "Loss G (total):  tensor(485.3930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 898/1000\n",
      "Loss G (total):  tensor(485.7175, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 899/1000\n",
      "Loss G (total):  tensor(487.5089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 900/1000\n",
      "Loss G (total):  tensor(487.0579, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 901/1000\n",
      "Loss G (total):  tensor(490.2951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 902/1000\n",
      "Loss G (total):  tensor(489.8308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 903/1000\n",
      "Loss G (total):  tensor(491.5072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 904/1000\n",
      "Loss G (total):  tensor(490.0834, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 905/1000\n",
      "Loss G (total):  tensor(492.5140, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 906/1000\n",
      "Loss G (total):  tensor(492.2683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 907/1000\n",
      "Loss G (total):  tensor(493.8074, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 908/1000\n",
      "Loss G (total):  tensor(494.7685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 909/1000\n",
      "Loss G (total):  tensor(495.4006, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 910/1000\n",
      "Loss G (total):  tensor(496.8176, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 911/1000\n",
      "Loss G (total):  tensor(497.3806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 912/1000\n",
      "Loss G (total):  tensor(499.6739, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 913/1000\n",
      "Loss G (total):  tensor(499.2959, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 914/1000\n",
      "Loss G (total):  tensor(500.6228, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 915/1000\n",
      "Loss G (total):  tensor(501.6172, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 916/1000\n",
      "Loss G (total):  tensor(501.6627, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 917/1000\n",
      "Loss G (total):  tensor(502.6776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 918/1000\n",
      "Loss G (total):  tensor(502.6227, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 919/1000\n",
      "Loss G (total):  tensor(504.9601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 920/1000\n",
      "Loss G (total):  tensor(505.5308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 921/1000\n",
      "Loss G (total):  tensor(506.2478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 922/1000\n",
      "Loss G (total):  tensor(507.7240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 923/1000\n",
      "Loss G (total):  tensor(509.0190, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 924/1000\n",
      "Loss G (total):  tensor(510.3509, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 925/1000\n",
      "Loss G (total):  tensor(511.4953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 926/1000\n",
      "Loss G (total):  tensor(510.2139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 927/1000\n",
      "Loss G (total):  tensor(511.8584, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 928/1000\n",
      "Loss G (total):  tensor(512.7007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 929/1000\n",
      "Loss G (total):  tensor(514.5134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 930/1000\n",
      "Loss G (total):  tensor(515.4803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 931/1000\n",
      "Loss G (total):  tensor(515.7666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 932/1000\n",
      "Loss G (total):  tensor(516.8395, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 933/1000\n",
      "Loss G (total):  tensor(517.2275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 934/1000\n",
      "Loss G (total):  tensor(518.9627, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 935/1000\n",
      "Loss G (total):  tensor(519.7305, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 936/1000\n",
      "Loss G (total):  tensor(520.5660, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 937/1000\n",
      "Loss G (total):  tensor(521.6552, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 938/1000\n",
      "Loss G (total):  tensor(522.3347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 939/1000\n",
      "Loss G (total):  tensor(522.9011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 940/1000\n",
      "Loss G (total):  tensor(522.2131, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 941/1000\n",
      "Loss G (total):  tensor(525.7112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 942/1000\n",
      "Loss G (total):  tensor(526.4313, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 943/1000\n",
      "Loss G (total):  tensor(525.8901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 944/1000\n",
      "Loss G (total):  tensor(527.8282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 945/1000\n",
      "Loss G (total):  tensor(529.7822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 946/1000\n",
      "Loss G (total):  tensor(529.4275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 947/1000\n",
      "Loss G (total):  tensor(530.3231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 948/1000\n",
      "Loss G (total):  tensor(530.6760, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 949/1000\n",
      "Loss G (total):  tensor(531.5319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 950/1000\n",
      "Loss G (total):  tensor(533.9389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 951/1000\n",
      "Loss G (total):  tensor(533.4485, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 952/1000\n",
      "Loss G (total):  tensor(535.6818, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 953/1000\n",
      "Loss G (total):  tensor(536.6674, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 954/1000\n",
      "Loss G (total):  tensor(536.6476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 955/1000\n",
      "Loss G (total):  tensor(537.4313, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 956/1000\n",
      "Loss G (total):  tensor(539.6948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 957/1000\n",
      "Loss G (total):  tensor(539.4672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 958/1000\n",
      "Loss G (total):  tensor(540.5034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 959/1000\n",
      "Loss G (total):  tensor(541.9424, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 960/1000\n",
      "Loss G (total):  tensor(545.4765, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 961/1000\n",
      "Loss G (total):  tensor(543.4477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 962/1000\n",
      "Loss G (total):  tensor(544.4248, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 963/1000\n",
      "Loss G (total):  tensor(546.0554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 964/1000\n",
      "Loss G (total):  tensor(548.0372, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 965/1000\n",
      "Loss G (total):  tensor(547.6790, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 966/1000\n",
      "Loss G (total):  tensor(548.4760, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 967/1000\n",
      "Loss G (total):  tensor(549.9103, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 968/1000\n",
      "Loss G (total):  tensor(550.2151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 969/1000\n",
      "Loss G (total):  tensor(551.3837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 970/1000\n",
      "Loss G (total):  tensor(551.1709, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 971/1000\n",
      "Loss G (total):  tensor(552.3362, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 972/1000\n",
      "Loss G (total):  tensor(553.5479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 973/1000\n",
      "Loss G (total):  tensor(554.1252, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 974/1000\n",
      "Loss G (total):  tensor(555.9039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 975/1000\n",
      "Loss G (total):  tensor(557.9500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 976/1000\n",
      "Loss G (total):  tensor(557.7731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 977/1000\n",
      "Loss G (total):  tensor(559.6464, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 978/1000\n",
      "Loss G (total):  tensor(559.6904, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 979/1000\n",
      "Loss G (total):  tensor(560.3668, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 980/1000\n",
      "Loss G (total):  tensor(560.6845, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 981/1000\n",
      "Loss G (total):  tensor(561.7769, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 982/1000\n",
      "Loss G (total):  tensor(564.3510, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 983/1000\n",
      "Loss G (total):  tensor(566.3044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 984/1000\n",
      "Loss G (total):  tensor(565.4359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 985/1000\n",
      "Loss G (total):  tensor(567.8041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 986/1000\n",
      "Loss G (total):  tensor(568.6324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 987/1000\n",
      "Loss G (total):  tensor(568.9792, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 988/1000\n",
      "Loss G (total):  tensor(569.4731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 989/1000\n",
      "Loss G (total):  tensor(572.0790, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 990/1000\n",
      "Loss G (total):  tensor(570.5953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 991/1000\n",
      "Loss G (total):  tensor(572.2308, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 992/1000\n",
      "Loss G (total):  tensor(573.9521, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 993/1000\n",
      "Loss G (total):  tensor(573.5358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 994/1000\n",
      "Loss G (total):  tensor(575.1038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 995/1000\n",
      "Loss G (total):  tensor(575.3387, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 996/1000\n",
      "Loss G (total):  tensor(577.6833, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 997/1000\n",
      "Loss G (total):  tensor(578.0742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 998/1000\n",
      "Loss G (total):  tensor(579.5226, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 999/1000\n",
      "Finish Synthetic Data Generation\n"
     ]
    }
   ],
   "source": [
    "from options_TGAN import Options\n",
    "from lib.TimeGAN import TimeGAN\n",
    "\n",
    "# 1. Options\n",
    "opt = Options().parse()\n",
    "\n",
    "# 2. Settings\n",
    "opt.seq_len = seq_len            # 256, mismo de arriba\n",
    "opt.iteration = 1000             # pocas iteraciones para probar\n",
    "opt.batch_size = 32              # batch pequeño\n",
    "opt.n_critic = 5                 # ya lo definiste para WGAN-GP\n",
    "opt.gp_lambda = 10.0             # peso del gradient penalty\n",
    "opt.name = \"TimeGAN_real_small\"  # nombre del experimento (carpeta de salida)\n",
    "\n",
    "# 3. Create model\n",
    "model = TimeGAN(opt, ori_data)\n",
    "\n",
    "# 4. Train\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a6c8e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧩 Generating 50 samples in 1 batches of 64...\n",
      "  ✅ Batch 1/1 generated (50 samples)\n"
     ]
    }
   ],
   "source": [
    "from generation_TGAN import safe_generation\n",
    "generated_data = safe_generation(model, num_samples=50, batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b7ec313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌀 Ejecutando t-SNE (puede tardar 20–40s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dario\\Desktop\\ThesiS JBP\\jordan_venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_vis = 1000  # target\n",
    "\n",
    "n_real = min(n_vis, len(ori_data))\n",
    "n_fake = min(n_vis, len(generated_data))\n",
    "\n",
    "idx_real = np.random.choice(len(ori_data), n_real, replace=False)\n",
    "idx_fake = np.random.choice(len(generated_data), n_fake, replace=False)\n",
    "\n",
    "ori_vis = [ori_data[i] for i in idx_real]\n",
    "gen_vis = [generated_data[i] for i in idx_fake]\n",
    "\n",
    "from visualization_TGAN import visualization\n",
    "\n",
    "visualization(ori_vis, gen_vis, 'pca')\n",
    "visualization(ori_vis, gen_vis, 'tsne')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification 1 : \n",
    "# Discriminator - Change the discriminator to WGAN-GP, delete Spectral Norm, no sigmoid  and add layernorm. \n",
    "# We did this in ordert to avoid saturation and collapse in discriminator. Normal TIMEGAN uses sigmoid + BCE  to classify real/fake, is classifficator not critic regression.\n",
    "# We delete the sigmoid to get real scores, dsicriminatori not classify now estiamte Wasserstein distance, delete spectral norm, and add layernorm to give stability.\n",
    "# We did this change because BCE tends to collapse, WGAN-GP produces smooth gradients, more stable. Is ideal for vibrations .\n",
    "\n",
    "# Modification 2: \n",
    "# We dont include LSTM , instead we add LayerNorm to all sub networks (5 networks also modified its forward functions accordign to this) in order to reduce the instability and collapse, specially in signals viration.\n",
    "#TimeGAN only has GRU raw, without normalziation, so high variation of activations , noise + lenght seq generates collapse and inestbaility.\n",
    "# So we add LayerNorm in each GRU Encoder, Recovery, Generator, Supervisor, Discriminator. Layernorm stabilize each step of time, GRU becomes more stable wiht real noise, training smooth. Improve convergence.\n",
    "# \n",
    "# \n",
    "#  Modification 3: \n",
    "# Add Gradient Penalty TimeGAN, and reeplce all the backward_d of discriminator to WGAN-GP. \n",
    "# Here th timegan original used the BCE which measures probabilities true or false, and have gradients 0 o 1. So we use WGAN, , and by force GRADIENT PENALTY.\n",
    "# WGAN measures the real distance between distributions, not probailities. Avoid collapse, is more stable for continuous signals, and and produce smooth training.\n",
    "\n",
    "# Modification 4:\n",
    "# Trainning Loop WGAN, in TIMEGAN it train 1:1 G y D, is bad for WGAN generates poor gradients. New modification in training loop we train the critic 5 times more than generator.\n",
    "# This is better because , critic is strong, gradients are high quality when send to generator. Convergence is stable.\n",
    "\n",
    "\n",
    "# Modification 5 : change the real data loading indata pre processing, now it overlap 75% of information. As vibration signals change fast, is not o stable. Considering overlaping 75% we are sure will take all infroamtion, noramlly papers consider 50,75 and 90%, in order to make \n",
    "# the model learn correctly. for example if u have 1000 points a windows of 200, and failure occurs between 350-400, u will have small infromation o nly one window, but if u apply 75% , u will have 6-7 windows. so will have more relvant infroamtion. So this help to catch transitions better.BaseExceptionimprove GAN trainning , increase the number of windows \n",
    "# and dont loose important parts per window.\n",
    "# Original timegan dont use overlapping, so have low windows, less transition, so bad representation of virbation. Now we use overlapping of 75%, so generate 4 times more windows, captures more information .\n",
    "# so more infromatio is better cause enrich timegan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4aa3d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Adjusted opt.z_dim to match data feature size: 6\n",
      "[DEBUG] Loss this iteration: 0.145167\n",
      "Encoder training step: 0/100\n",
      "[DEBUG] Loss this iteration: 0.111384\n",
      "Encoder training step: 1/100\n",
      "[DEBUG] Loss this iteration: 0.091704\n",
      "Encoder training step: 2/100\n",
      "[DEBUG] Loss this iteration: 0.061466\n",
      "Encoder training step: 3/100\n",
      "[DEBUG] Loss this iteration: 0.046996\n",
      "Encoder training step: 4/100\n",
      "[DEBUG] Loss this iteration: 0.039331\n",
      "Encoder training step: 5/100\n",
      "[DEBUG] Loss this iteration: 0.026369\n",
      "Encoder training step: 6/100\n",
      "[DEBUG] Loss this iteration: 0.018642\n",
      "Encoder training step: 7/100\n",
      "[DEBUG] Loss this iteration: 0.017502\n",
      "Encoder training step: 8/100\n",
      "[DEBUG] Loss this iteration: 0.016454\n",
      "Encoder training step: 9/100\n",
      "[DEBUG] Loss this iteration: 0.017662\n",
      "Encoder training step: 10/100\n",
      "[DEBUG] Loss this iteration: 0.018016\n",
      "Encoder training step: 11/100\n",
      "[DEBUG] Loss this iteration: 0.015132\n",
      "Encoder training step: 12/100\n",
      "[DEBUG] Loss this iteration: 0.019403\n",
      "Encoder training step: 13/100\n",
      "[DEBUG] Loss this iteration: 0.018092\n",
      "Encoder training step: 14/100\n",
      "[DEBUG] Loss this iteration: 0.016845\n",
      "Encoder training step: 15/100\n",
      "[DEBUG] Loss this iteration: 0.017196\n",
      "Encoder training step: 16/100\n",
      "[DEBUG] Loss this iteration: 0.017374\n",
      "Encoder training step: 17/100\n",
      "[DEBUG] Loss this iteration: 0.018453\n",
      "Encoder training step: 18/100\n",
      "[DEBUG] Loss this iteration: 0.018084\n",
      "Encoder training step: 19/100\n",
      "[DEBUG] Loss this iteration: 0.017100\n",
      "Encoder training step: 20/100\n",
      "[DEBUG] Loss this iteration: 0.017156\n",
      "Encoder training step: 21/100\n",
      "[DEBUG] Loss this iteration: 0.017134\n",
      "Encoder training step: 22/100\n",
      "[DEBUG] Loss this iteration: 0.019035\n",
      "Encoder training step: 23/100\n",
      "[DEBUG] Loss this iteration: 0.016850\n",
      "Encoder training step: 24/100\n",
      "[DEBUG] Loss this iteration: 0.015904\n",
      "Encoder training step: 25/100\n",
      "[DEBUG] Loss this iteration: 0.017111\n",
      "Encoder training step: 26/100\n",
      "[DEBUG] Loss this iteration: 0.015386\n",
      "Encoder training step: 27/100\n",
      "[DEBUG] Loss this iteration: 0.016466\n",
      "Encoder training step: 28/100\n",
      "[DEBUG] Loss this iteration: 0.016668\n",
      "Encoder training step: 29/100\n",
      "[DEBUG] Loss this iteration: 0.016192\n",
      "Encoder training step: 30/100\n",
      "[DEBUG] Loss this iteration: 0.016431\n",
      "Encoder training step: 31/100\n",
      "[DEBUG] Loss this iteration: 0.014446\n",
      "Encoder training step: 32/100\n",
      "[DEBUG] Loss this iteration: 0.016496\n",
      "Encoder training step: 33/100\n",
      "[DEBUG] Loss this iteration: 0.019337\n",
      "Encoder training step: 34/100\n",
      "[DEBUG] Loss this iteration: 0.018813\n",
      "Encoder training step: 35/100\n",
      "[DEBUG] Loss this iteration: 0.018294\n",
      "Encoder training step: 36/100\n",
      "[DEBUG] Loss this iteration: 0.011522\n",
      "Encoder training step: 37/100\n",
      "[DEBUG] Loss this iteration: 0.015841\n",
      "Encoder training step: 38/100\n",
      "[DEBUG] Loss this iteration: 0.016682\n",
      "Encoder training step: 39/100\n",
      "[DEBUG] Loss this iteration: 0.015683\n",
      "Encoder training step: 40/100\n",
      "[DEBUG] Loss this iteration: 0.017322\n",
      "Encoder training step: 41/100\n",
      "[DEBUG] Loss this iteration: 0.016898\n",
      "Encoder training step: 42/100\n",
      "[DEBUG] Loss this iteration: 0.013775\n",
      "Encoder training step: 43/100\n",
      "[DEBUG] Loss this iteration: 0.015852\n",
      "Encoder training step: 44/100\n",
      "[DEBUG] Loss this iteration: 0.016330\n",
      "Encoder training step: 45/100\n",
      "[DEBUG] Loss this iteration: 0.015097\n",
      "Encoder training step: 46/100\n",
      "[DEBUG] Loss this iteration: 0.014832\n",
      "Encoder training step: 47/100\n",
      "[DEBUG] Loss this iteration: 0.018974\n",
      "Encoder training step: 48/100\n",
      "[DEBUG] Loss this iteration: 0.016693\n",
      "Encoder training step: 49/100\n",
      "[DEBUG] Loss this iteration: 0.016586\n",
      "Encoder training step: 50/100\n",
      "[DEBUG] Loss this iteration: 0.013908\n",
      "Encoder training step: 51/100\n",
      "[DEBUG] Loss this iteration: 0.015968\n",
      "Encoder training step: 52/100\n",
      "[DEBUG] Loss this iteration: 0.015532\n",
      "Encoder training step: 53/100\n",
      "[DEBUG] Loss this iteration: 0.014824\n",
      "Encoder training step: 54/100\n",
      "[DEBUG] Loss this iteration: 0.013629\n",
      "Encoder training step: 55/100\n",
      "[DEBUG] Loss this iteration: 0.015570\n",
      "Encoder training step: 56/100\n",
      "[DEBUG] Loss this iteration: 0.017907\n",
      "Encoder training step: 57/100\n",
      "[DEBUG] Loss this iteration: 0.014926\n",
      "Encoder training step: 58/100\n",
      "[DEBUG] Loss this iteration: 0.015097\n",
      "Encoder training step: 59/100\n",
      "[DEBUG] Loss this iteration: 0.014314\n",
      "Encoder training step: 60/100\n",
      "[DEBUG] Loss this iteration: 0.015698\n",
      "Encoder training step: 61/100\n",
      "[DEBUG] Loss this iteration: 0.018378\n",
      "Encoder training step: 62/100\n",
      "[DEBUG] Loss this iteration: 0.015575\n",
      "Encoder training step: 63/100\n",
      "[DEBUG] Loss this iteration: 0.013087\n",
      "Encoder training step: 64/100\n",
      "[DEBUG] Loss this iteration: 0.020018\n",
      "Encoder training step: 65/100\n",
      "[DEBUG] Loss this iteration: 0.015409\n",
      "Encoder training step: 66/100\n",
      "[DEBUG] Loss this iteration: 0.016484\n",
      "Encoder training step: 67/100\n",
      "[DEBUG] Loss this iteration: 0.012810\n",
      "Encoder training step: 68/100\n",
      "[DEBUG] Loss this iteration: 0.015825\n",
      "Encoder training step: 69/100\n",
      "[DEBUG] Loss this iteration: 0.013685\n",
      "Encoder training step: 70/100\n",
      "[DEBUG] Loss this iteration: 0.016921\n",
      "Encoder training step: 71/100\n",
      "[DEBUG] Loss this iteration: 0.016561\n",
      "Encoder training step: 72/100\n",
      "[DEBUG] Loss this iteration: 0.017592\n",
      "Encoder training step: 73/100\n",
      "[DEBUG] Loss this iteration: 0.019224\n",
      "Encoder training step: 74/100\n",
      "[DEBUG] Loss this iteration: 0.016958\n",
      "Encoder training step: 75/100\n",
      "[DEBUG] Loss this iteration: 0.015233\n",
      "Encoder training step: 76/100\n",
      "[DEBUG] Loss this iteration: 0.012857\n",
      "Encoder training step: 77/100\n",
      "[DEBUG] Loss this iteration: 0.012212\n",
      "Encoder training step: 78/100\n",
      "[DEBUG] Loss this iteration: 0.014869\n",
      "Encoder training step: 79/100\n",
      "[DEBUG] Loss this iteration: 0.020314\n",
      "Encoder training step: 80/100\n",
      "[DEBUG] Loss this iteration: 0.011303\n",
      "Encoder training step: 81/100\n",
      "[DEBUG] Loss this iteration: 0.015507\n",
      "Encoder training step: 82/100\n",
      "[DEBUG] Loss this iteration: 0.014472\n",
      "Encoder training step: 83/100\n",
      "[DEBUG] Loss this iteration: 0.013920\n",
      "Encoder training step: 84/100\n",
      "[DEBUG] Loss this iteration: 0.012748\n",
      "Encoder training step: 85/100\n",
      "[DEBUG] Loss this iteration: 0.016033\n",
      "Encoder training step: 86/100\n",
      "[DEBUG] Loss this iteration: 0.014145\n",
      "Encoder training step: 87/100\n",
      "[DEBUG] Loss this iteration: 0.014375\n",
      "Encoder training step: 88/100\n",
      "[DEBUG] Loss this iteration: 0.015415\n",
      "Encoder training step: 89/100\n",
      "[DEBUG] Loss this iteration: 0.013344\n",
      "Encoder training step: 90/100\n",
      "[DEBUG] Loss this iteration: 0.016890\n",
      "Encoder training step: 91/100\n",
      "[DEBUG] Loss this iteration: 0.015113\n",
      "Encoder training step: 92/100\n",
      "[DEBUG] Loss this iteration: 0.013972\n",
      "Encoder training step: 93/100\n",
      "[DEBUG] Loss this iteration: 0.011942\n",
      "Encoder training step: 94/100\n",
      "[DEBUG] Loss this iteration: 0.016158\n",
      "Encoder training step: 95/100\n",
      "[DEBUG] Loss this iteration: 0.009467\n",
      "Encoder training step: 96/100\n",
      "[DEBUG] Loss this iteration: 0.015565\n",
      "Encoder training step: 97/100\n",
      "[DEBUG] Loss this iteration: 0.012770\n",
      "Encoder training step: 98/100\n",
      "[DEBUG] Loss this iteration: 0.015531\n",
      "Encoder training step: 99/100\n",
      "Loss S:  tensor(0.0846, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 0/100\n",
      "Loss S:  tensor(0.0809, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 1/100\n",
      "Loss S:  tensor(0.0758, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 2/100\n",
      "Loss S:  tensor(0.0723, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 3/100\n",
      "Loss S:  tensor(0.0680, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 4/100\n",
      "Loss S:  tensor(0.0639, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 5/100\n",
      "Loss S:  tensor(0.0611, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 6/100\n",
      "Loss S:  tensor(0.0580, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 7/100\n",
      "Loss S:  tensor(0.0535, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 8/100\n",
      "Loss S:  tensor(0.0497, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 9/100\n",
      "Loss S:  tensor(0.0473, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 10/100\n",
      "Loss S:  tensor(0.0441, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 11/100\n",
      "Loss S:  tensor(0.0409, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 12/100\n",
      "Loss S:  tensor(0.0382, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 13/100\n",
      "Loss S:  tensor(0.0345, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 14/100\n",
      "Loss S:  tensor(0.0320, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 15/100\n",
      "Loss S:  tensor(0.0294, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 16/100\n",
      "Loss S:  tensor(0.0272, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 17/100\n",
      "Loss S:  tensor(0.0247, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 18/100\n",
      "Loss S:  tensor(0.0224, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 19/100\n",
      "Loss S:  tensor(0.0216, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 20/100\n",
      "Loss S:  tensor(0.0199, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 21/100\n",
      "Loss S:  tensor(0.0189, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 22/100\n",
      "Loss S:  tensor(0.0175, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 23/100\n",
      "Loss S:  tensor(0.0177, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 24/100\n",
      "Loss S:  tensor(0.0164, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 25/100\n",
      "Loss S:  tensor(0.0144, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 26/100\n",
      "Loss S:  tensor(0.0144, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 27/100\n",
      "Loss S:  tensor(0.0126, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 28/100\n",
      "Loss S:  tensor(0.0120, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 29/100\n",
      "Loss S:  tensor(0.0113, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 30/100\n",
      "Loss S:  tensor(0.0107, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 31/100\n",
      "Loss S:  tensor(0.0103, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 32/100\n",
      "Loss S:  tensor(0.0094, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 33/100\n",
      "Loss S:  tensor(0.0100, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 34/100\n",
      "Loss S:  tensor(0.0094, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 35/100\n",
      "Loss S:  tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 36/100\n",
      "Loss S:  tensor(0.0081, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 37/100\n",
      "Loss S:  tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 38/100\n",
      "Loss S:  tensor(0.0073, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 39/100\n",
      "Loss S:  tensor(0.0078, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 40/100\n",
      "Loss S:  tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 41/100\n",
      "Loss S:  tensor(0.0075, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 42/100\n",
      "Loss S:  tensor(0.0063, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 43/100\n",
      "Loss S:  tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 44/100\n",
      "Loss S:  tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 45/100\n",
      "Loss S:  tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 46/100\n",
      "Loss S:  tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 47/100\n",
      "Loss S:  tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 48/100\n",
      "Loss S:  tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 49/100\n",
      "Loss S:  tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 50/100\n",
      "Loss S:  tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 51/100\n",
      "Loss S:  tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 52/100\n",
      "Loss S:  tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 53/100\n",
      "Loss S:  tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 54/100\n",
      "Loss S:  tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 55/100\n",
      "Loss S:  tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 56/100\n",
      "Loss S:  tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 57/100\n",
      "Loss S:  tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 58/100\n",
      "Loss S:  tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 59/100\n",
      "Loss S:  tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 60/100\n",
      "Loss S:  tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 61/100\n",
      "Loss S:  tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 62/100\n",
      "Loss S:  tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 63/100\n",
      "Loss S:  tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 64/100\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 65/100\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 66/100\n",
      "Loss S:  tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 67/100\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 68/100\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 69/100\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 70/100\n",
      "Loss S:  tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 71/100\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 72/100\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 73/100\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 74/100\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 75/100\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 76/100\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 77/100\n",
      "Loss S:  tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 78/100\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 79/100\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 80/100\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 81/100\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 82/100\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 83/100\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 84/100\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 85/100\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 86/100\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 87/100\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 88/100\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 89/100\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 90/100\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 91/100\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 92/100\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 93/100\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 94/100\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 95/100\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 96/100\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 97/100\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 98/100\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 99/100\n",
      "Loss G (total):  tensor(16.3235, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 0/100\n",
      "Loss G (total):  tensor(19.7423, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 1/100\n",
      "Loss G (total):  tensor(15.7866, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 2/100\n",
      "Loss G (total):  tensor(21.3697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 3/100\n",
      "Loss G (total):  tensor(18.1681, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 4/100\n",
      "Loss G (total):  tensor(19.9594, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 5/100\n",
      "Loss G (total):  tensor(19.8694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 6/100\n",
      "Loss G (total):  tensor(19.1765, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 7/100\n",
      "Loss G (total):  tensor(19.4721, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 8/100\n",
      "Loss G (total):  tensor(18.6358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 9/100\n",
      "Loss G (total):  tensor(18.7292, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 10/100\n",
      "Loss G (total):  tensor(20.7057, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 11/100\n",
      "Loss G (total):  tensor(20.2190, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 12/100\n",
      "Loss G (total):  tensor(22.9548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 13/100\n",
      "Loss G (total):  tensor(25.1890, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 14/100\n",
      "Loss G (total):  tensor(25.4858, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 15/100\n",
      "Loss G (total):  tensor(26.7764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 16/100\n",
      "Loss G (total):  tensor(26.2669, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 17/100\n",
      "Loss G (total):  tensor(27.2694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 18/100\n",
      "Loss G (total):  tensor(25.6228, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 19/100\n",
      "Loss G (total):  tensor(26.5173, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 20/100\n",
      "Loss G (total):  tensor(26.1450, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 21/100\n",
      "Loss G (total):  tensor(28.2852, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 22/100\n",
      "Loss G (total):  tensor(27.6358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 23/100\n",
      "Loss G (total):  tensor(25.5721, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 24/100\n",
      "Loss G (total):  tensor(26.4249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 25/100\n",
      "Loss G (total):  tensor(29.9946, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 26/100\n",
      "Loss G (total):  tensor(28.5015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 27/100\n",
      "Loss G (total):  tensor(29.9626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 28/100\n",
      "Loss G (total):  tensor(30.5901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 29/100\n",
      "Loss G (total):  tensor(29.8999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 30/100\n",
      "Loss G (total):  tensor(31.2858, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 31/100\n",
      "Loss G (total):  tensor(29.8102, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 32/100\n",
      "Loss G (total):  tensor(29.2996, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 33/100\n",
      "Loss G (total):  tensor(29.5732, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 34/100\n",
      "Loss G (total):  tensor(29.0503, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 35/100\n",
      "Loss G (total):  tensor(26.8042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 36/100\n",
      "Loss G (total):  tensor(26.2797, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 37/100\n",
      "Loss G (total):  tensor(28.6235, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 38/100\n",
      "Loss G (total):  tensor(27.5391, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 39/100\n",
      "Loss G (total):  tensor(27.4253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 40/100\n",
      "Loss G (total):  tensor(29.6392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 41/100\n",
      "Loss G (total):  tensor(30.0542, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 42/100\n",
      "Loss G (total):  tensor(29.8631, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 43/100\n",
      "Loss G (total):  tensor(27.7532, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 44/100\n",
      "Loss G (total):  tensor(28.9554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 45/100\n",
      "Loss G (total):  tensor(27.3791, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 46/100\n",
      "Loss G (total):  tensor(28.2923, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 47/100\n",
      "Loss G (total):  tensor(26.4764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 48/100\n",
      "Loss G (total):  tensor(30.0299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 49/100\n",
      "Loss G (total):  tensor(28.7686, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 50/100\n",
      "Loss G (total):  tensor(29.5390, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 51/100\n",
      "Loss G (total):  tensor(30.0304, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 52/100\n",
      "Loss G (total):  tensor(28.7914, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 53/100\n",
      "Loss G (total):  tensor(29.5031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 54/100\n",
      "Loss G (total):  tensor(30.1065, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 55/100\n",
      "Loss G (total):  tensor(28.5960, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 56/100\n",
      "Loss G (total):  tensor(28.8949, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 57/100\n",
      "Loss G (total):  tensor(32.2188, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 58/100\n",
      "Loss G (total):  tensor(29.3073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 59/100\n",
      "Loss G (total):  tensor(31.9266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 60/100\n",
      "Loss G (total):  tensor(33.9547, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 61/100\n",
      "Loss G (total):  tensor(31.0046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 62/100\n",
      "Loss G (total):  tensor(31.2922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 63/100\n",
      "Loss G (total):  tensor(29.7139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 64/100\n",
      "Loss G (total):  tensor(30.6888, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 65/100\n",
      "Loss G (total):  tensor(33.1409, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 66/100\n",
      "Loss G (total):  tensor(30.1718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 67/100\n",
      "Loss G (total):  tensor(31.8798, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 68/100\n",
      "Loss G (total):  tensor(33.8873, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 69/100\n",
      "Loss G (total):  tensor(30.7093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 70/100\n",
      "Loss G (total):  tensor(33.5979, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 71/100\n",
      "Loss G (total):  tensor(31.2871, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 72/100\n",
      "Loss G (total):  tensor(32.0288, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 73/100\n",
      "Loss G (total):  tensor(30.0682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 74/100\n",
      "Loss G (total):  tensor(29.6672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 75/100\n",
      "Loss G (total):  tensor(29.2027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 76/100\n",
      "Loss G (total):  tensor(32.8708, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 77/100\n",
      "Loss G (total):  tensor(30.4226, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 78/100\n",
      "Loss G (total):  tensor(33.6742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 79/100\n",
      "Loss G (total):  tensor(29.4959, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 80/100\n",
      "Loss G (total):  tensor(30.6367, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 81/100\n",
      "Loss G (total):  tensor(33.2570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 82/100\n",
      "Loss G (total):  tensor(31.7886, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 83/100\n",
      "Loss G (total):  tensor(30.3431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 84/100\n",
      "Loss G (total):  tensor(30.4008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 85/100\n",
      "Loss G (total):  tensor(33.1589, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 86/100\n",
      "Loss G (total):  tensor(31.3342, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 87/100\n",
      "Loss G (total):  tensor(30.9817, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 88/100\n",
      "Loss G (total):  tensor(34.3133, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 89/100\n",
      "Loss G (total):  tensor(32.3316, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 90/100\n",
      "Loss G (total):  tensor(35.7262, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 91/100\n",
      "Loss G (total):  tensor(32.0027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 92/100\n",
      "Loss G (total):  tensor(38.2840, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 93/100\n",
      "Loss G (total):  tensor(33.9397, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 94/100\n",
      "Loss G (total):  tensor(33.1172, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 95/100\n",
      "Loss G (total):  tensor(35.1080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 96/100\n",
      "Loss G (total):  tensor(31.1690, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 97/100\n",
      "Loss G (total):  tensor(31.3048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 98/100\n",
      "Loss G (total):  tensor(35.1494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 99/100\n",
      "Finish Synthetic Data Generation\n"
     ]
    }
   ],
   "source": [
    "# This point we modified the PCA to make multivariable comparisson . To properly compare real and synthetic sequences, you must use all variables and all time steps.\n",
    "# Before visualization averaged all variables, not reliable for evaluating. now we flatten the full multivariate time-series (all variables included) so PCA and t-SNE compare complete real and generated sequences correctly\n",
    "\n",
    "from options_TGAN import Options\n",
    "from lib.TimeGAN import TimeGAN\n",
    "\n",
    "# 1. Options\n",
    "opt = Options().parse()\n",
    "\n",
    "# 2. Set paper-style hyperparameters\n",
    "opt.seq_len = seq_len            # 256\n",
    "opt.lr = 1e-4                    # Learning rate paper\n",
    "opt.beta1 = 0.5                  # Momentum recomend\n",
    "opt.batch_size = 64              # Batch paper\n",
    "opt.iteration = 100               # Iteraciones \n",
    "opt.n_critic = 5                 # WGAN-GP \n",
    "opt.gp_lambda = 10.0             # Weight penalty\n",
    "opt.name = \"TimeGAN_real_paper_settings\"\n",
    "\n",
    "# 3. Create model\n",
    "model = TimeGAN(opt, ori_data)\n",
    "\n",
    "# 4. Train\n",
    "model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c3aa7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧩 Generating 50 samples in 1 batches of 64...\n",
      "  ✅ Batch 1/1 generated (50 samples)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌀 Ejecutando t-SNE (puede tardar unos segundos)...\n",
      "[t-SNE] Computing 99 nearest neighbors...\n",
      "[t-SNE] Indexed 100 samples in 0.015s...\n",
      "[t-SNE] Computed neighbors for 100 samples in 0.016s...\n",
      "[t-SNE] Computed conditional probabilities for sample 100 / 100\n",
      "[t-SNE] Mean sigma: 2.076680\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 47.191734\n",
      "[t-SNE] KL divergence after 500 iterations: 0.101105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dario\\Desktop\\ThesiS JBP\\jordan_venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from generation_TGAN import safe_generation\n",
    "generated_data = safe_generation(model, num_samples=50, batch_size=64)\n",
    "\n",
    "n_vis = 5000  # target\n",
    "\n",
    "n_real = min(n_vis, len(ori_data))\n",
    "n_fake = min(n_vis, len(generated_data))\n",
    "\n",
    "idx_real = np.random.choice(len(ori_data), n_real, replace=False)\n",
    "idx_fake = np.random.choice(len(generated_data), n_fake, replace=False)\n",
    "\n",
    "ori_vis = [ori_data[i] for i in idx_real]\n",
    "gen_vis = [generated_data[i] for i in idx_fake]\n",
    "\n",
    "from visualization_TGAN import visualization\n",
    "\n",
    "visualization(ori_vis, gen_vis, 'pca')\n",
    "visualization(ori_vis, gen_vis, 'tsne')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6483ae22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌀 Ejecutando t-SNE (puede tardar unos segundos)...\n",
      "[t-SNE] Computing 99 nearest neighbors...\n",
      "[t-SNE] Indexed 100 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 100 samples in 0.031s...\n",
      "[t-SNE] Computed conditional probabilities for sample 100 / 100\n",
      "[t-SNE] Mean sigma: 1.271805\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 45.873119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dario\\Desktop\\ThesiS JBP\\jordan_venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] KL divergence after 500 iterations: 0.057000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07a592a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Adjusted opt.z_dim to match data feature size: 6\n",
      "[DEBUG] Loss this iteration: 0.096269\n",
      "Encoder training step: 0/100\n",
      "[DEBUG] Loss this iteration: 0.057858\n",
      "Encoder training step: 1/100\n",
      "[DEBUG] Loss this iteration: 0.037553\n",
      "Encoder training step: 2/100\n",
      "[DEBUG] Loss this iteration: 0.025531\n",
      "Encoder training step: 3/100\n",
      "[DEBUG] Loss this iteration: 0.019284\n",
      "Encoder training step: 4/100\n",
      "[DEBUG] Loss this iteration: 0.018414\n",
      "Encoder training step: 5/100\n",
      "[DEBUG] Loss this iteration: 0.016636\n",
      "Encoder training step: 6/100\n",
      "[DEBUG] Loss this iteration: 0.019950\n",
      "Encoder training step: 7/100\n",
      "[DEBUG] Loss this iteration: 0.017644\n",
      "Encoder training step: 8/100\n",
      "[DEBUG] Loss this iteration: 0.018578\n",
      "Encoder training step: 9/100\n",
      "[DEBUG] Loss this iteration: 0.015881\n",
      "Encoder training step: 10/100\n",
      "[DEBUG] Loss this iteration: 0.017772\n",
      "Encoder training step: 11/100\n",
      "[DEBUG] Loss this iteration: 0.019119\n",
      "Encoder training step: 12/100\n",
      "[DEBUG] Loss this iteration: 0.016356\n",
      "Encoder training step: 13/100\n",
      "[DEBUG] Loss this iteration: 0.017848\n",
      "Encoder training step: 14/100\n",
      "[DEBUG] Loss this iteration: 0.016625\n",
      "Encoder training step: 15/100\n",
      "[DEBUG] Loss this iteration: 0.018294\n",
      "Encoder training step: 16/100\n",
      "[DEBUG] Loss this iteration: 0.017049\n",
      "Encoder training step: 17/100\n",
      "[DEBUG] Loss this iteration: 0.017384\n",
      "Encoder training step: 18/100\n",
      "[DEBUG] Loss this iteration: 0.015181\n",
      "Encoder training step: 19/100\n",
      "[DEBUG] Loss this iteration: 0.017666\n",
      "Encoder training step: 20/100\n",
      "[DEBUG] Loss this iteration: 0.014245\n",
      "Encoder training step: 21/100\n",
      "[DEBUG] Loss this iteration: 0.017963\n",
      "Encoder training step: 22/100\n",
      "[DEBUG] Loss this iteration: 0.014016\n",
      "Encoder training step: 23/100\n",
      "[DEBUG] Loss this iteration: 0.020553\n",
      "Encoder training step: 24/100\n",
      "[DEBUG] Loss this iteration: 0.019022\n",
      "Encoder training step: 25/100\n",
      "[DEBUG] Loss this iteration: 0.017768\n",
      "Encoder training step: 26/100\n",
      "[DEBUG] Loss this iteration: 0.015570\n",
      "Encoder training step: 27/100\n",
      "[DEBUG] Loss this iteration: 0.014057\n",
      "Encoder training step: 28/100\n",
      "[DEBUG] Loss this iteration: 0.013950\n",
      "Encoder training step: 29/100\n",
      "[DEBUG] Loss this iteration: 0.013809\n",
      "Encoder training step: 30/100\n",
      "[DEBUG] Loss this iteration: 0.018893\n",
      "Encoder training step: 31/100\n",
      "[DEBUG] Loss this iteration: 0.017758\n",
      "Encoder training step: 32/100\n",
      "[DEBUG] Loss this iteration: 0.014273\n",
      "Encoder training step: 33/100\n",
      "[DEBUG] Loss this iteration: 0.019223\n",
      "Encoder training step: 34/100\n",
      "[DEBUG] Loss this iteration: 0.015484\n",
      "Encoder training step: 35/100\n",
      "[DEBUG] Loss this iteration: 0.015527\n",
      "Encoder training step: 36/100\n",
      "[DEBUG] Loss this iteration: 0.019997\n",
      "Encoder training step: 37/100\n",
      "[DEBUG] Loss this iteration: 0.018811\n",
      "Encoder training step: 38/100\n",
      "[DEBUG] Loss this iteration: 0.020378\n",
      "Encoder training step: 39/100\n",
      "[DEBUG] Loss this iteration: 0.016873\n",
      "Encoder training step: 40/100\n",
      "[DEBUG] Loss this iteration: 0.017814\n",
      "Encoder training step: 41/100\n",
      "[DEBUG] Loss this iteration: 0.016208\n",
      "Encoder training step: 42/100\n",
      "[DEBUG] Loss this iteration: 0.014490\n",
      "Encoder training step: 43/100\n",
      "[DEBUG] Loss this iteration: 0.015987\n",
      "Encoder training step: 44/100\n",
      "[DEBUG] Loss this iteration: 0.014306\n",
      "Encoder training step: 45/100\n",
      "[DEBUG] Loss this iteration: 0.015132\n",
      "Encoder training step: 46/100\n",
      "[DEBUG] Loss this iteration: 0.013605\n",
      "Encoder training step: 47/100\n",
      "[DEBUG] Loss this iteration: 0.015631\n",
      "Encoder training step: 48/100\n",
      "[DEBUG] Loss this iteration: 0.013975\n",
      "Encoder training step: 49/100\n",
      "[DEBUG] Loss this iteration: 0.016947\n",
      "Encoder training step: 50/100\n",
      "[DEBUG] Loss this iteration: 0.016234\n",
      "Encoder training step: 51/100\n",
      "[DEBUG] Loss this iteration: 0.017815\n",
      "Encoder training step: 52/100\n",
      "[DEBUG] Loss this iteration: 0.014583\n",
      "Encoder training step: 53/100\n",
      "[DEBUG] Loss this iteration: 0.013971\n",
      "Encoder training step: 54/100\n",
      "[DEBUG] Loss this iteration: 0.015073\n",
      "Encoder training step: 55/100\n",
      "[DEBUG] Loss this iteration: 0.014784\n",
      "Encoder training step: 56/100\n",
      "[DEBUG] Loss this iteration: 0.015954\n",
      "Encoder training step: 57/100\n",
      "[DEBUG] Loss this iteration: 0.013688\n",
      "Encoder training step: 58/100\n",
      "[DEBUG] Loss this iteration: 0.016647\n",
      "Encoder training step: 59/100\n",
      "[DEBUG] Loss this iteration: 0.015672\n",
      "Encoder training step: 60/100\n",
      "[DEBUG] Loss this iteration: 0.016365\n",
      "Encoder training step: 61/100\n",
      "[DEBUG] Loss this iteration: 0.016117\n",
      "Encoder training step: 62/100\n",
      "[DEBUG] Loss this iteration: 0.013785\n",
      "Encoder training step: 63/100\n",
      "[DEBUG] Loss this iteration: 0.015711\n",
      "Encoder training step: 64/100\n",
      "[DEBUG] Loss this iteration: 0.018106\n",
      "Encoder training step: 65/100\n",
      "[DEBUG] Loss this iteration: 0.014302\n",
      "Encoder training step: 66/100\n",
      "[DEBUG] Loss this iteration: 0.017732\n",
      "Encoder training step: 67/100\n",
      "[DEBUG] Loss this iteration: 0.015081\n",
      "Encoder training step: 68/100\n",
      "[DEBUG] Loss this iteration: 0.015653\n",
      "Encoder training step: 69/100\n",
      "[DEBUG] Loss this iteration: 0.016071\n",
      "Encoder training step: 70/100\n",
      "[DEBUG] Loss this iteration: 0.014139\n",
      "Encoder training step: 71/100\n",
      "[DEBUG] Loss this iteration: 0.015121\n",
      "Encoder training step: 72/100\n",
      "[DEBUG] Loss this iteration: 0.017122\n",
      "Encoder training step: 73/100\n",
      "[DEBUG] Loss this iteration: 0.014185\n",
      "Encoder training step: 74/100\n",
      "[DEBUG] Loss this iteration: 0.013340\n",
      "Encoder training step: 75/100\n",
      "[DEBUG] Loss this iteration: 0.012463\n",
      "Encoder training step: 76/100\n",
      "[DEBUG] Loss this iteration: 0.011394\n",
      "Encoder training step: 77/100\n",
      "[DEBUG] Loss this iteration: 0.013723\n",
      "Encoder training step: 78/100\n",
      "[DEBUG] Loss this iteration: 0.013563\n",
      "Encoder training step: 79/100\n",
      "[DEBUG] Loss this iteration: 0.014732\n",
      "Encoder training step: 80/100\n",
      "[DEBUG] Loss this iteration: 0.014487\n",
      "Encoder training step: 81/100\n",
      "[DEBUG] Loss this iteration: 0.013996\n",
      "Encoder training step: 82/100\n",
      "[DEBUG] Loss this iteration: 0.013958\n",
      "Encoder training step: 83/100\n",
      "[DEBUG] Loss this iteration: 0.016363\n",
      "Encoder training step: 84/100\n",
      "[DEBUG] Loss this iteration: 0.014829\n",
      "Encoder training step: 85/100\n",
      "[DEBUG] Loss this iteration: 0.011528\n",
      "Encoder training step: 86/100\n",
      "[DEBUG] Loss this iteration: 0.013330\n",
      "Encoder training step: 87/100\n",
      "[DEBUG] Loss this iteration: 0.015376\n",
      "Encoder training step: 88/100\n",
      "[DEBUG] Loss this iteration: 0.015580\n",
      "Encoder training step: 89/100\n",
      "[DEBUG] Loss this iteration: 0.016191\n",
      "Encoder training step: 90/100\n",
      "[DEBUG] Loss this iteration: 0.013772\n",
      "Encoder training step: 91/100\n",
      "[DEBUG] Loss this iteration: 0.014623\n",
      "Encoder training step: 92/100\n",
      "[DEBUG] Loss this iteration: 0.014759\n",
      "Encoder training step: 93/100\n",
      "[DEBUG] Loss this iteration: 0.012843\n",
      "Encoder training step: 94/100\n",
      "[DEBUG] Loss this iteration: 0.016441\n",
      "Encoder training step: 95/100\n",
      "[DEBUG] Loss this iteration: 0.015265\n",
      "Encoder training step: 96/100\n",
      "[DEBUG] Loss this iteration: 0.014449\n",
      "Encoder training step: 97/100\n",
      "[DEBUG] Loss this iteration: 0.013811\n",
      "Encoder training step: 98/100\n",
      "[DEBUG] Loss this iteration: 0.015890\n",
      "Encoder training step: 99/100\n",
      "Loss S:  tensor(0.0770, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 0/100\n",
      "Loss S:  tensor(0.0709, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 1/100\n",
      "Loss S:  tensor(0.0664, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 2/100\n",
      "Loss S:  tensor(0.0603, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 3/100\n",
      "Loss S:  tensor(0.0550, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 4/100\n",
      "Loss S:  tensor(0.0520, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 5/100\n",
      "Loss S:  tensor(0.0457, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 6/100\n",
      "Loss S:  tensor(0.0434, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 7/100\n",
      "Loss S:  tensor(0.0394, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 8/100\n",
      "Loss S:  tensor(0.0358, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 9/100\n",
      "Loss S:  tensor(0.0335, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 10/100\n",
      "Loss S:  tensor(0.0300, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 11/100\n",
      "Loss S:  tensor(0.0285, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 12/100\n",
      "Loss S:  tensor(0.0250, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 13/100\n",
      "Loss S:  tensor(0.0240, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 14/100\n",
      "Loss S:  tensor(0.0220, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 15/100\n",
      "Loss S:  tensor(0.0198, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 16/100\n",
      "Loss S:  tensor(0.0199, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 17/100\n",
      "Loss S:  tensor(0.0183, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 18/100\n",
      "Loss S:  tensor(0.0165, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 19/100\n",
      "Loss S:  tensor(0.0148, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 20/100\n",
      "Loss S:  tensor(0.0140, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 21/100\n",
      "Loss S:  tensor(0.0140, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 22/100\n",
      "Loss S:  tensor(0.0121, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 23/100\n",
      "Loss S:  tensor(0.0120, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 24/100\n",
      "Loss S:  tensor(0.0106, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 25/100\n",
      "Loss S:  tensor(0.0108, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 26/100\n",
      "Loss S:  tensor(0.0097, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 27/100\n",
      "Loss S:  tensor(0.0096, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 28/100\n",
      "Loss S:  tensor(0.0086, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 29/100\n",
      "Loss S:  tensor(0.0088, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 30/100\n",
      "Loss S:  tensor(0.0078, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 31/100\n",
      "Loss S:  tensor(0.0075, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 32/100\n",
      "Loss S:  tensor(0.0070, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 33/100\n",
      "Loss S:  tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 34/100\n",
      "Loss S:  tensor(0.0069, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 35/100\n",
      "Loss S:  tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 36/100\n",
      "Loss S:  tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 37/100\n",
      "Loss S:  tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 38/100\n",
      "Loss S:  tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 39/100\n",
      "Loss S:  tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 40/100\n",
      "Loss S:  tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 41/100\n",
      "Loss S:  tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 42/100\n",
      "Loss S:  tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 43/100\n",
      "Loss S:  tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 44/100\n",
      "Loss S:  tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 45/100\n",
      "Loss S:  tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 46/100\n",
      "Loss S:  tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 47/100\n",
      "Loss S:  tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 48/100\n",
      "Loss S:  tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 49/100\n",
      "Loss S:  tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 50/100\n",
      "Loss S:  tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 51/100\n",
      "Loss S:  tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 52/100\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 53/100\n",
      "Loss S:  tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 54/100\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 55/100\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 56/100\n",
      "Loss S:  tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 57/100\n",
      "Loss S:  tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 58/100\n",
      "Loss S:  tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 59/100\n",
      "Loss S:  tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 60/100\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 61/100\n",
      "Loss S:  tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 62/100\n",
      "Loss S:  tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 63/100\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 64/100\n",
      "Loss S:  tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 65/100\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 66/100\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 67/100\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 68/100\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 69/100\n",
      "Loss S:  tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 70/100\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 71/100\n",
      "Loss S:  tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 72/100\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 73/100\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 74/100\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 75/100\n",
      "Loss S:  tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 76/100\n",
      "Loss S:  tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 77/100\n",
      "Loss S:  tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 78/100\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 79/100\n",
      "Loss S:  tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 80/100\n",
      "Loss S:  tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 81/100\n",
      "Loss S:  tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 82/100\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 83/100\n",
      "Loss S:  tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 84/100\n",
      "Loss S:  tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 85/100\n",
      "Loss S:  tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 86/100\n",
      "Loss S:  tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 87/100\n",
      "Loss S:  tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 88/100\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 89/100\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 90/100\n",
      "Loss S:  tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 91/100\n",
      "Loss S:  tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 92/100\n",
      "Loss S:  tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 93/100\n",
      "Loss S:  tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 94/100\n",
      "Loss S:  tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 95/100\n",
      "Loss S:  tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 96/100\n",
      "Loss S:  tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 97/100\n",
      "Loss S:  tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 98/100\n",
      "Loss S:  tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 99/100\n",
      "Loss G (total):  tensor(14.5669, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 0/100\n",
      "Loss G (total):  tensor(20.6440, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 1/100\n",
      "Loss G (total):  tensor(19.3367, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 2/100\n",
      "Loss G (total):  tensor(13.2354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 3/100\n",
      "Loss G (total):  tensor(13.2982, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 4/100\n",
      "Loss G (total):  tensor(19.2371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 5/100\n",
      "Loss G (total):  tensor(15.4822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 6/100\n",
      "Loss G (total):  tensor(18.2266, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 7/100\n",
      "Loss G (total):  tensor(18.6797, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 8/100\n",
      "Loss G (total):  tensor(20.6544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 9/100\n",
      "Loss G (total):  tensor(22.4058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 10/100\n",
      "Loss G (total):  tensor(22.0430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 11/100\n",
      "Loss G (total):  tensor(22.9975, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 12/100\n",
      "Loss G (total):  tensor(24.3270, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 13/100\n",
      "Loss G (total):  tensor(21.2360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 14/100\n",
      "Loss G (total):  tensor(24.6774, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 15/100\n",
      "Loss G (total):  tensor(27.5139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 16/100\n",
      "Loss G (total):  tensor(26.9837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 17/100\n",
      "Loss G (total):  tensor(28.0208, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 18/100\n",
      "Loss G (total):  tensor(27.4983, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 19/100\n",
      "Loss G (total):  tensor(29.7697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 20/100\n",
      "Loss G (total):  tensor(26.7306, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 21/100\n",
      "Loss G (total):  tensor(28.2357, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 22/100\n",
      "Loss G (total):  tensor(32.5713, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 23/100\n",
      "Loss G (total):  tensor(28.9942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 24/100\n",
      "Loss G (total):  tensor(30.3629, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 25/100\n",
      "Loss G (total):  tensor(30.8901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 26/100\n",
      "Loss G (total):  tensor(31.5380, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 27/100\n",
      "Loss G (total):  tensor(29.3498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 28/100\n",
      "Loss G (total):  tensor(28.6235, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 29/100\n",
      "Loss G (total):  tensor(30.3029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 30/100\n",
      "Loss G (total):  tensor(29.1627, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 31/100\n",
      "Loss G (total):  tensor(31.3051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 32/100\n",
      "Loss G (total):  tensor(29.8832, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 33/100\n",
      "Loss G (total):  tensor(32.3488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 34/100\n",
      "Loss G (total):  tensor(32.1640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 35/100\n",
      "Loss G (total):  tensor(33.1930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 36/100\n",
      "Loss G (total):  tensor(33.4922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 37/100\n",
      "Loss G (total):  tensor(31.6528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 38/100\n",
      "Loss G (total):  tensor(31.5558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 39/100\n",
      "Loss G (total):  tensor(33.3489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 40/100\n",
      "Loss G (total):  tensor(31.7867, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 41/100\n",
      "Loss G (total):  tensor(33.7451, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 42/100\n",
      "Loss G (total):  tensor(30.5509, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 43/100\n",
      "Loss G (total):  tensor(31.9443, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 44/100\n",
      "Loss G (total):  tensor(36.2403, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 45/100\n",
      "Loss G (total):  tensor(31.8030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 46/100\n",
      "Loss G (total):  tensor(34.4477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 47/100\n",
      "Loss G (total):  tensor(33.2803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 48/100\n",
      "Loss G (total):  tensor(31.6703, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 49/100\n",
      "Loss G (total):  tensor(31.1086, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 50/100\n",
      "Loss G (total):  tensor(30.8499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 51/100\n",
      "Loss G (total):  tensor(33.9739, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 52/100\n",
      "Loss G (total):  tensor(33.9019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 53/100\n",
      "Loss G (total):  tensor(34.0513, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 54/100\n",
      "Loss G (total):  tensor(38.6850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 55/100\n",
      "Loss G (total):  tensor(35.7231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 56/100\n",
      "Loss G (total):  tensor(35.0882, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 57/100\n",
      "Loss G (total):  tensor(31.2036, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 58/100\n",
      "Loss G (total):  tensor(33.3655, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 59/100\n",
      "Loss G (total):  tensor(32.0607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 60/100\n",
      "Loss G (total):  tensor(33.7442, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 61/100\n",
      "Loss G (total):  tensor(34.9529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 62/100\n",
      "Loss G (total):  tensor(32.3438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 63/100\n",
      "Loss G (total):  tensor(34.9527, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 64/100\n",
      "Loss G (total):  tensor(32.4393, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 65/100\n",
      "Loss G (total):  tensor(33.6341, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 66/100\n",
      "Loss G (total):  tensor(31.1238, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 67/100\n",
      "Loss G (total):  tensor(32.2819, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 68/100\n",
      "Loss G (total):  tensor(33.0001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 69/100\n",
      "Loss G (total):  tensor(33.3889, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 70/100\n",
      "Loss G (total):  tensor(34.1465, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 71/100\n",
      "Loss G (total):  tensor(33.1005, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 72/100\n",
      "Loss G (total):  tensor(33.5949, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 73/100\n",
      "Loss G (total):  tensor(37.6562, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 74/100\n",
      "Loss G (total):  tensor(37.6704, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 75/100\n",
      "Loss G (total):  tensor(32.0813, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 76/100\n",
      "Loss G (total):  tensor(37.0065, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 77/100\n",
      "Loss G (total):  tensor(34.6281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 78/100\n",
      "Loss G (total):  tensor(36.7076, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 79/100\n",
      "Loss G (total):  tensor(35.1107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 80/100\n",
      "Loss G (total):  tensor(33.8303, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 81/100\n",
      "Loss G (total):  tensor(34.2992, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 82/100\n",
      "Loss G (total):  tensor(33.3957, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 83/100\n",
      "Loss G (total):  tensor(36.6547, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 84/100\n",
      "Loss G (total):  tensor(35.1007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 85/100\n",
      "Loss G (total):  tensor(35.3493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 86/100\n",
      "Loss G (total):  tensor(33.6338, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 87/100\n",
      "Loss G (total):  tensor(38.1007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 88/100\n",
      "Loss G (total):  tensor(38.9466, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 89/100\n",
      "Loss G (total):  tensor(36.5005, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 90/100\n",
      "Loss G (total):  tensor(38.6764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 91/100\n",
      "Loss G (total):  tensor(37.8158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 92/100\n",
      "Loss G (total):  tensor(36.0562, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 93/100\n",
      "Loss G (total):  tensor(34.7593, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 94/100\n",
      "Loss G (total):  tensor(36.4399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 95/100\n",
      "Loss G (total):  tensor(36.3647, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 96/100\n",
      "Loss G (total):  tensor(36.1747, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 97/100\n",
      "Loss G (total):  tensor(34.4927, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 98/100\n",
      "Loss G (total):  tensor(35.3106, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 99/100\n",
      "Finish Synthetic Data Generation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from options_TGAN import Options\n",
    "from lib.TimeGAN import TimeGAN\n",
    "\n",
    "# 1. Options\n",
    "opt = Options().parse()\n",
    "\n",
    "# 2. Set paper-style hyperparameters\n",
    "opt.lr = 1e-4\n",
    "opt.beta1 = 0.5\n",
    "\n",
    "opt.batch_size = 64\n",
    "opt.iteration = 100\n",
    "\n",
    "opt.hidden_dim = 128   # muy importante\n",
    "opt.num_layer = 3\n",
    "\n",
    "opt.n_critic = 5       # OK\n",
    "opt.gp_lambda = 10.0   # OK\n",
    "opt.name = \"TimeGAN_real_paper_settings\"\n",
    "\n",
    "# 3. Create model\n",
    "model = TimeGAN(opt, ori_data)\n",
    "\n",
    "# 4. Train\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd1b2c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧩 Generating 50 samples in 1 batches of 64...\n",
      "  ✅ Batch 1/1 generated (50 samples)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌀 Ejecutando t-SNE (puede tardar unos segundos)...\n",
      "[t-SNE] Computing 99 nearest neighbors...\n",
      "[t-SNE] Indexed 100 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 100 samples in 0.022s...\n",
      "[t-SNE] Computed conditional probabilities for sample 100 / 100\n",
      "[t-SNE] Mean sigma: 1.853221\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 49.559792\n",
      "[t-SNE] KL divergence after 500 iterations: 0.076414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dario\\Desktop\\ThesiS JBP\\jordan_venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from generation_TGAN import safe_generation\n",
    "generated_data = safe_generation(model, num_samples=50, batch_size=64)\n",
    "\n",
    "n_vis = 5000  # target\n",
    "\n",
    "n_real = min(n_vis, len(ori_data))\n",
    "n_fake = min(n_vis, len(generated_data))\n",
    "\n",
    "idx_real = np.random.choice(len(ori_data), n_real, replace=False)\n",
    "idx_fake = np.random.choice(len(generated_data), n_fake, replace=False)\n",
    "\n",
    "ori_vis = [ori_data[i] for i in idx_real]\n",
    "gen_vis = [generated_data[i] for i in idx_fake]\n",
    "\n",
    "from visualization_TGAN import visualization\n",
    "\n",
    "visualization(ori_vis, gen_vis, 'pca')\n",
    "visualization(ori_vis, gen_vis, 'tsne')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30421ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Adjusted opt.z_dim to match data feature size: 6\n",
      "[DEBUG] Loss this iteration: 0.157543\n",
      "Encoder training step: 0/50\n",
      "[DEBUG] Loss this iteration: 0.102054\n",
      "Encoder training step: 1/50\n",
      "[DEBUG] Loss this iteration: 0.049876\n",
      "Encoder training step: 2/50\n",
      "[DEBUG] Loss this iteration: 0.029747\n",
      "Encoder training step: 3/50\n",
      "[DEBUG] Loss this iteration: 0.021945\n",
      "Encoder training step: 4/50\n",
      "[DEBUG] Loss this iteration: 0.013907\n",
      "Encoder training step: 5/50\n",
      "[DEBUG] Loss this iteration: 0.020296\n",
      "Encoder training step: 6/50\n",
      "[DEBUG] Loss this iteration: 0.017364\n",
      "Encoder training step: 7/50\n",
      "[DEBUG] Loss this iteration: 0.020229\n",
      "Encoder training step: 8/50\n",
      "[DEBUG] Loss this iteration: 0.015434\n",
      "Encoder training step: 9/50\n",
      "[DEBUG] Loss this iteration: 0.016949\n",
      "Encoder training step: 10/50\n",
      "[DEBUG] Loss this iteration: 0.013384\n",
      "Encoder training step: 11/50\n",
      "[DEBUG] Loss this iteration: 0.016507\n",
      "Encoder training step: 12/50\n",
      "[DEBUG] Loss this iteration: 0.017443\n",
      "Encoder training step: 13/50\n",
      "[DEBUG] Loss this iteration: 0.017676\n",
      "Encoder training step: 14/50\n",
      "[DEBUG] Loss this iteration: 0.019791\n",
      "Encoder training step: 15/50\n",
      "[DEBUG] Loss this iteration: 0.018146\n",
      "Encoder training step: 16/50\n",
      "[DEBUG] Loss this iteration: 0.015773\n",
      "Encoder training step: 17/50\n",
      "[DEBUG] Loss this iteration: 0.016741\n",
      "Encoder training step: 18/50\n",
      "[DEBUG] Loss this iteration: 0.015343\n",
      "Encoder training step: 19/50\n",
      "[DEBUG] Loss this iteration: 0.011497\n",
      "Encoder training step: 20/50\n",
      "[DEBUG] Loss this iteration: 0.016664\n",
      "Encoder training step: 21/50\n",
      "[DEBUG] Loss this iteration: 0.016641\n",
      "Encoder training step: 22/50\n",
      "[DEBUG] Loss this iteration: 0.016924\n",
      "Encoder training step: 23/50\n",
      "[DEBUG] Loss this iteration: 0.015984\n",
      "Encoder training step: 24/50\n",
      "[DEBUG] Loss this iteration: 0.015769\n",
      "Encoder training step: 25/50\n",
      "[DEBUG] Loss this iteration: 0.014336\n",
      "Encoder training step: 26/50\n",
      "[DEBUG] Loss this iteration: 0.013728\n",
      "Encoder training step: 27/50\n",
      "[DEBUG] Loss this iteration: 0.015022\n",
      "Encoder training step: 28/50\n",
      "[DEBUG] Loss this iteration: 0.016579\n",
      "Encoder training step: 29/50\n",
      "[DEBUG] Loss this iteration: 0.012595\n",
      "Encoder training step: 30/50\n",
      "[DEBUG] Loss this iteration: 0.011774\n",
      "Encoder training step: 31/50\n",
      "[DEBUG] Loss this iteration: 0.011959\n",
      "Encoder training step: 32/50\n",
      "[DEBUG] Loss this iteration: 0.015768\n",
      "Encoder training step: 33/50\n",
      "[DEBUG] Loss this iteration: 0.016172\n",
      "Encoder training step: 34/50\n",
      "[DEBUG] Loss this iteration: 0.016731\n",
      "Encoder training step: 35/50\n",
      "[DEBUG] Loss this iteration: 0.013235\n",
      "Encoder training step: 36/50\n",
      "[DEBUG] Loss this iteration: 0.013559\n",
      "Encoder training step: 37/50\n",
      "[DEBUG] Loss this iteration: 0.015367\n",
      "Encoder training step: 38/50\n",
      "[DEBUG] Loss this iteration: 0.017234\n",
      "Encoder training step: 39/50\n",
      "[DEBUG] Loss this iteration: 0.013714\n",
      "Encoder training step: 40/50\n",
      "[DEBUG] Loss this iteration: 0.014401\n",
      "Encoder training step: 41/50\n",
      "[DEBUG] Loss this iteration: 0.015850\n",
      "Encoder training step: 42/50\n",
      "[DEBUG] Loss this iteration: 0.013370\n",
      "Encoder training step: 43/50\n",
      "[DEBUG] Loss this iteration: 0.013993\n",
      "Encoder training step: 44/50\n",
      "[DEBUG] Loss this iteration: 0.014471\n",
      "Encoder training step: 45/50\n",
      "[DEBUG] Loss this iteration: 0.011354\n",
      "Encoder training step: 46/50\n",
      "[DEBUG] Loss this iteration: 0.013878\n",
      "Encoder training step: 47/50\n",
      "[DEBUG] Loss this iteration: 0.011025\n",
      "Encoder training step: 48/50\n",
      "[DEBUG] Loss this iteration: 0.015428\n",
      "Encoder training step: 49/50\n",
      "Loss S:  tensor(0.0870, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 0/50\n",
      "Loss S:  tensor(0.0796, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 1/50\n",
      "Loss S:  tensor(0.0729, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 2/50\n",
      "Loss S:  tensor(0.0666, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 3/50\n",
      "Loss S:  tensor(0.0610, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 4/50\n",
      "Loss S:  tensor(0.0550, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 5/50\n",
      "Loss S:  tensor(0.0505, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 6/50\n",
      "Loss S:  tensor(0.0462, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 7/50\n",
      "Loss S:  tensor(0.0421, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 8/50\n",
      "Loss S:  tensor(0.0387, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 9/50\n",
      "Loss S:  tensor(0.0352, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 10/50\n",
      "Loss S:  tensor(0.0328, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 11/50\n",
      "Loss S:  tensor(0.0300, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 12/50\n",
      "Loss S:  tensor(0.0274, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 13/50\n",
      "Loss S:  tensor(0.0252, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 14/50\n",
      "Loss S:  tensor(0.0235, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 15/50\n",
      "Loss S:  tensor(0.0214, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 16/50\n",
      "Loss S:  tensor(0.0197, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 17/50\n",
      "Loss S:  tensor(0.0181, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 18/50\n",
      "Loss S:  tensor(0.0168, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 19/50\n",
      "Loss S:  tensor(0.0160, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 20/50\n",
      "Loss S:  tensor(0.0141, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 21/50\n",
      "Loss S:  tensor(0.0135, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 22/50\n",
      "Loss S:  tensor(0.0131, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 23/50\n",
      "Loss S:  tensor(0.0120, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 24/50\n",
      "Loss S:  tensor(0.0113, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 25/50\n",
      "Loss S:  tensor(0.0104, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 26/50\n",
      "Loss S:  tensor(0.0098, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 27/50\n",
      "Loss S:  tensor(0.0097, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 28/50\n",
      "Loss S:  tensor(0.0095, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 29/50\n",
      "Loss S:  tensor(0.0084, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 30/50\n",
      "Loss S:  tensor(0.0081, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 31/50\n",
      "Loss S:  tensor(0.0075, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 32/50\n",
      "Loss S:  tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 33/50\n",
      "Loss S:  tensor(0.0075, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 34/50\n",
      "Loss S:  tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 35/50\n",
      "Loss S:  tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 36/50\n",
      "Loss S:  tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 37/50\n",
      "Loss S:  tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 38/50\n",
      "Loss S:  tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 39/50\n",
      "Loss S:  tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 40/50\n",
      "Loss S:  tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 41/50\n",
      "Loss S:  tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 42/50\n",
      "Loss S:  tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 43/50\n",
      "Loss S:  tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 44/50\n",
      "Loss S:  tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 45/50\n",
      "Loss S:  tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 46/50\n",
      "Loss S:  tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 47/50\n",
      "Loss S:  tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 48/50\n",
      "Loss S:  tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Supervisor training step: 49/50\n",
      "Loss G (total):  tensor(16.5435, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 0/50\n",
      "Loss G (total):  tensor(20.4394, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 1/50\n",
      "Loss G (total):  tensor(13.8559, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 2/50\n",
      "Loss G (total):  tensor(21.7483, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 3/50\n",
      "Loss G (total):  tensor(19.8555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 4/50\n",
      "Loss G (total):  tensor(16.1962, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 5/50\n",
      "Loss G (total):  tensor(17.5610, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 6/50\n",
      "Loss G (total):  tensor(17.0031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 7/50\n",
      "Loss G (total):  tensor(24.8299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 8/50\n",
      "Loss G (total):  tensor(17.9724, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 9/50\n",
      "Loss G (total):  tensor(19.1959, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 10/50\n",
      "Loss G (total):  tensor(20.9231, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 11/50\n",
      "Loss G (total):  tensor(20.0443, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 12/50\n",
      "Loss G (total):  tensor(22.6175, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 13/50\n",
      "Loss G (total):  tensor(23.2688, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 14/50\n",
      "Loss G (total):  tensor(23.4099, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 15/50\n",
      "Loss G (total):  tensor(19.6068, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 16/50\n",
      "Loss G (total):  tensor(20.9520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 17/50\n",
      "Loss G (total):  tensor(22.6624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 18/50\n",
      "Loss G (total):  tensor(24.9108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 19/50\n",
      "Loss G (total):  tensor(26.4662, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 20/50\n",
      "Loss G (total):  tensor(23.6480, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 21/50\n",
      "Loss G (total):  tensor(25.6067, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 22/50\n",
      "Loss G (total):  tensor(28.1292, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 23/50\n",
      "Loss G (total):  tensor(27.6468, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 24/50\n",
      "Loss G (total):  tensor(29.3663, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 25/50\n",
      "Loss G (total):  tensor(26.2954, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 26/50\n",
      "Loss G (total):  tensor(25.6415, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 27/50\n",
      "Loss G (total):  tensor(27.9146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 28/50\n",
      "Loss G (total):  tensor(27.6432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 29/50\n",
      "Loss G (total):  tensor(28.3496, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 30/50\n",
      "Loss G (total):  tensor(28.8032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 31/50\n",
      "Loss G (total):  tensor(28.2500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 32/50\n",
      "Loss G (total):  tensor(27.6926, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 33/50\n",
      "Loss G (total):  tensor(28.4272, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 34/50\n",
      "Loss G (total):  tensor(28.2084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 35/50\n",
      "Loss G (total):  tensor(26.3732, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 36/50\n",
      "Loss G (total):  tensor(27.5566, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 37/50\n",
      "Loss G (total):  tensor(29.4640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 38/50\n",
      "Loss G (total):  tensor(30.1923, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 39/50\n",
      "Loss G (total):  tensor(28.9753, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 40/50\n",
      "Loss G (total):  tensor(31.2981, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 41/50\n",
      "Loss G (total):  tensor(32.5333, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 42/50\n",
      "Loss G (total):  tensor(31.7688, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 43/50\n",
      "Loss G (total):  tensor(28.5856, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 44/50\n",
      "Loss G (total):  tensor(34.3090, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 45/50\n",
      "Loss G (total):  tensor(32.9568, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 46/50\n",
      "Loss G (total):  tensor(28.7392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 47/50\n",
      "Loss G (total):  tensor(37.6679, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 48/50\n",
      "Loss G (total):  tensor(27.9772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "WGAN adversarial step: 49/50\n",
      "Finish Synthetic Data Generation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from options_TGAN import Options\n",
    "from lib.TimeGAN import TimeGAN\n",
    "\n",
    "# 1. Options\n",
    "opt = Options().parse()\n",
    "\n",
    "\n",
    "# 2. Set paper-style hyperparameters\n",
    "opt.lr = 1e-4\n",
    "opt.beta1 = 0.5\n",
    "\n",
    "opt.batch_size = 64\n",
    "opt.iteration = 50\n",
    "\n",
    "opt.hidden_dim = 256   # muy importante\n",
    "opt.num_layer = 3\n",
    "\n",
    "opt.n_critic = 5       # OK\n",
    "opt.gp_lambda = 10.0   # OK\n",
    "opt.name = \"TimeGAN_real_paper_settings\"\n",
    "\n",
    "opt.w_g    = 1.0   # antes 80\n",
    "opt.w_e0   = 1.0   # antes 10\n",
    "opt.w_es   = 1.0   # antes 0.1, lo dejamos en la misma escala\n",
    "opt.w_gamma = 0.5  # este se puede mantener\n",
    "\n",
    "\n",
    "# 3. Create model\n",
    "model = TimeGAN(opt, ori_data)\n",
    "\n",
    "# 4. Train\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f733bd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧩 Generating 50 samples in 1 batches of 64...\n",
      "  ✅ Batch 1/1 generated (50 samples)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌀 Ejecutando t-SNE (puede tardar unos segundos)...\n",
      "[t-SNE] Computing 99 nearest neighbors...\n",
      "[t-SNE] Indexed 100 samples in 0.001s...\n",
      "[t-SNE] Computed neighbors for 100 samples in 0.023s...\n",
      "[t-SNE] Computed conditional probabilities for sample 100 / 100\n",
      "[t-SNE] Mean sigma: 1.670499\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 49.422096\n",
      "[t-SNE] KL divergence after 500 iterations: 0.061375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dario\\Desktop\\ThesiS JBP\\jordan_venv\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from generation_TGAN import safe_generation\n",
    "generated_data = safe_generation(model, num_samples=50, batch_size=64)\n",
    "\n",
    "n_vis = 5000  # target\n",
    "\n",
    "n_real = min(n_vis, len(ori_data))\n",
    "n_fake = min(n_vis, len(generated_data))\n",
    "\n",
    "idx_real = np.random.choice(len(ori_data), n_real, replace=False)\n",
    "idx_fake = np.random.choice(len(generated_data), n_fake, replace=False)\n",
    "\n",
    "ori_vis = [ori_data[i] for i in idx_real]\n",
    "gen_vis = [generated_data[i] for i in idx_fake]\n",
    "\n",
    "from visualization_TGAN import visualization\n",
    "\n",
    "visualization(ori_vis, gen_vis, 'pca')\n",
    "visualization(ori_vis, gen_vis, 'tsne')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jordan_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
